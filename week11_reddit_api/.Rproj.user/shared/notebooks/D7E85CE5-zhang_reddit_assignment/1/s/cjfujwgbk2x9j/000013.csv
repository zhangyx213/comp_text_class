"0","#Tokenize the text column"
"0","abort_text <- str_replace_all(top_abort_urls$text, ""- "", """")"
"0","abort_text_df <- tibble(abort_text,)"
"0",""
"0","abort_tokenized <- abort_text_df %>%"
"0","  unnest_tokens(word,abort_text)"
"0",""
"0","#remove stop words"
"0","data(stop_words)"
"0","abort_tokenized <- abort_tokenized%>%"
"0","  anti_join(stop_words, by = c(""word"" = ""word"")) %>%"
"0","  filter(word != ""temp_file"") %>%"
"0","  filter(word != ""stories_corpus"") %>%"
"0","  filter(!grepl('[0-9]', word))"
"0",""
"0","bigrams <- abort_text_df %>%"
"0","  unnest_tokens(bigram, abort_text, token=""ngrams"", n=2)"
"0",""
"0","bigrams"
