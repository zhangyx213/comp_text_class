filter(!tokens %in% stop_words$word)
# join tokenized words with the sentiment dictionary
harris_sentiment_analysis <- harris_sentiment |>
inner_join(afinn, by = c("tokens"="word")) |>
group_by(index) |>
summarize(sentiment = sum(value), .groups = "drop")
# aggregate at article level, total sentiment score (Positive or Negative)
harris_sentiment_analysis <- harris_sentiment_analysis |>
group_by(index) |>
mutate(sentiment_type = ifelse(sentiment >= 0, "Positive", "Negative")) |>
inner_join(final_index, by=("index")) |>
select(index, date, sentiment, sentiment_type, filename, url)
View(harris_sentiment)
View(harris_sentiment_analysis)
# aggregate at article level, total sentiment score (Positive or Negative)
harris_sentiment_analysis <- harris_sentiment_analysis |>
group_by(index) |>
mutate(sentiment_type = ifelse(sentiment >= 0, "Positive", "Negative")) |>
inner_join(final_index, by=("index")) |>
select(index, date, sentiment, sentiment_type, filename, url)
#tokenize the dataframe, grouping by article number
harris_sentiment <- harris_articles_df |>
select(sentence, index) |>
mutate(text = str_squish(sentence)) |>
mutate(text = tolower(text)) |>
mutate(text = str_replace_all(text, "- ", "")) |>
group_by(index) |>
unnest_tokens(tokens, text) |>
filter(!tokens %in% stop_words$word)
# join tokenized words with the sentiment dictionary
harris_sentiment_analysis <- harris_sentiment |>
inner_join(afinn, by = c("tokens"="word")) |>
group_by(index) |>
summarize(sentiment = sum(value), .groups = "drop")
# aggregate at article level, total sentiment score (Positive or Negative)
harris_sentiment_analysis <- harris_sentiment_analysis |>
group_by(index) |>
mutate(sentiment_type = ifelse(sentiment >= 0, "Positive", "Negative")) |>
inner_join(final_index, by=("index")) |>
select(index, date, sentiment, sentiment_type, filename, url)
View(harris_sentiment_analysis)
ggplot(harris_sentiment_analysis, aes(x = index, y = sentiment, fill = sentiment_type)) +
geom_col(position = "dodge", stat = "identity") +
scale_fill_manual(values = c("Positive" = "darkgreen", "Negative" = "red")) +
labs(title = "Sentiment of Daily Wire coverage of Kamala Harris, October 2024",
caption = "n=96 articles. Afinn sentiment. Graphic by Rob Wells 1-25-2025",
x = "Articles",
y = "Sentiment Score")
combined <- read.csv("./assets/trump-harris-DW-combined.csv")
#There are nine common articles the Trump / Harris corpra. Create a new category for shared article
combined_index <- combined |>
distinct(filename, url, candidate, .keep_all = TRUE) |>
group_by(filename) |>
mutate(shared_article = n() > 1) |>
ungroup()
write.csv(combined_index, "./assets/combined_index.csv")
combined |>
distinct(filename, url, candidate) |>
count(candidate)
# load sentiment dictionary
afinn <- get_sentiments("afinn")
#tokenize the dataframe, grouping by article number
combined_sentiment <- combined |>
select(sentence, candidate, index) |>
mutate(text = str_squish(sentence)) |>
mutate(text = tolower(text)) |>
mutate(text = str_replace_all(text, "- ", "")) |>
group_by(index, candidate) |>
unnest_tokens(tokens, text) |>
filter(!tokens %in% stop_words$word)
# Sentiment analysis by joining the tokenized words with the AFINN lexicon
combined_sentiment_analysis <- combined_sentiment |>
inner_join(afinn, by = c("tokens"="word")) |>
group_by(index, candidate) |>
summarize(sentiment = sum(value), .groups = "drop") |>
inner_join(combined_index, by = c("index", "candidate"))
# aggregate at article level, total sentiment score (Positive or Negative)
combined_sentiment_analysis <- combined_sentiment_analysis |>
group_by(index) |>
mutate(sentiment_type = ifelse(sentiment >= 0, "Positive", "Negative"))
ggplot(combined_sentiment_analysis,
aes(x = date, y = sentiment, fill = sentiment_type)) +
geom_col(position = "dodge") +
scale_fill_manual(values = c("Positive" = "darkgreen", "Negative" = "red")) +
facet_wrap(~candidate, ncol = 1) +  # This creates separate panels for each candidate
labs(title = "Harris, Trump Sentiment in Daily Wire coverage",
caption = "Oct 14-Nov 4, 2024. Afinn sentiment. Graphic by Rob Wells 1-25-2025",
x = "Date",
y = "Sentiment Score") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Makes date labels more readable
negative_bigrams <- combined |>
select(sentence, candidate) |>
mutate(text = str_squish(sentence)) |>
mutate(text = tolower(text)) |>
mutate(text = str_replace_all(text, "- ", "")) |>
unnest_tokens(bigram, text, token="ngrams", n=2 ) |>
separate(bigram, c("word1", "word2"), sep = " ") |>
filter(!word1 %in% stop_words$word) |>
filter(!word2 %in% stop_words$word) |>
#adding candidate here to specify Trump v Harris
count(candidate, word1, word2, sort = TRUE) |>
filter(!is.na(word1)) |>
mutate(bigram = paste0(word1," ", word2))|>
#left join keeps all scores, need to do it twice
left_join(afinn, by = c("word1" = "word")) |>
rename(value1 = value) |>
# left join second word
left_join(afinn, by = c("word2" = "word")) |>
rename(value2 = value) |>
# Combine scores
mutate(
total_sentiment = coalesce(value1, 0) + coalesce(value2, 0)
) |>
filter(total_sentiment < 0)
top_50_negative_bigrams <- negative_bigrams |>
slice_min(total_sentiment, n= 50) |>
select(bigram, total_sentiment, candidate) |>
arrange(total_sentiment)
datatable(top_50_negative_bigrams,
caption = "Top 50 Negative Phrases",
options = list(pageLength = 20))
# Create a corpus using the 'sentence' column as the text field
my_corpus <- corpus(combined, text_field = "sentence") # build a new corpus from the texts
docvars(my_corpus)$candidate <- combined$candidate
# First add a unique identifier to combined
combined <- combined |>
mutate(doc_id = paste0(filename, "_", row_number()))
# First, create a lookup dataframe from your original combined data
lookup_df <- combined |>
select(doc_id, candidate) |>
distinct()
my_tokens <- tokens(my_corpus)
# Now modify your KWIC analysis to join with this lookup
quanteda_test <- kwic(my_tokens, pattern = "media", valuetype = "regex") |>
as.data.frame() |>
# Extract the filename part before the underscore and row number
mutate(doc_id = docname) |>
left_join(lookup_df, by = "doc_id")
media_narratives <- kwic(my_tokens,
pattern = c("media", "reporter", "newspaper", "journalist"),
valuetype = "fixed") |>
as.data.frame() |>
mutate(doc_id = docname) |>
left_join(lookup_df, by = "doc_id")
media_narratives_table <- media_narratives |>
select(candidate, pre, keyword, post, doc_id)
datatable(media_narratives_table,
caption = "Top 50 Negative Phrases",
options = list(pageLength = 20))
View(combined_sentiment_analysis)
View(combined_sentiment)
View(media_narratives)
#tokenize the headlines only
headline_sentiment <- combined_index |>
select(filename, candidate, index) |>
mutate(text = str_replace_all(filename, "_", " "),
text = str_replace_all(text, ".txt", "")) |>    group_by(index, candidate) |>
unnest_tokens(tokens, text) |>
filter(!tokens %in% stop_words$word)
# Sentiment analysis by joining the tokenized words with the AFINN lexicon
headline_sentiment_analysis <- headline_sentiment |>
inner_join(afinn, by = c("tokens"="word")) |>
group_by(index, candidate) |>
summarize(sentiment = sum(value), .groups = "drop") |>
inner_join(combined_index, by = c("index", "candidate"))
# aggregate at article level, total sentiment score (Positive or Negative)
headline_sentiment_analysis <- headline_sentiment_analysis|>
group_by(index) |>
mutate(sentiment_type = ifelse(sentiment >= 0, "Positive", "Negative"))
ggplot(headline_sentiment_analysis,
aes(x = date, y = sentiment, fill = sentiment_type)) +
geom_col(position = "dodge") +
scale_fill_manual(values = c("Positive" = "darkgreen", "Negative" = "red")) +
facet_wrap(~candidate, ncol = 1) +  # This creates separate panels for each candidate
labs(title = "Harris, Trump Headline Sentiment in Daily Wire coverage",
caption = "Oct 14-Nov 4, 2024. Afinn sentiment. Graphic by Rob Wells 1-25-2025",
x = "Date",
y = "Sentiment Score") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Makes date labels more readable
tfiles <- list.files("./trump_extracted_text", pattern="*.txt") |>
as.data.frame() |>
rename(filename = 1) |>
mutate(name = str_replace_all(filename, ".txt", "")) |>
mutate(name = str_replace_all(tolower(name), " ", "_")) |>
mutate(name = str_replace_all(name, "[[:punct:]]", "")) |>
arrange((name)) |>
mutate(index = row_number())
trump_index <- rio::import("./assets/DW_Trump_Articles.xlsx") |>
mutate(name = str_replace_all(tolower(Name), " ", "_")) |>
mutate(name = str_replace_all(name, "[[:punct:]]", "")) |>
arrange((name)) |>
mutate(index = row_number()) |>
rename(date = N) |>
distinct(name, .keep_all = TRUE)
tfinal_index <- tfiles |>
inner_join(trump_index, by=c("name")) |>
mutate(filepath = paste0("./trump_extracted_text/", filename)) |>
janitor::clean_names() |>
rename(index = index_x) |>
distinct(index, .keep_all = TRUE) |>
select(index, date,filename,url, filepath, filename)
#fact check
anti <- tfiles |>
anti_join(trump_index, by=c("name"))
###
# Define function to loop through each text file
###
create_article_text <- function(row_value) {
#row_value is the single argument that is passed to the function
# Take each row of the dataframe
temp <- tfinal_index |>
slice(row_value)
# Store the filename for  use in constructing articles dataframe
temp_filename <- temp$filename
# Create a dataframe by reading in lines of a given textfile
# Add a filename column
articles_df_temp <- read_lines(temp$filepath) |>
as_tibble() |>
mutate(filename = temp_filename)
# Bind results to master articles_df
# <<- returns to global environment
articles_df <<- articles_df |>
bind_rows(articles_df_temp)
}
###
# Create elements needed to run function
###
# Create empty tibble to store results
articles_df <- tibble()
#running once to test
#create_article_text(2)
# Create an array of numbers to loop through, from 1 to the number of rows in our index dataframe
row_values <- 1:nrow(tfinal_index)
###
# Execute function using lapply
# This loops through each row of the dataframe and append results to master file
###
lapply(row_values, create_article_text)
###
# Clean up articles_df and join to index dataframe
###
trump_articles_df <- articles_df |>
select(filename, sentence=value) |>
inner_join(tfinal_index)
phrases <- c("Date:" = "",
"Article Text:" = "",
"Headline:" = "",
"Already have an account?" = "",
"Your information could be the missing piece to an important story. Submit your tip today and make a difference." = "",
"Stay up-to-date on the latestnews, podcasts, and more." = "",
"https" = "")
trump_articles_df <- trump_articles_df |>
mutate(sentence = str_replace_all(sentence, phrases))
#write.csv(trump_articles_df, "trump_extracted_text_jan2025.csv")
#library(tidytext)
trump_bigrams <- trump_articles_df |>
select(sentence) |>
mutate(text = str_squish(sentence)) |>
mutate(text = tolower(text)) |>
mutate(text = str_replace_all(text, "- ", "")) |>
unnest_tokens(bigram, text, token="ngrams", n=2 ) |>
separate(bigram, c("word1", "word2"), sep = " ") |>
filter(!word1 %in% stop_words$word) |>
filter(!word2 %in% stop_words$word) |>
count(word1, word2, sort = TRUE) |>
filter(!is.na(word1))
top_20_trump_bigrams <- trump_bigrams |>
top_n(20) |>
mutate(bigram = paste(word1, " ", word2)) |>
select(bigram, n)
ggplot(top_20_trump_bigrams, aes(x = reorder(bigram, n), y = n, fill=n)) +
geom_bar(stat = "identity") +
theme(legend.position = "none") +
coord_flip() +
labs(title = "Top Two-Word phrases about Donald Trump on Daily Wire coverage",
caption = "n=99 articles, 2024. Graphic by Rob Wells. 1-23-2025",
x = "Phrase",
y = "Count of terms")
# load sentiment dictionary
afinn <- get_sentiments("afinn")
#tokenize the dataframe, grouping by article number
trump_sentiment <- trump_articles_df |>
select(sentence, index) |>
mutate(text = str_squish(sentence)) |>
mutate(text = tolower(text)) |>
mutate(text = str_replace_all(text, "- ", "")) |>
group_by(index) |>
unnest_tokens(tokens, text) |>
filter(!tokens %in% stop_words$word)
# Sentiment analysis by joining the tokenized words with the AFINN lexicon
trump_sentiment_analysis <- trump_sentiment |>
inner_join(afinn, by = c("tokens"="word")) |>
group_by(index) |>
summarize(sentiment = sum(value), .groups = "drop")
# aggregate at article level, total sentiment score (Positive or Negative)
trump_sentiment_analysis <- trump_sentiment_analysis |>
group_by(index) |>
mutate(sentiment_type = ifelse(sentiment >= 0, "Positive", "Negative")) |>
inner_join(tfinal_index, by=("index")) |>
select(index, date, sentiment, sentiment_type, filename, url)
ggplot(trump_sentiment_analysis, aes(x = index, y = sentiment, fill = sentiment_type)) +
geom_col(position = "dodge", stat = "identity") +
scale_fill_manual(values = c("Positive" = "darkgreen", "Negative" = "red")) +
labs(title = "Sentiment of Daily Wire coverage of Donald Trump, October 2024",
caption = "n=99 articles. Afinn sentiment. Graphic by Rob Wells 1-25-2025",
x = "Articles",
y = "Sentiment Score")
harris_articles_df <- harris_articles_df |>
mutate(candidate = "harris")
trump_articles_df <- trump_articles_df |>
mutate(candidate = "trump")
#combined <- rbind(harris_articles_df, trump_articles_df)
#write.csv(combined, "trump-harris-DW-combined.csv")
harris_articles_df <- harris_articles_df |>
mutate(candidate = "harris")
trump_articles_df <- trump_articles_df |>
mutate(candidate = "trump")
combined <- rbind(harris_articles_df, trump_articles_df)
#install.packages("udpipe")
library(udpipe)
library(tidyverse)
udmodel <- udpipe_download_model(language = "english")
udmodel <- udpipe_load_model(udmodel$file_model)
# Then run your annotation
annotated_text <- udpipe_annotate(udmodel,
x = harris_articles_df$sentence,
doc_id = harris_articles_df$filename)
#Install packages if you don't have them already
#install.packages("tidyverse")
#install.packages("tidytext")
#install.packages("quanteda")
#install.packages("readtext")
#install.packages("DT")
#NICAR note: add these two
#install.packages("rio")
#install.packages("lubridate")
library(tidyverse)
library(tidytext)
library(quanteda)
library(readtext)
library(DT)
library(lubridate)
library(rio)
#text obtained with a web scraper
files <- list.files("./kamala_extracted_text", pattern="*.txt") |>
as.data.frame() |>
rename(filename = 1) |>
mutate(name = str_replace_all(filename, ".txt", "")) |>
mutate(name = str_replace_all(tolower(name), " ", "_")) |>
mutate(name = str_replace_all(name, "[[:punct:]]", "")) |>
arrange((name)) |>
mutate(index = row_number())
#  #create an index with the file name
# mutate(index = str_extract(filename, "\\d+")) |>
#  mutate(index = as.numeric(index))
kharris_index <- rio::import("./assets/DW_Kamala_Articles.xlsx") |>
mutate(name = str_replace_all(tolower(NAME), " ", "_")) |>
mutate(name = str_replace_all(name, "[[:punct:]]", "")) |>
arrange((name)) |>
mutate(index = row_number()) |>
distinct(name, .keep_all = TRUE) |>
mutate(DATE = case_when(
str_detect(as.character(DATE), "2023-11-04") ~ as_datetime("2024-11-04"),
TRUE ~ DATE
))
final_index <- files |>
inner_join(kharris_index, by=c("name")) |>
mutate(filepath = paste0("./kamala_extracted_text/", filename)) |>
janitor::clean_names() |>
rename(index = index_x) |>
distinct(index, .keep_all = TRUE) |>
select(index, date,filename,url, filepath, filename)
#fact check
anti <- files |>
anti_join(kharris_index, by=c("name"))
###
# Define function to loop through each text file
###
create_article_text <- function(row_value) {
#row_value is the single argument that is passed to the function
# Take each row of the dataframe
temp <- final_index |>
slice(row_value)
# Store the filename for  use in constructing articles dataframe
temp_filename <- temp$filename
# Create a dataframe by reading in lines of a given textfile
# Add a filename column
articles_df_temp <- read_lines(temp$filepath) |>
as_tibble() |>
mutate(filename = temp_filename)
# Bind results to master articles_df
# <<- returns to global environment
articles_df <<- articles_df |>
bind_rows(articles_df_temp)
}
###
# Create elements needed to run function
###
# Create empty tibble to store results
articles_df <- tibble()
#running once to test
#create_article_text(2)
# Create an array of numbers to loop through, from 1 to the number of rows in our index dataframe
row_values <- 1:nrow(final_index)
###
# Execute function using lapply
# This loops through each row of the dataframe and append results to master file
###
lapply(row_values, create_article_text)
###
# Clean up articles_df and join to index dataframe
###
articles_df <- articles_df |>
select(filename, sentence=value) |>
inner_join(final_index)
phrases <- c("Date:" = "",
"Article Text:" = "",
"Headline:" = "",
"Already have an account?" = "",
"Your information could be the missing piece to an important story. Submit your tip today and make a difference." = "",
"Stay up-to-date on the latestnews, podcasts, and more." = "",
"https" = "")
harris_articles_df <- articles_df |>
mutate(sentence = str_replace_all(sentence, phrases))
#write.csv(harris_articles_df, "kharris_extracted_text_jan2025.csv")
harris_articles_df <- read.csv("./assets/kharris_extracted_text_jan2025.csv")
harris_articles_df |>
distinct(filename) |>
count(filename) |>
summarise(sum(n))
# Then run your annotation
annotated_text <- udpipe_annotate(udmodel,
x = harris_articles_df$sentence,
doc_id = harris_articles_df$filename)
annotated_df <- as.data.frame(annotated_text)
View(annotated_df)
# Filter for adjectives (upos = "ADJ") and count frequencies
adjective_counts <- annotated_df %>%
filter(upos == "ADJ") %>%
group_by(token) %>%
summarise(
frequency = n(),
articles = n_distinct(doc_id)
) %>%
arrange(desc(frequency))
# View the top 20 most frequent adjectives
head(adjective_counts, 20)
datatable(adjective_counts,
caption = "Top adjectives in Harris DW coverage",
options = list(pageLength = 50))
adjectives_by_article <- annotated_df %>%
filter(upos == "ADJ") %>%
group_by(doc_id, token) %>%
summarise(
frequency = n()
) %>%
arrange(doc_id, desc(frequency))
datatable(adjectives_by_article,
caption = "Adjectives by article in Harris DW coverage",
options = list(pageLength = 50))
adj_noun_pairs <- annotated_df %>%
inner_join(
annotated_df %>% select(doc_id, sentence_id, token, token_id, upos),
by = c("doc_id", "sentence_id", "head_token_id" = "token_id")
) %>%
# Filter for adjectives modifying nouns
filter(
dep_rel == "amod" &    # amod = adjectival modifier
upos.x == "ADJ" &      # first token is adjective
upos.y == "NOUN"       # head word is noun
) %>%
# Select and rename relevant columns
select(
article_id = doc_id,
sentence_id,
adjective = token.x,
noun = token.y
)
# Count frequencies of adjective-noun pairs
adj_noun_counts <- adj_noun_pairs %>%
group_by(adjective, noun) %>%
summarise(
frequency = n(),
articles = n_distinct(article_id)
) %>%
arrange(desc(frequency))
datatable(adj_noun_counts,
caption = "Top adjectives-noun pairs in Harris DW coverage",
options = list(pageLength = 50))
pairs_by_article <- adj_noun_pairs %>%
group_by(article_id, adjective, noun) %>%
summarise(
frequency = n()
) %>%
arrange(article_id, desc(frequency))
datatable(pairs_by_article,
caption = "Adjectives-noun pairs by article in Harris DW coverage",
options = list(pageLength = 50))
library(lattice)
stats <- txt_freq(annotated_df$upos)
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = stats, col = "cadetblue",
main = "Parts of Speech in Harris DW coverage \n frequency of occurrence",
xlab = "Freq")
## NOUNS
stats <- subset(annotated_df, upos %in% c("NOUN"))
stats <- txt_freq(stats$token)
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = head(stats, 20), col = "cadetblue",
main = "Most occurring nouns", xlab = "Freq")
