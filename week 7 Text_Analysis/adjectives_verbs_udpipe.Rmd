---
title: "Linguistic analysis: Extracting adjectives, adverbs"
author: "Wells"
date: "2025-03-05"
output: html_document
editor_options: 
  markdown: 
    wrap: sentence
---

#Introduction

This tutorial begins to explore how to extract adjectives, verbs and nouns from text.

It uses the [udpipe](https://cran.r-project.org/web/packages/udpipe/index.html) package in R:

Tokenization, Parts of Speech Tagging, Lemmatization and Dependency Parsing with the 'UDPipe' 'NLP' Toolkit.
This natural language processing toolkit provides language-agnostic 'tokenization', 'parts of speech tagging', 'lemmatization' and 'dependency parsing' of raw text.

This code loads an English language model to permit tagging of parts of speech.
This version only deals with adjectives.
It then counts all adjectives across all articles and then counts adjectives by article. Credit to Claude.ai for providing the basis of this code.

```{r}
#install.packages("udpipe")
library(udpipe)
library(tidyverse)
```

### Load English language model

```{r}
udmodel <- udpipe_download_model(language = "english")
udmodel <- udpipe_load_model(udmodel$file_model)
```

### Annotate parts of speech on Harris articles

```{r}
# Then run your annotation
annotated_text <- udpipe_annotate(udmodel, 
                                 x = harris_articles_df$sentence, 
                                 doc_id = harris_articles_df$filename)
annotated_df <- as.data.frame(annotated_text)
```

The annotated_df columns follow a "CoNLL-U" format for annotated linguistic data in Universal Dependencies.
It permits analysis of grammatical and syntactic structure of text.

Breaking down the details in an annotated dataframe from UDPipe:

-   doc_id: Identifier for the document - this matches harris_articles_df\$filename.

-   paragraph_id: Identifier for paragraphs within each document.

-   sentence_id: Identifier for sentences within each document.

-   sentence: The full text of the sentence.

-   token_id: Identifier for each token (word, punctuation) within a sentence.

-   token: The actual word or punctuation mark.

-   lemma: The base or dictionary form of the token (e.g., "running" â†’ "run").
    \
    upos: Universal Part of Speech tag - a standardized grammatical category (e.g., NOUN, VERB, ADJ).

-   xpos: Language-specific part of speech tag - more detailed than upos, varies by language.
    feats: Morphological features - additional grammatical information like Number=Sing, Tense=Past.

-   head_token_id: The ID of the token that the current token depends on in the dependency parse.

-   dep_rel: Dependency relation - describes the syntactic relationship between a token and its head (e.g., "nsubj" for nominal subject).

-   deps: Enhanced dependency graph in the format of head-relation pairs.

-   misc: Miscellaneous information that doesn't fit elsewhere, often contains spans of tokens or other metadata.

Learn more about [Universal Dependencies format](https://universaldependencies.org/format.html)



A nerdy paper about this process [Tokenizing, POS Tagging, Lemmatizing and Parsing UD 2.0 with UDPipe](https://ufal.mff.cuni.cz/~straka/papers/2017-conll_udpipe.pdf)


## Count adjectives
```{r}
# Filter for adjectives (upos = "ADJ") and count frequencies
adjective_counts <- annotated_df %>%
  filter(upos == "ADJ") %>%
  group_by(token) %>%
  summarise(
    frequency = n(),
    articles = n_distinct(doc_id)
  ) %>%
  arrange(desc(frequency))

# View the top 20 most frequent adjectives
head(adjective_counts, 20)

datatable(adjective_counts,
          caption = "Top adjectives in Harris DW coverage",
          options = list(pageLength = 50)) 

```

## Count adjectives by article
```{r}
adjectives_by_article <- annotated_df %>%
  filter(upos == "ADJ") %>%
  group_by(doc_id, token) %>%
  summarise(
    frequency = n()
  ) %>%
  arrange(doc_id, desc(frequency))

datatable(adjectives_by_article,
          caption = "Adjectives by article in Harris DW coverage",
          options = list(pageLength = 50)) 

```

# Extract adjective-noun pairs

This section extracts adjective-noun pairs where the adjective directly modifies the noun. It then tabulates the pairs by frequency and by article.

```{r}
adj_noun_pairs <- annotated_df %>%
  inner_join(
    annotated_df %>% select(doc_id, sentence_id, token, token_id, upos),
    by = c("doc_id", "sentence_id", "head_token_id" = "token_id")
  ) %>%
  # Filter for adjectives modifying nouns
  filter(
    dep_rel == "amod" &    # amod = adjectival modifier
    upos.x == "ADJ" &      # first token is adjective
    upos.y == "NOUN"       # head word is noun
  ) %>%
  # Select and rename relevant columns
  select(
    article_id = doc_id,
    sentence_id,
    adjective = token.x,
    noun = token.y
  )

# Count frequencies of adjective-noun pairs
adj_noun_counts <- adj_noun_pairs %>%
  group_by(adjective, noun) %>%
  summarise(
    frequency = n(),
    articles = n_distinct(article_id)
  ) %>%
  arrange(desc(frequency))

datatable(adj_noun_counts,
          caption = "Top adjectives-noun pairs in Harris DW coverage",
          options = list(pageLength = 50)) 

```


### Adjective - noun pairs by article
```{r}
pairs_by_article <- adj_noun_pairs %>%
  group_by(article_id, adjective, noun) %>%
  summarise(
    frequency = n()
  ) %>%
  arrange(article_id, desc(frequency))

datatable(pairs_by_article,
          caption = "Adjectives-noun pairs by article in Harris DW coverage",
          options = list(pageLength = 50)) 
```


#Bonus Zone
[Apply these ideas to our data.](https://cran.r-project.org/web/packages/udpipe/vignettes/udpipe-usecase-postagging-lemmatisation.html)


### Basic frequency statistics
[For a detailed list of all POS tags](https://universaldependencies.org/u/pos/index.html)

```{r}
library(lattice)
stats <- txt_freq(annotated_df$upos)
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = stats, col = "cadetblue", 
         main = "Parts of Speech in Harris DW coverage \n frequency of occurrence", 
         xlab = "Freq")
```
### Examine nouns
```{r}
## NOUNS
stats <- subset(annotated_df, upos %in% c("NOUN")) 
stats <- txt_freq(stats$token)
stats$key <- factor(stats$key, levels = rev(stats$key))
barchart(key ~ freq, data = head(stats, 20), col = "cadetblue", 
         main = "Most occurring nouns", xlab = "Freq")
```

