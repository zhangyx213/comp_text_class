#load tidyverse, tidytext, rio and quanteda libraries
#install.packages("quanteda")
library(tidyverse)
library(tidytext)
library(quanteda)
#Import dataframe
lynch <- rio::import("https://raw.githubusercontent.com/wellsdata/CompText_Jour/refs/heads/main/data/articles_oct_19.csv")
#Show range of years covered
years_ct <- lynch %>%
distinct(filename, .keep_all = TRUE) %>%
count(year)
y <- lynch %>%
distinct(filename, .keep_all = TRUE)
#Create chart of years
ggplot(years_ct,aes(x = year, y = n,
fill = n)) +
geom_col(position = "dodge") +
theme(legend.position = "none") +
labs(title = "Years of Lynching Coverage",
subtitle = "Based in 7,162 extracted articles",
caption = "Graphic by Rob Wells, 10-30-2023",
y="Articles",
x="Year")
# ggsave("../output_images_tables/Figure2_years_lynching_coverage_10.30.23.png",device = "png",width=9,height=6, dpi=800)
#Filter articles from 1940s forward
post1940 <-  lynch %>%
filter(year >= 1940)
post1940 %>%
select(filename) %>%
distinct(filename, .keep_all = TRUE) %>%
count(filename) %>%
summarize(total =sum(n))
#62 articles
statespost1940 <- post1940 %>%
select(newspaper_state, filename) %>%
distinct(filename, .keep_all = TRUE) %>%
count(newspaper_state) %>%
arrange(desc(n))
statespost1940 %>%
select(newspaper_state, n) %>%
slice_max(n, n=10)
#newspaper_state
# Michigan	20
# Minnesota	18
# District of Columbia	5
# Nebraska	4
# Illinois	3
# Mississippi	2
# North Carolina	2
# Washington	2
# Alaska	1
# Arizona	1
#Fact Check
#sum(statesthe1850s$n)
x <- post1940 %>%
distinct(filename, .keep_all = TRUE) %>%
arrange(date)
#write_csv(x, "post1940_index.csv")
stories <- str_replace_all(post1940$sentence, "- ", "")
stories_df <- tibble(stories,)
# unnest includes lower, punct removal
stories_tokenized <- stories_df %>%
unnest_tokens(word,stories)
stories_tokenized
bigrams2 <- bigrams1 %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) |>
filter(!is.na(word1)) |>
filter(!is.na(word2))
bigrams1 <- bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ")
bigrams <- stories_df %>%
unnest_tokens(bigram, stories, token="ngrams", n=2)
bigrams
#Filter out stop words.
bigrams1 <- bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ")
bigrams2 <- bigrams1 %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) |>
filter(!is.na(word1)) |>
filter(!is.na(word2))
bigram3 <- bigrams2 %>%
count(word1, word2, sort = TRUE)
bigram3 <- bigram3 %>%
mutate(decade = "post1940")
View(bigram3)
trigrams <- stories_df %>%
unnest_tokens(trigram, stories, token="ngrams", n=3)
#a simple count of the trigrams
trigrams |>
count(trigram) |>
arrange(desc(n))
trigrams2 <- trigrams |>
filter(!is.na(trigram)) |>
separate(trigram, c("word1", "word2", "word3"), sep = " ")
trigrams2 <- trigrams2 |>
count(word1, word2, word3, sort = TRUE)
#filtering stop words doesn't help much
trigrams2 <- trigrams2|>
filter(!word1 %in% stop_words$word) |>
filter(!word2 %in% stop_words$word)|>
filter(!word3 %in% stop_words$word) |>
mutate(decade = "post1940")
#write_csv(stories_trigrams_ct_post1940, "../output/post1940_lynch_trigram_count.csv")
View(trigrams2)
quint = stories_df %>%
unnest_tokens(quintgram, stories, token="ngrams", n=4)
quintgrams |>
count(quintgram) |>
arrange(desc(n))
quint = stories_df %>%
unnest_tokens(quintgram, stories, token="ngrams", n=4)
quint |>
count(quintgram) |>
arrange(desc(n))
quint = stories_df %>%
unnest_tokens(quintgram, stories, token="ngrams", n=4)
quint |>
filter(!is.na(words))
quint = stories_df %>%
unnest_tokens(quintgram, stories, token="ngrams", n=4)
quint |>
count(quintgram) |>
arrange(desc(n))
