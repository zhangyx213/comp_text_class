# Filter for international articles
kwic_international <- kwic_with_metadata |>
filter(category == "international")
coun
# Filter for international articles
kwic_international <- kwic_with_metadata |>
filter(category == "international")
View(kwic_international)
View(article_index)
library(quanteda)
library(readtext)
library(tidyverse)
read_csv("articles_df.csv")
articles <-read_csv("articles_df.csv")
articles_pub_count <- articles |>
count(publication_4)
print(articles_pub_count)
articles_pub_count <- articles |>
count(publication_4) |>
arrange(desc)
articles_pub_count <- articles |>
count(publication_4)
arrange(desc) |>
print(articles_pub_count)
articles_pub_count <- articles |>
count(publication_4)
arrange(desc)
articles_pub_count <- articles |>
count(publication_4)  |>
arrange(desc)
articles_pub_count <- articles |>
count(publication_4)  |>
arrange(publication_4)desc)
articles_pub_count <- articles |>
count(publication_4)  |>
arrange(desc(publication_4))
print(articles_pub_count)
View(articles_pub_count)
articles_pub_count <- articles |>
count(publication_4)  |>
arrange(desc(n))
print(articles_pub_count)
library(quanteda)
library(readtext)
library(tidyverse)
articles <-read_csv("articles_df.csv")
articles_pub_count <- articles |>
distinct( publication_4) |>  # Ensure unique articles per publication
count(publication_4) |>  # Count occurrences per publication
arrange(desc(n))
articles_pub_count <- articles |>
distinct(filename, publication_4) |>  # Ensure unique articles per publication
count(publication_4) |>  # Count occurrences per publication
arrange(desc(n))
trump_dei_articles_df <- trump_dei_articles_df %>%
mutate(unique_file_id = paste0(filename, "_", row_number()))
# Now create the corpus with the new unique identifier
trump_dei_corpus <- corpus(trump_dei_articles_df, text_field = "sentence",
docid_field = "unique_file_id")
# Tokenize the corpus
dei_tokens <- tokens(trump_dei_corpus)
kwic_dei_results <- kwic(
dei_tokens,
phrase(c("dei", "donald trump", "administration", "biden", "backlash",
"race", "controversy", "historical",
"miniorties", "hurt")),
window = 50, valuetype = "regex"
) |>
as.data.frame()
#Strips out unique file name from article index
kwic_dei_results <- kwic_dei_results |>
mutate(matchrow = str_extract(docname, "^[^_]+_[^_]+"))
metadata <- article_index
# Join KWIC results with metadata
kwic_with_metadata <- kwic_dei_results %>%
inner_join(metadata, by = c("matchrow" = "filename")) |>
distinct(docname, .keep_all= TRUE) |>
select(matchrow, docname, from, to, pre, keyword, post, title, pattern, published_date, publication_location, countries)
us_states <- c("Alabama", "Alaska", "Arizona", "Arkansas", "California", "Colorado",
"Connecticut", "Delaware", "Florida", "Georgia", "Hawaii", "Idaho",
"Illinois", "Indiana", "Iowa", "Kansas", "Kentucky", "Louisiana",
"Maine", "Maryland", "Massachusetts", "Michigan", "Minnesota",
"Mississippi", "Missouri", "Montana", "Nebraska", "Nevada",
"New Hampshire", "New Jersey", "New Mexico", "New York",
"North Carolina", "North Dakota", "Ohio", "Oklahoma", "Oregon",
"Pennsylvania", "Rhode Island", "South Carolina", "South Dakota",
"Tennessee", "Texas", "Utah", "Vermont", "Virginia", "Washington",
"West Virginia", "Wisconsin", "Wyoming", "Dist. of Columbia", "U.S. Federal")
# Add a column to classify as 'domestic' or 'international'
kwic_with_metadata <- kwic_with_metadata|>
mutate(category = ifelse(publication_location %in% us_states, "domestic", "international"))
kwic_domestic <- kwic_with_metadata|>
filter(category == "domestic")
View(kwic_domestic)
View(kwic_international)
kwic_domestic |>
head(100)
kwic_international |>
heaD(100)
# Filter for international articles
kwic_international <- kwic_with_metadata |>
filter(category == "international")
kwic_international |>
heaD(100)
kwic_international |>
head(100)
kwic_domestic |>
head(50)
kwic_international |>
head(50)
kwic_dei_results <- kwic(
dei_tokens,
phrase(c("dei", "donald trump", "administration", "biden", "backlash",
"race", "controversy", "historical",
"miniorties", "hurt", "culture war","affirmative action", "Trump rhetoric", "anti_woke", "global diversity", "global DEI", "Trump and global policies", "global")),
window = 50, valuetype = "regex"
) |>
as.data.frame()
View(articles)
View(articles)
View(articles)
View(articles)
View(articles)
View(trump_dei_articles_df)
count_articles <- trump_dei_articles_df
distinct(filename)
count_articles <- trump_dei_articles_df
distinct(filename) |>
count(filename)
count_articles <- trump_dei_articles_df |>
distinct(filename) |>
count(filename)
View(count_articles)
View(count_articles)
library(quanteda)
library(readtext)
library(tidyverse)
kwic_dei_results <- kwic(
dei_tokens,
phrase(c("dei", "donald trump", "administration", "biden", "backlash",
"race", "controversy", "historical",
"miniorties", "hurt", "culture war","affirmative action", "Trump rhetoric", "anti_woke", "global diversity", "global DEI", "Trump and global policies", "global")),
window = 50, valuetype = "regex"
) |>
as.data.frame()
library(quanteda)
library(readtext)
library(tidyverse)
trump_dei_articles_df <- read.csv("articles_df.csv")
article_index <- trump_dei_articles_df|>
distinct(filename, .keep_all = TRUE) |>
as.data.frame()
trump_dei_articles_df <- trump_dei_articles_df %>%
mutate(unique_file_id = paste0(filename, "_", row_number()))
# Now create the corpus with the new unique identifier
trump_dei_corpus <- corpus(trump_dei_articles_df, text_field = "sentence",
docid_field = "unique_file_id")
# Tokenize the corpus
dei_tokens <- tokens(trump_dei_corpus)
kwic_dei_results <- kwic(
dei_tokens,
phrase(c("dei", "donald trump", "administration", "biden", "backlash",
"race", "controversy", "historical",
"miniorties", "hurt", "culture war","affirmative action", "Trump rhetoric", "anti_woke", "global diversity", "global DEI", "Trump and global policies", "global")),
window = 50, valuetype = "regex"
) |>
as.data.frame()
#Strips out unique file name from article index
kwic_dei_results <- kwic_dei_results |>
mutate(matchrow = str_extract(docname, "^[^_]+_[^_]+"))
metadata <- article_index
# Join KWIC results with metadata
kwic_with_metadata <- kwic_dei_results %>%
inner_join(metadata, by = c("matchrow" = "filename")) |>
distinct(docname, .keep_all= TRUE) |>
select(matchrow, docname, from, to, pre, keyword, post, title, pattern, published_date, publication_location, countries)
us_states <- c("Alabama", "Alaska", "Arizona", "Arkansas", "California", "Colorado",
"Connecticut", "Delaware", "Florida", "Georgia", "Hawaii", "Idaho",
"Illinois", "Indiana", "Iowa", "Kansas", "Kentucky", "Louisiana",
"Maine", "Maryland", "Massachusetts", "Michigan", "Minnesota",
"Mississippi", "Missouri", "Montana", "Nebraska", "Nevada",
"New Hampshire", "New Jersey", "New Mexico", "New York",
"North Carolina", "North Dakota", "Ohio", "Oklahoma", "Oregon",
"Pennsylvania", "Rhode Island", "South Carolina", "South Dakota",
"Tennessee", "Texas", "Utah", "Vermont", "Virginia", "Washington",
"West Virginia", "Wisconsin", "Wyoming", "Dist. of Columbia", "U.S. Federal")
# Add a column to classify as 'domestic' or 'international'
kwic_with_metadata <- kwic_with_metadata|>
mutate(category = ifelse(publication_location %in% us_states, "domestic", "international"))
kwic_domestic <- kwic_with_metadata|>
filter(category == "domestic")
# Filter for international articles
kwic_international <- kwic_with_metadata |>
filter(category == "international")
kwic_domestic |>
head(50)
kwic_international |>
head(50)
#cut useless returns
deijunk <- c("mobile",  "automobile",  "automobiles", "mobilize", "mobley", "mobilizing", "democratic party of", "mobe", "moberly",  "mobil", "mobiles", "mobilized", "mobold")
kwic_with_metadata <- kwic_with_metadata %>%
filter(!keyword %in% mobjunk)
install.packages("here")
install.packages("tidytext")
install.packages("quanteda")
install.packages("tm")
install.packages("topicmodels")
install.packages("reshape2")
install.packages("ggplot2")
install.packages("wordcloud")
install.packages("pals")
install.packages("SnowballC")
install.packages("lda")
install.packages("ldatuning")
install.packages("kableExtra")
install.packages("DT")
install.packages("flextable")
install.packages("remotes")
remotes::install_github("rlesur/klippy")
# set options
options(stringsAsFactors = F)         # no automatic data transformation
options("scipen" = 100, "digits" = 4) # suppress math annotation
# load packages
here::here()
library(tidyverse)
library(tidytext)
library(rio)
library(readtext)
#topic modeling
library(quanteda)
library(tm)
library(topicmodels)
library(lda)
library(ldatuning)
# from tutorial packages
library(DT)
library(knitr)
library(kableExtra)
library(reshape2)
library(ggplot2)
library(wordcloud)
library(pals)
library(SnowballC)
library(flextable)
install.packages("here")
install.packages("tidytext")
# set options
options(stringsAsFactors = F)         # no automatic data transformation
options("scipen" = 100, "digits" = 4) # suppress math annotation
# load packages
here::here()
library(tidyverse)
library(tidytext)
library(rio)
library(readtext)
#topic modeling
library(quanteda)
library(tm)
library(topicmodels)
library(lda)
library(ldatuning)
# from tutorial packages
library(DT)
library(knitr)
library(kableExtra)
library(reshape2)
library(ggplot2)
library(wordcloud)
library(pals)
library(SnowballC)
library(flextable)
#import 11,194 text files that were compiled into a df
dei_articles <- read_csv("articles_df.csv")
textdata <- dei_articles |>
select(filename, sentence, year) |>
as.data.frame() %>%
rename(doc_id = filename, text= sentence)
View(dei_articles)
textdata <- dei_articles |>
select(filename, sentence, published_date) |>
as.data.frame() %>%
rename(doc_id = filename, text= sentence)
# load stopwords
english_stopwords <- readLines("https://slcladal.github.io/resources/stopwords_en.txt", encoding = "UTF-8")
# create corpus object
corpus <- Corpus(DataframeSource(textdata))
# Preprocessing chain
processedCorpus <- tm_map(corpus, content_transformer(tolower))
processedCorpus <- tm_map(processedCorpus, removeWords, english_stopwords)
processedCorpus <- tm_map(processedCorpus, removePunctuation, preserve_intra_word_dashes = TRUE)
processedCorpus <- tm_map(processedCorpus, removeNumbers)
processedCorpus <- tm_map(processedCorpus, stemDocument, language = "en")
processedCorpus <- tm_map(processedCorpus, stripWhitespace)
View(processedCorpus)
#DTM: rows correspond to the documents in the corpus. Columns correspond to the terms in the documents. Cells correspond to the weights of the terms. (Girder)
# compute document term matrix with terms >= minimumFrequency
minimumFrequency <- 5
DTM <- DocumentTermMatrix(processedCorpus, control = list(bounds = list(global = c(minimumFrequency, Inf))))
# have a look at the number of documents and terms in the matrix
dim(DTM)
# due to vocabulary pruning, we have empty rows in our DTM
# LDA does not like this. So we remove those docs from the
# DTM and the metadata
sel_idx <- slam::row_sums(DTM) > 0
DTM <- DTM[sel_idx, ]
textdata <- textdata[sel_idx, ]
#5 term minimum[1] 1387 3019
#5 term minimum[1] 308597 10339
View(DTM)
View(textdata)
# append decade information for aggregation
textdata$day <- paste0(substr(textdata$published_date, 0, 3), "0")
View(textdata)
# append decade information for aggregation
textdata$day <- weekdays(as.Date(textdata$published_date, format="%Y-%m-%d"))
#Fact check 9589 articles
sum(articles_day$n)
#Fact check 9589 articles
sum(articles_per_day$n)
#install.packages("formattable")
articles_per_day <- textdata |>
distinct(doc_id, .keep_all=TRUE) |>
count(day) |>
mutate(pct_total= (n/sum(n))) |>
mutate(pct_total= formattable::percent(pct_total)) |>
# mutate(pct_total = round(pct_total, 1)) %>%
arrange(desc(day))
#Fact check 9589 articles
sum(articles_per_day$n)
#install.packages("formattable")
articles_per_day <- textdata |>
distinct(doc_id, .keep_all=TRUE) |>
count(day) |>
mutate(pct_total= (n/sum(n))) |>
mutate(pct_total= formattable::percent(pct_total)) |>
# mutate(pct_total = round(pct_total, 1)) %>%
arrange(desc(day))
install.packages("formattable")
articles_per_day <- textdata |>
distinct(doc_id, .keep_all=TRUE) |>
count(day) |>
mutate(pct_total= (n/sum(n))) |>
mutate(pct_total= formattable::percent(pct_total)) |>
# mutate(pct_total = round(pct_total, 1)) %>%
arrange(desc(day))
library(kableExtra)
articles_per_day %>%
kbl(caption = "Trump DEI Articles Per DAY" (n=9,589, 10/23/2024)", font_size = 30) %>%
kable_classic(full_width = F, html_font = "Cambria") %>%
install.packages("formattable")
articles_per_day <- textdata |>
distinct(doc_id, .keep_all=TRUE) |>
count(day) |>
mutate(pct_total= (n/sum(n))) |>
mutate(pct_total= formattable::percent(pct_total)) |>
# mutate(pct_total = round(pct_total, 1)) %>%
arrange(desc(day))
library(kableExtra)
articles_per_day %>%
kbl(caption = "Trump DEI Articles Per Day (n=9,589, 10/23/2024)", font_size = 30) %>%
kable_classic(full_width = F, html_font = "Cambria") %>%
column_spec(1, bold = T, border_right = T) %>%
column_spec(2, width = "5em") %>%
column_spec(3, width = "5em", background = "yellow")
#Fact check 9589 articles
sum(articles_per_day$n)
install.packages("formattable")
#Fact check 9589 articles
sum(articles_per_day$n)
install.packages("formattable")
articles_per_day <- textdata |>
distinct(doc_id, .keep_all=TRUE) |>
count(day) |>
mutate(pct_total= (n/sum(n))) |>
mutate(pct_total= formattable::percent(pct_total)) |>
# mutate(pct_total = round(pct_total, 1)) %>%
arrange(desc(day))
library(kableExtra)
articles_per_day %>%
kbl(caption = "Trump DEI Articles Per Day (n=1916, 10/23/2024)", font_size = 30) %>%
kable_classic(full_width = F, html_font = "Cambria") %>%
column_spec(1, bold = T, border_right = T) %>%
column_spec(2, width = "5em") %>%
column_spec(3, width = "5em", background = "yellow")
#Fact check 9589 articles
sum(articles_per_day$n)
install.packages("formattable")
# number of topics
# K <- 20
K <- 6
# set random number generator seed
set.seed(9161)
#Latent Dirichlet Allocation, LDA
topicModel2 <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25, alpha = 0.2))
tmResult <- posterior(topicModel2)
theta <- tmResult$topics
beta <- tmResult$terms
topicNames <- apply(terms(topicModel2, 10), 2, paste, collapse = " ")  # reset topicnames
# Step 1: Check dimensions
n_theta <- nrow(theta)
n_textdata <- length(textdata$day)
cat("Number of rows in theta: ", n_theta, "\n")
cat("Number of documents in textdata: ", n_textdata, "\n")
# Check if textdata contains all the documents in theta
common_ids <- intersect(rownames(theta), textdata$doc_id) # Assuming textdata has a 'doc_id' column
# Filter textdata to include only the documents present in theta
textdata_filtered <- textdata[textdata$doc_id %in% common_ids, ]
# Check dimensions after filtering
n_textdata_filtered <- nrow(textdata_filtered)
cat("Number of documents in filtered textdata: ", n_textdata_filtered, "\n")
# Ensure the lengths match now
if (n_theta != n_textdata_filtered) {
stop("The number of rows in 'theta' still does not match the length of 'textdata_filtered$decade'.")
}
# Align rownames of theta with filtered textdata
theta_aligned <- theta[rownames(theta) %in% textdata_filtered$doc_id, ]
# Optional: Verify the order of documents
if (!all(rownames(theta_aligned) == textdata_filtered$doc_id)) {
# If the order doesn't match, reorder one to match the other
textdata_filtered <- textdata_filtered[match(rownames(theta_aligned), textdata_filtered$doc_id), ]
}
# Ensure they are now aligned and can be combined
if (!all(rownames(theta_aligned) == textdata_filtered$doc_id)) {
stop("The document IDs still do not match. Please check the data alignment.")
}
# Step 2: Combine data
topic_data <- data.frame(theta_aligned, decade = textdata_filtered$decade)
# Step 1: Check dimensions
n_theta <- nrow(theta)
n_textdata <- length(textdata$day)
cat("Number of rows in theta: ", n_theta, "\n")
cat("Number of documents in textdata: ", n_textdata, "\n")
# Check if textdata contains all the documents in theta
common_ids <- intersect(rownames(theta), textdata$doc_id) # Assuming textdata has a 'doc_id' column
# Filter textdata to include only the documents present in theta
textdata_filtered <- textdata[textdata$doc_id %in% common_ids, ]
# Check dimensions after filtering
n_textdata_filtered <- nrow(textdata_filtered)
cat("Number of documents in filtered textdata: ", n_textdata_filtered, "\n")
# Ensure the lengths match now
if (n_theta != n_textdata_filtered) {
stop("The number of rows in 'theta' still does not match the length of 'textdata_filtered$decade'.")
}
# Align rownames of theta with filtered textdata
theta_aligned <- theta[rownames(theta) %in% textdata_filtered$doc_id, ]
# Optional: Verify the order of documents
if (!all(rownames(theta_aligned) == textdata_filtered$doc_id)) {
# If the order doesn't match, reorder one to match the other
textdata_filtered <- textdata_filtered[match(rownames(theta_aligned), textdata_filtered$doc_id), ]
}
# Ensure they are now aligned and can be combined
if (!all(rownames(theta_aligned) == textdata_filtered$doc_id)) {
stop("The document IDs still do not match. Please check the data alignment.")
}
# Step 2: Combine data
topic_data <- data.frame(theta_aligned, day = textdata_filtered$day)
# Step 3: Aggregate data
topic_proportion_per_day <- aggregate(. ~ day, data = topic_data, FUN = mean)
# get mean topic proportions per decade
# topic_proportion_per_decade <- aggregate(theta, by = list(decade = textdata$decade), mean)
# set topic names to aggregated columns
colnames(topic_proportion_per_day)[2:(K+1)] <- topicNames
# reshape data frame
vizDataFrame <- melt(topic_proportion_per_day, id.vars = "day")
# #filter out 1960 - one article
vizDataFrame <- vizDataFrame %>%
#filter(!decade==1960)
# Step 1: Check dimensions
n_theta <- nrow(theta)
n_textdata <- length(textdata$day)
cat("Number of rows in theta: ", n_theta, "\n")
cat("Number of documents in textdata: ", n_textdata, "\n")
# Check if textdata contains all the documents in theta
common_ids <- intersect(rownames(theta), textdata$doc_id) # Assuming textdata has a 'doc_id' column
# Filter textdata to include only the documents present in theta
textdata_filtered <- textdata[textdata$doc_id %in% common_ids, ]
# Check dimensions after filtering
n_textdata_filtered <- nrow(textdata_filtered)
cat("Number of documents in filtered textdata: ", n_textdata_filtered, "\n")
# Ensure the lengths match now
if (n_theta != n_textdata_filtered) {
stop("The number of rows in 'theta' still does not match the length of 'textdata_filtered$decade'.")
}
# Align rownames of theta with filtered textdata
theta_aligned <- theta[rownames(theta) %in% textdata_filtered$doc_id, ]
# Optional: Verify the order of documents
if (!all(rownames(theta_aligned) == textdata_filtered$doc_id)) {
# If the order doesn't match, reorder one to match the other
textdata_filtered <- textdata_filtered[match(rownames(theta_aligned), textdata_filtered$doc_id), ]
}
# Ensure they are now aligned and can be combined
if (!all(rownames(theta_aligned) == textdata_filtered$doc_id)) {
stop("The document IDs still do not match. Please check the data alignment.")
}
# Step 2: Combine data
topic_data <- data.frame(theta_aligned, day = textdata_filtered$day)
# Step 3: Aggregate data
topic_proportion_per_day <- aggregate(. ~ day, data = topic_data, FUN = mean)
# get mean topic proportions per decade
# topic_proportion_per_decade <- aggregate(theta, by = list(decade = textdata$decade), mean)
# set topic names to aggregated columns
colnames(topic_proportion_per_day)[2:(K+1)] <- topicNames
# reshape data frame
vizDataFrame <- melt(topic_proportion_per_day, id.vars = "day")
# #filter out 1960 - one article
vizDataFrame <- vizDataFrame
#filter(!decade==1960)
View(vizDataFrame)
#enframe(): Converts a named list into a dataframe.
topics <- enframe(topicNames, name = "number", value = "text") %>%
unnest(cols = c(text))
topics
