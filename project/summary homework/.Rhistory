}
}
# 应用函数到 "Published date" 列
df$Published_date_final <- sapply(df$`Published date`, convert_to_date)
# 检查转换结果（可选）
head(df[, c("Published date", "Published_date_final")])
# 导出转换后的数据到新的 Excel 文件
write_xlsx(df, "combined_results_list_dates_final.xlsx")
#data cleaning
# 加载包
library(readxl)
library(lubridate)
library(writexl)
# 导入 Excel 文件（注意替换文件名为你的文件名）
df <- read_excel("Results list us.xlsx")
#data cleaning
# 加载包
library(readxl)
library(lubridate)
library(writexl)
# 导入 Excel 文件（注意替换文件名为你的文件名）
df <- read_excel("Results list us.xlsx")
View(df)
#data cleaning
# 加载包
library(readxl)
library(lubridate)
library(writexl)
# 导入 Excel 文件（注意替换文件名为你的文件名）
df <- read_excel("Results list us.xlsx")
# 定义一个函数，将各种格式的日期转换为 Date 类型
convert_to_date <- function(x) {
if (inherits(x, "Date")) {
# 如果已经是 Date 类型，直接返回
return(x)
} else if (inherits(x, "POSIXct") || inherits(x, "POSIXt")) {
# 如果是 POSIXct/POSIXt 类型，转换为 Date
return(as.Date(x))
} else if (is.character(x)) {
# 对字符型的日期，尝试多种格式解析
d <- parse_date_time(x, orders = c("ymd", "B d, Y", "b d, Y", "m/d/Y", "d-b-Y"), exact = FALSE)
return(as.Date(d))
} else {
# 否则返回 NA
return(NA)
}
}
# 应用函数到 "Published date" 列
df$Published_date_final <- sapply(df$Published date, convert_to_date)
# 导入 Excel 文件（注意替换文件名为你的文件名）
df <- read_excel("Results list us.xlsx")
clean_names(df)
#data cleaning
# 加载包
library(readxl)
library(lubridate)
library(writexl)
# 导入 Excel 文件（注意替换文件名为你的文件名）
df <- read_excel("Results list us.xlsx")
clean_names(df)
# 定义一个函数，将各种格式的日期转换为 Date 类型
convert_to_date <- function(x) {
if (inherits(x, "Date")) {
# 如果已经是 Date 类型，直接返回
return(x)
} else if (inherits(x, "POSIXct") || inherits(x, "POSIXt")) {
# 如果是 POSIXct/POSIXt 类型，转换为 Date
return(as.Date(x))
} else if (is.character(x)) {
# 对字符型的日期，尝试多种格式解析
d <- parse_date_time(x, orders = c("ymd", "B d, Y", "b d, Y", "m/d/Y", "d-b-Y"), exact = FALSE)
return(as.Date(d))
} else {
# 否则返回 NA
return(NA)
}
}
# 应用函数到 "Published date" 列
df$Published_date_final <- sapply(df$published_date, convert_to_date)
View(df)
df <- read_excel("Results list us.xlsx")
clean_names(df)
View(df)
library(readxl)
Results_list_us <- read_excel("Results list us.xlsx",
col_types = c("text", "date", "text",
"text", "text", "numeric", "text",
"numeric", "numeric", "text", "numeric",
"text", "text", "text", "text", "text",
"text", "numeric", "numeric", "numeric",
"numeric"))
View(Results_list_us)
View(Results_list_us)
df_clean <- Results_list_us %>%
filter(!is.na(`Published date`))
library(tidyverse)
library(janitor)
library(striprtf)
library(dplyr)
library(readxl)
library(lubridate)
library(writexl)
setwd('D:/comp_text_class/project/dataset/summary homework')
df_clean <- Results_list_us %>%
filter(!is.na(`Published date`))
# 查看前几行，确认筛选结果
head(df_clean)
library(tidyverse)
library(janitor)
library(striprtf)
library(dplyr)
library(readxl)
library(lubridate)
library(writexl)
setwd('D:/comp_text_class/project/dataset/summary homework')
#data cleaning
library(readxl)
vasus <- read_excel("Results list us.xlsx",
col_types = c("text", "date", "text",
"text", "text", "numeric", "text",
"numeric", "numeric", "text", "numeric",
"text", "text", "text", "text", "text",
"text", "numeric", "numeric", "numeric",
"numeric"))
View(vasus)
library(readr)
CoverageoverTime_vasectomyvasectomies <- read_csv("us_charts/CoverageoverTime_vasectomyvasectomies.csv")
View(CoverageoverTime_vasectomyvasectomies)
library(readr)
Coverus <- read_csv("us_charts/CoverageoverTime_vasectomyvasectomies.csv")
View(Coverus)
Coverus <- Coverus %>%
mutate(`Coverage over Time` = as.Date(time))
library(readr)
Coverus <- read_csv("us_charts/CoverageoverTime_vasectomyvasectomies.csv")
Coverus <- Coverus %>%
mutate(`Coverage over Time` = as.Date(covertime))
Coverus <- read_csv("us_charts/CoverageoverTime_vasectomyvasectomies.csv")
Coverus <- janitor::clean_names(Coverus)
library(readr)
Coverus <- read_csv("us_charts/CoverageoverTime_vasectomyvasectomies.csv")
Coverus <- janitor::clean_names(Coverus)
Coverus <- Coverus %>%
mutate(covertime = as.Date(coverage_over_time))
library(readr)
Coverus <- read_csv("us_charts/CoverageoverTime_vasectomyvasectomies.csv")
Coverus <- janitor::clean_names(Coverus)
Coverus <- Coverus %>%
mutate(covertime = as.Date(coverage_over_time))
Coverus_year <- Coverus %>%
mutate(year = year(covertime)) %>%
group_by(year) %>%
summarise(count = n()) %>%
ungroup()
print(data_year)
library(readr)
Coverus <- read_csv("us_charts/CoverageoverTime_vasectomyvasectomies.csv")
Coverus <- janitor::clean_names(Coverus)
Coverus <- Coverus %>%
mutate(covertime = as.Date(coverage_over_time))
Coverus_year <- Coverus %>%
mutate(year = year(covertime)) %>%
group_by(year) %>%
summarise(count = n()) %>%
ungroup()
print(Coverus_year)
# 5. 绘制时间序列图
p <- ggplot(Coverus_year, aes(x = year, y = count)) +
geom_line(color = "blue", size = 1) +
geom_point(color = "red", size = 2) +
labs(title = "Coverage of Vasectomy/Vasectomies Over Time",
x = "Year",
y = "Number of Articles") +
theme_minimal()
p
View(Coverus_year)
library(readr)
Coverus <- read_csv("us_charts/CoverageoverTime_vasectomyvasectomies.csv")
Coverus <- janitor::clean_names(Coverus)
Coverus <- Coverus %>%
mutate(covertime = as.Date(coverage_over_time))
# 5. 绘制时间序列图
p <- ggplot(Coverus, aes(x = covertime, y = coverage)) +
geom_line(color = "blue", size = 1) +
geom_point(color = "red", size = 2) +
labs(title = "Coverage of Vasectomy/Vasectomies Over Time",
x = "Year",
y = "Number of Articles") +
theme_minimal()
p
top_pub <- read_csv("TopPublications_vasectomyvasectomies.csv")
top_pub <- read_csv("us_charts/TopPublications_vasectomyvasectomies.csv")
View(top_pub)
top_pub <- read_csv("us_charts/TopPublications_vasectomyvasectomies.csv")
ggplot(top_pub, aes(x = reorder(Publication), y = Coverage)) +
geom_col(fill = "steelblue") +
labs(title = "Top Publications for Vasectomy/Vasectomies",
x = "Publication",
y = "Count") +
coord_flip()
top_pub <- read_csv("us_charts/TopPublications_vasectomyvasectomies.csv")
ggplot(top_pub, aes(x = reorder(Publication, Coverage), y = Coverage)) +
geom_col(fill = "steelblue") +
labs(title = "Top Publications for Vasectomy/Vasectomies",
x = "Publication",
y = "Count") +
coord_flip()
theme_minimal()
#data cleaning
library(readxl)
vasus <- read_excel("Results list us.xlsx",
col_types = c("text", "date", "text",
"text", "text", "numeric", "text",
"numeric", "numeric", "text", "numeric",
"text", "text", "text", "text", "text",
"text", "numeric", "numeric", "numeric",
"numeric"))
#This cleans column names
vasus <- janitor::clean_names(vasus)
#This processes dates for analysis
vasus$year <- year(vasus$publised_date)
View(vasus)
#This processes dates for analysis
vasus$year <- year(vasus$published_date)
#data importing
library(readxl)
vasus <- read_excel("Results list us.xlsx",
col_types = c("text", "date", "text",
"text", "text", "numeric", "text",
"numeric", "numeric", "text", "numeric",
"text", "text", "text", "text", "text",
"text", "numeric", "numeric", "numeric",
"numeric"))
#This cleans column names
vasus <- janitor::clean_names(vasus)
#Using code, describe the number of rows and columns of the dataset
nrow(vasus)
ncol(vasus)
print(paste0("The number of rows in the dataset is ", nrow(vas), " and the number of columns in the dataset is ", ncol(vas)))
#data importing
library(readxl)
vasus <- read_excel("Results list us.xlsx",
col_types = c("text", "date", "text",
"text", "text", "numeric", "text",
"numeric", "numeric", "text", "numeric",
"text", "text", "text", "text", "text",
"text", "numeric", "numeric", "numeric",
"numeric"))
#This cleans column names
vasus <- janitor::clean_names(vasus)
#Using code, describe the number of rows and columns of the dataset
nrow(vasus)
ncol(vasus)
print(paste0("The number of rows in the dataset is ", nrow(vasus), " and the number of columns in the dataset is ", ncol(vasus)))
#data importing
library(readxl)
vasus <- read_excel("Results list us.xlsx",
col_types = c("text", "date", "text",
"text", "text", "numeric", "text",
"numeric", "numeric", "text", "numeric",
"text", "text", "text", "text", "text",
"text", "numeric", "numeric", "numeric",
"numeric"))
#This cleans column names
vasus <- janitor::clean_names(vasus)
#Using code, describe the number of rows and columns of the dataset
nrow(vasus)
ncol(vasus)
print(paste0("The number of rows in the dataset is ", nrow(vasus), " and the number of columns in the dataset is ", ncol(vasus)))
#This processes year for analysis
vasus$year <- year(vasus$published_date)
vas_time <- vasus %>%
mutate(year = year(published_date)) %>%
group_by(year) %>%
summarise(count = n()) %>%
ungroup()
p <- ggplot(df_time, aes(x = year, y = count)) +
geom_bar(stat = "identity", fill = "steelblue") +
labs(title = "Distribution of Articles Over Time",
x = "Year",
y = "Number of Articles") +
theme_minimal()
vas_time <- vasus %>%
mutate(year = year(published_date)) %>%
group_by(year) %>%
summarise(count = n()) %>%
ungroup()
p <- ggplot(vas_time, aes(x = year, y = count)) +
geom_bar(stat = "identity", fill = "steelblue") +
labs(title = "Distribution of Articles Over Time",
x = "Year",
y = "Number of Articles") +
theme_minimal()
print(p)
bigrams <- vasus %>%
select(hlead) %>%
unnest_tokens(bigram, hlead, token = "ngrams", n = 2)
library(tidyverse)
library(janitor)
library(striprtf)
library(dplyr)
library(readxl)
library(lubridate)
library(writexl)
library(tidytext)
library(stringr)
setwd('D:/comp_text_class/project/dataset/summary homework')
library(readr)
Coverus <- read_csv("us_charts/CoverageoverTime_vasectomyvasectomies.csv")
Coverus <- janitor::clean_names(Coverus)
Coverus <- Coverus %>%
mutate(covertime = as.Date(coverage_over_time))
p <- ggplot(Coverus, aes(x = covertime, y = coverage)) +
geom_line(color = "blue", size = 1) +
geom_point(color = "red", size = 2) +
labs(title = "Coverage of Vasectomy/Vasectomies Over Time",
x = "Year",
y = "Number of Articles") +
theme_minimal()
p
top_pub <- read_csv("us_charts/TopPublications_vasectomyvasectomies.csv")
ggplot(top_pub, aes(x = reorder(Publication, Coverage), y = Coverage)) +
geom_col(fill = "steelblue") +
labs(title = "Top Publications for Vasectomy/Vasectomies",
x = "Publication",
y = "Count") +
coord_flip()
theme_minimal()
#data importing
library(readxl)
vasus <- read_excel("Results list us.xlsx",
col_types = c("text", "date", "text",
"text", "text", "numeric", "text",
"numeric", "numeric", "text", "numeric",
"text", "text", "text", "text", "text",
"text", "numeric", "numeric", "numeric",
"numeric"))
#This cleans column names
vasus <- janitor::clean_names(vasus)
#Using code, describe the number of rows and columns of the dataset
nrow(vasus)
ncol(vasus)
print(paste0("The number of rows in the dataset is ", nrow(vasus), " and the number of columns in the dataset is ", ncol(vasus)))
vas_time <- vasus %>%
mutate(year = year(published_date)) %>%
group_by(year) %>%
summarise(count = n()) %>%
ungroup()
p <- ggplot(vas_time, aes(x = year, y = count)) +
geom_bar(stat = "identity", fill = "steelblue") +
labs(title = "Distribution of Articles Over Time",
x = "Year",
y = "Number of Articles") +
theme_minimal()
print(p)
bigrams <- vasus %>%
select(hlead) %>%
unnest_tokens(bigram, hlead, token = "ngrams", n = 2)
top20_bigrams <- bigrams %>%
count(bigram, sort = TRUE) %>%
top_n(20, n)
print(top20_bigrams)
#Tokenize the hlead column
vas_hlead <- str_replace_all(vas$hlead, "- ", "")
#Tokenize the hlead column
vas_hlead <- str_replace_all(vasus$hlead, "- ", "")
vas_hlead_df <- tibble(vas_hlead,)
vas_tokenized <- vas_hlead_df %>%
unnest_tokens(word,vas_hlead)
vas_tokenized
library(rio)
data(stop_words)
vas_tokenized <- vas_tokenized %>%
anti_join(stop_words, by = c("word" = "word")) %>%
filter(word != "temp_file") %>%
filter(word != "stories_corpus") %>%
filter(!grepl('[0-9]', word))
bigrams <- vas_hlead_df %>%
unnest_tokens(bigram, vas_hlead, token="ngrams", n=2)
bigrams
#Filter out stop words.
bigrams1 <- bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ")
bigrams2 <- bigrams1 %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) |>
filter(!is.na(word1)) |>
filter(!is.na(word2))
# Word Count
bigram3 <- bigrams2 %>%
count(word1, word2, sort = TRUE)
bigram3
# word count and top 20 bigram
bigram3 <- bigrams2 %>%
count(word1, word2, sort = TRUE)
top_n(20, n)
# word count and top 20 bigram
bigram3 <- bigrams2 %>%
count(word1, word2, sort = TRUE)%>%
top_n(20, n)
bigram3
library(tidyverse)
library(janitor)
library(striprtf)
# Set the paths for your folders
input_folder <- "./us_files_random_2000"
output_folder <- "./us_files_raw_text"
# Create output folder if it doesn't exist
if (!dir.exists(output_folder)) {
dir.create(output_folder)
}
# Get a list of all .rtf files in the input folder
rtf_files <- list.files(path = input_folder, pattern = "\\.RTF$", full.names = TRUE)
# Convert each .rtf file to .txt
for (file in rtf_files) {
# Extract the file name without extension
file_name <- tools::file_path_sans_ext(basename(file))
# Read the RTF content
rtf_content <- read_rtf(file)
# Create output file path
output_file <- file.path(output_folder, paste0(file_name, ".txt"))
# Write the content to a .txt file
writeLines(rtf_content, output_file)
# Print progress
cat("Converted:", file, "to", output_file, "\n")
}
cat("Conversion complete!\n")
#This creates an index with the file path to the stories. And then it compiles the stories into a dataframe
#####################
# Begin SM Code #####
#####################
###
# List out text files that match pattern .txt, create DF
###
#Adjust thisline for your file name
files <- list.files("./us_files_raw_text", pattern="*.txt") %>%
as.data.frame() |>
rename(filename = 1) |>
#create an matching file name
mutate(index = str_replace_all(filename, ".txt", "")) %>%
mutate(index = tolower(gsub("[[:punct:][:space:]|]", "", index))) |>
mutate(index = tolower(index)) |>
mutate(index = str_sub(index, 1, 25)) |>
distinct(index, .keep_all = TRUE)
#Join the file list to the index
final_data <- rio::import("Results list us.XLSX") |>
clean_names() |>
#create an matching file name
mutate(index = tolower(gsub("[[:punct:][:space:]|]", "", title))) |>
mutate(index = tolower(index)) |>
mutate(index = str_sub(index, 1, 25)) |>
distinct(index, .keep_all = TRUE)
final_data |>
count(title) |>
arrange(desc(n))
dupe_data <- rio::import("Results list us.XLSX") |>
clean_names() |>
#create an matching file name
mutate(index = tolower(gsub("[[:punct:][:space:]|]", "", title))) |>
mutate(index = tolower(index)) |>
mutate(index = str_sub(index, 1, 25))
dupe_data |>
count(title) |>
arrange(desc(n))
final_index <- final_data |>
inner_join(files, c("index")) |>
#you need the actual hard-coded path on this line below to the text
mutate(filepath = paste0("./us_files_raw_text/", filename))
head(final_index)
anti_final_index <- final_data |>
anti_join(files, c("index"))
final_index |>
count(title) |>
arrange(desc(n))
create_article_text <- function(row_value) {
#row_value is the single argument that is passed to the function
# Take each row of the dataframe
temp <- final_index %>%
slice(row_value)
# Store the filename for  use in constructing articles dataframe
temp_filename <- temp$filename
# Create a dataframe by reading in lines of a given textfile
# Add a filename column
articles_df_temp <- read_lines(temp$filepath) %>%
as_tibble() %>%
mutate(filename = temp_filename)
# Bind results to master articles_df
# <<- returns to global environment
articles_df <<- articles_df %>%
bind_rows(articles_df_temp)
}
###
# Create elements needed to run function
###
# Create empty tibble to store results
articles_df <- tibble()
#running once to test
#create_article_text(2)
# Create an array of numbers to loop through, from 1 to the number of rows in our index dataframe
row_values <- 1:nrow(final_index)
###
# Execute function using lapply
# This loops through each row of the dataframe and append results to master file
###
lapply(row_values, create_article_text)
###
# Clean up articles_df and join to index dataframe
###
articles_df <- articles_df %>%
select(filename, sentence=value) %>%
inner_join(final_index)
write.csv(articles_df, "./us_article_df.csv")
