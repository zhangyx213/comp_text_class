"0","#Tokenize the sentence column"
"0","vasus_bigram <- str_replace_all(vascn_sent$sentence, ""- "", """")"
"0","vasus_bigram_df <- tibble(vasus_bigram,)"
"0",""
"0","vasus_tokenized <- vasus_bigram_df %>%"
"0","  unnest_tokens(word,vasus_bigram)"
"0",""
"0","#remove stop words"
"0","data(stop_words)"
"0","vasus_tokenized <- vasus_tokenized %>%"
"0","  anti_join(stop_words, by = c(""word"" = ""word"")) %>%"
"0","  filter(word != ""temp_file"") %>%"
"0","  filter(word != ""stories_corpus"") %>%"
"0","  filter(!grepl('[0-9]', word))"
"0",""
"0","usbigrams <- vasus_bigram_df %>%"
"0","  unnest_tokens(bigram, vasus_bigram, token=""ngrams"", n=2)"
"0",""
"0","#Filter out stop words."
"0",""
"0","usbigrams1 <- usbigrams %>%"
"0","  separate(bigram, c(""word1"", ""word2""), sep = "" "")"
"0",""
"0","usbigrams2 <- usbigrams1 %>%"
"0","  filter(!word1 %in% stop_words$word) %>%"
"0","  filter(!word2 %in% stop_words$word) %>%"
"0","  filter(!is.na(word1), !is.na(word2)) %>%"
"0","  filter(!word1 %in% c(""newspaper"", ""type"",""china"",""south"",""reserved"",""morning"",""copyright"",""type"",""publication"",""language"",""english"",""load"",""date"")) %>%"
"0","  filter(!word2 %in% c(""newspaper"", ""type"",""china"",""south"",""reserved"",""morning"",""copyright"",""type"",""publication"",""language"",""english"",""load"",""date""))"
"0",""
"0","# word count and top 30 bigram"
"0","usbigram3 <- usbigrams2 %>%"
"0","  count(word1, word2, sort = TRUE)%>%"
"0","  top_n(30, n)"
"0",""
"0","usbigram3"
