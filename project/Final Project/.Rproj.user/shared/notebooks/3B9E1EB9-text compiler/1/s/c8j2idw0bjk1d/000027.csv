"0","library(widyr)"
"0","library(tidyverse)"
"0","library(tidytext)"
"0","library(igraph)"
"0","library(ggraph)"
"0","library(tidygraph)"
"0",""
"0","df <- read_csv(""cn_article_df.csv"")"
"2","New names:"
"2","[1mRows: [22m[34m3814[39m [1mColumns: [22m[34m26[39m"
"2","[36m──[39m [1mColumn specification[22m [36m─────────────────────────────────────────────────────────────────────────────────────[39m
[1mDelimiter:[22m "",""
[31mchr[39m  (17): filename, sentence, title, publication_location, publication_4, publication_type_5, section, c...
[32mdbl[39m   (3): ...1, length, word_count
[33mlgl[39m   (5): agg_copyright, pub_copyright, show, term, ticker
[34mdate[39m  (1): published_date"
"2","
[36mℹ[39m Use `spec()` to retrieve the full column specification for this data.
[36mℹ[39m Specify the column types or set `show_col_types = FALSE` to quiet this message."
"0","df_article <- df %>%"
"0","  group_by(filename) %>%"
"0","  summarise(text = paste(sentence, collapse = "" "")) %>%"
"0","  ungroup()"
"0",""
"0","# Step 3: 分词 & 清洗"
"0","data(""stop_words"")"
"0","tokens <- df_article %>%"
"0","  unnest_tokens(word, text) %>%"
"0","  filter(!word %in% stop_words$word) %>%"
"0","  filter(!str_detect(word, ""^[0-9]+$"")) %>%"
"0","  filter(!word %in% c(""temp_file"", ""stories_corpus"", ""newspaper"", ""type"",""china"",""south"",""reserved"",""morning"",""copyright"",""type"",""publication"",""language"",""geographic"",""length"", ""na"",""subject"",""industry"",""document"",""body"",""words"",""classification"",""english"",""load"",""date"", ""byline"",""news"",""post"",""section""))"
"0",""
"0","# Step 4: 计算词对共现（按文章 ID）"
"0","co_words <- tokens %>%"
"0","  pairwise_count(item = word, feature = filename, sort = TRUE, upper = FALSE)"
"0",""
"0","# Step 5: 查看前30个常共现词对"
"0","head(co_words,30)"
