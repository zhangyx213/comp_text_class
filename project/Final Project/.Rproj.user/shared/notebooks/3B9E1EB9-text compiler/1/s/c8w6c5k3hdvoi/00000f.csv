"0","#Tokenize sentences"
"0",""
"0","vas_scent_df <- tibble(sentence = str_replace_all(vascn_sent$sentence, ""- "", """"),"
"0","                       year = vascn_sent$year)"
"0",""
"0","vas_tokenized <- vas_scent_df %>%"
"0","  unnest_tokens(word, sentence)"
"0",""
"0","#remove stop words"
"0",""
"0","data(stop_words)"
"0",""
"0","vas_tokenized <- vas_tokenized %>%"
"0","  anti_join(stop_words, by = c(""word"" = ""word"")) %>%"
"0","  filter(word != ""temp_file"", word != ""stories_corpus"") %>%"
"0","  filter(!grepl(""[0-9]"", word))"
"0",""
