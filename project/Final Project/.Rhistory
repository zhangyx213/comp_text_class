library(quanteda)
library(readtext)
#impotying U.S. data
vasus <- read_excel("Results list us.xlsx",
col_types = c("text", "date", "text",
"text", "text", "numeric", "text",
"numeric", "numeric", "text", "numeric",
"text", "text", "text", "text", "text",
"text", "numeric", "numeric", "numeric",
"numeric"))
vasus <- janitor::clean_names(vasus)
nrow(vasus)
ncol(vasus)
print(paste0("The number of rows in the dataset is ", nrow(vasus), " and the number of columns in the dataset is ", ncol(vasus)))
library(tidyverse)
library(janitor)
library(striprtf)
library(dplyr)
library(readxl)
library(lubridate)
library(writexl)
library(tidytext)
library(stringr)
library(rio)
library(quanteda)
library(readtext)
#impotying U.S. data
vasus <- read_excel("Results list us.xlsx",
col_types = c("text", "date", "text",
"text", "text", "numeric", "text",
"numeric", "numeric", "text", "numeric",
"text", "text", "text", "text", "text",
"text", "numeric", "numeric", "numeric",
"numeric"))
vasus <- janitor::clean_names(vasus)
nrow(vasus)
ncol(vasus)
print(paste0("The number of rows in the dataset is ", nrow(vasus), " and the number of columns in the dataset is ", ncol(vasus)))
#impotying China data
vascn <- read_excel("Results list cn.xlsx",
col_types = c("text", "date", "text",
"text", "text", "numeric", "text",
"numeric", "numeric", "text", "numeric",
"text", "text", "text", "text", "text",
"text", "numeric", "numeric", "numeric",
"numeric"))
vascn <- janitor::clean_names(vascn)
nrow(vascn)
ncol(vascn)
print(paste0("The number of rows in the dataset is ", nrow(vascn), " and the number of columns in the dataset is ", ncol(vascn)))
library(readr)
Coverus <- read_csv("us_charts/CoverageoverTime_vasectomyvasectomies.csv")
Coverus <- janitor::clean_names(Coverus)
Coverus <- Coverus %>%
mutate(covertime = as.Date(coverage_over_time))
p <- ggplot(Coverus, aes(x = covertime, y = coverage)) +
geom_line(color = "blue", size = 1) +
geom_point(color = "red", size = 2) +
labs(title = "Coverage of Vasectomy/Vasectomies Over Time, U.S.",
x = "Year",
y = "Number of Articles",
caption = "Data source: Nexis Uni, accessed on May 3, 2025"
) +
theme_minimal() +
theme(
plot.caption = element_text(hjust = 0, size = 9, face = "italic") )
p
vas_time <- vasus %>%
mutate(year = year(published_date)) %>%
group_by(year) %>%
summarise(count = n()) %>%
ungroup()
p2 <- ggplot(vas_time, aes(x = year, y = count)) +
geom_bar(stat = "identity", fill = "steelblue") +
labs(title = "Distribution of Articles Over Time",
x = "Year",
y = "Number of Articles",
caption = "Data source: Nexis Uni, accessed on May 3, 2025") +
theme_minimal()
print(p2)
Covercn <- read_csv("cn_charts/CoverageoverTime_vasectomyvasectomies.csv")
Covercn <- janitor::clean_names(Covercn)
Covercn <- Covercn %>%
mutate(covertime = as.Date(coverage_over_time))
c <- ggplot(Covercn, aes(x = covertime, y = coverage)) +
geom_line(color = "blue", size = 1) +
geom_point(color = "red", size = 2) +
labs(title = "Coverage of Vasectomy/Vasectomies Over Time, China",
x = "Year",
y = "Number of Articles",
caption = "Data source: Nexis Uni, accessed on May 3, 2025") +
theme_minimal()
c
top_pub <- read_csv("us_charts/TopPublications_vasectomyvasectomies.csv")
ggplot(top_pub, aes(x = reorder(Publication, Coverage), y = Coverage)) +
geom_col(fill = "steelblue") +
labs(title = "Top Publications for Vasectomy/Vasectomies, U.S.",
x = "Publication",
y = "Count",
caption = "Data source: Nexis Uni, accessed on May 3, 2025") +
coord_flip() +
theme_minimal()
top_pubcn <- read_csv("cn_charts/TopPublications_vasectomyvasectomies.csv")
ggplot(top_pubcn, aes(x = reorder(Publication, Coverage), y = Coverage)) +
geom_col(fill = "steelblue") +
labs(title = "Top Publications for Vasectomy/Vasectomies, China",
x = "Publication",
y = "Count",
caption = "Data source: Nexis Uni, accessed on May 3, 2025") +
coord_flip() +
theme_minimal()
library(treemapify)
subus <- read_csv("us_charts/TopSubjectsCovered_vasectomyvasectomies.csv")
ggplot(subus, aes(
area = Coverage,
fill = `Top Subjects Covered`,
label = `Top Subjects Covered`
)) +
geom_treemap() +
geom_treemap_text(
colour = "white",
place = "centre",
grow = TRUE,
reflow = TRUE
) +
labs(title = "Top Subjects Covered, U.S.",
caption = "Data source: Nexis Uni, accessed on May 3, 2025") +
theme_minimal()
subcn <- read_csv("cn_charts/TopSubjectsCovered_vasectomyvasectomies.csv")
ggplot(subcn, aes(
area = Coverage,
fill = `Top Subjects Covered`,
label = `Top Subjects Covered`
)) +
geom_treemap() +
geom_treemap_text(
colour = "white",
place = "centre",
grow = TRUE,
reflow = TRUE
) +
labs(title = "Top Subjects Covered, China",
caption = "Data source: Nexis Uni, accessed on May 3, 2025") +
theme_minimal()
#Tokenize sentences
vas_scent_df <- tibble(sentence = str_replace_all(vasus_sent$sentence, "- ", ""),
year = vasus_sent$year)
#import text files that were compiled into a df
vasus_sent <- read_csv("./us_article_df.csv")
#add a year column
vasus_sent <- vasus_sent %>%
mutate(year = year(published_date))
vascn_sent <- read_csv("./cn_article_df.csv")
#add a year column
vascn_sent <- vascn_sent %>%
mutate(year = year(published_date))
#install.packages("textdata")
nrc_sentiments <- get_sentiments("nrc")
afinn_sentiments <- get_sentiments("afinn")
afinn_sentiment <- vas_tokenized %>%
inner_join(afinn_sentiments, by = "word")
#Tokenize sentences
vas_scent_df <- tibble(sentence = str_replace_all(vasus_sent$sentence, "- ", ""),
year = vasus_sent$year)
vas_tokenized <- vas_scent_df %>%
unnest_tokens(word, sentence)
#remove stop words
data(stop_words)
vas_tokenized <- vas_tokenized %>%
anti_join(stop_words, by = c("word" = "word")) %>%
filter(word != "temp_file", word != "stories_corpus") %>%
filter(!grepl("[0-9]", word))
#install.packages("textdata")
nrc_sentiments <- get_sentiments("nrc")
afinn_sentiments <- get_sentiments("afinn")
afinn_sentiment <- vas_tokenized %>%
inner_join(afinn_sentiments, by = "word")
afinn_yearly <- afinn_sentiment %>%
group_by(year) %>%
summarise(mean_sentiment = mean(value, na.rm = TRUE))
ggplot(afinn_yearly, aes(x = year, y = mean_sentiment, fill = mean_sentiment > 0)) +
geom_col() +
scale_fill_manual(values = c("TRUE" = "firebrick", "FALSE" = "forestgreen"),
labels = c("Negative", "Positive")) +
labs(title = "Average Sentiment of Vasectomy Coverage (AFINN)",
x = "Year",
y = "Mean Sentiment Score",
fill = "Sentiment") +
theme_minimal()
nrc_sentiments %>%
count(sentiment, sort = TRUE)
sentiments_all <- vas_tokenized %>%
inner_join(nrc_sentiments, by = "word")
vas_tokenized %>%
inner_join(nrc_sentiments) %>%
count(word, sort = TRUE)
sentiments_all %>%
count(sentiment, sort = TRUE)
over_sen <-sentiments_all %>%
count(sentiment, sort = TRUE)
ggplot(over_sen, aes(x = reorder(sentiment,n), y = n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
labs(title = "NRC Sentiment Analysis for Vasectomy Reportings, United States",
x = "Sentiment",
y = NULL)
ggplot(over_sen, aes(x = reorder(sentiment,n), y = n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
labs(title = "NRC Sentiment Analysis for Vasectomy Reportings, United States",
x = "Sentiment",
y = NULL)
sentiments_all %>%
count(sentiment, word) %>%
group_by(sentiment) %>%
slice_max(n, n = 10) %>%
ungroup() %>%
mutate(word = reorder_within(word, n, sentiment)) %>%
ggplot(aes(n, word, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
scale_y_reordered() +
labs(x = "Contribution to sentiment",
y = NULL)
library(reshape2)
vas_tokenized %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("forestgreen", "firebrick"), max.words = 100)
#Tokenize the sentence column
vasus_bigram <- str_replace_all(vasus_sent$sentence, "- ", "")
vasus_bigram_df <- tibble(vasus_bigram,)
vasus_tokenized <- vasus_bigram_df %>%
unnest_tokens(word,vasus_bigram)
#remove stop words
data(stop_words)
vasus_tokenized <- vasus_tokenized %>%
anti_join(stop_words, by = c("word" = "word")) %>%
filter(word != "temp_file") %>%
filter(word != "stories_corpus") %>%
filter(!grepl('[0-9]', word))
usbigrams <- vasus_bigram_df %>%
unnest_tokens(bigram, vasus_bigram, token="ngrams", n=2)
#Filter out stop words.
usbigrams1 <- usbigrams %>%
separate(bigram, c("word1", "word2"), sep = " ")
usbigrams2 <- usbigrams1 %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
filter(!is.na(word1), !is.na(word2)) %>%
filter(!str_detect(word1, "^\\d+$")) %>%
filter(!str_detect(word2, "^\\d+$")) %>%
filter(!word1 %in% c("nw", "york","times","ave","reserved","st","load","date")) %>%
filter(!word2 %in% c("nw", "york","times","ave","reserved","st","load","date"))
# word count and top 30 bigram
usbigram3 <- usbigrams2 %>%
count(word1, word2, sort = TRUE)%>%
top_n(30, n)
usbigram3
#Tokenize the hlead column
vasus_bigram <- str_replace_all(vasus$hlead, "- ", "")
vasus_bigram_df <- tibble(vasus_bigram,)
vasus_tokenized <- vasus_bigram_df %>%
unnest_tokens(word,vasus_bigram)
#remove stop words
data(stop_words)
vasus_tokenized <- vasus_tokenized %>%
anti_join(stop_words, by = c("word" = "word")) %>%
filter(word != "temp_file") %>%
filter(word != "stories_corpus") %>%
filter(!grepl('[0-9]', word))
usbigrams <- vasus_bigram_df %>%
unnest_tokens(bigram, vasus_bigram, token="ngrams", n=2)
#Filter out stop words.
usbigrams1 <- usbigrams %>%
separate(bigram, c("word1", "word2"), sep = " ")
usbigrams2 <- usbigrams1 %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
filter(!is.na(word1), !is.na(word2))
# word count and top 20 bigram
usbigram3 <- usbigrams2 %>%
count(word1, word2, sort = TRUE)%>%
top_n(20, n)
usbigram3
supreme_court_mention <- vasus_sent %>%
mutate(mention_sup = str_detect(str_to_lower(sentence), "birth control"))
mention_counts <- supreme_court_mention %>%
group_by(year) %>%
summarise(mentions = sum(mention_sup, na.rm = TRUE)) %>%
arrange(year)
ggplot(mention_counts, aes(x = year, y = mentions)) +
geom_line(color = "steelblue") +
geom_point(color = "red", size = 2) +
labs(title = "Mentions of 'birth control' Over Year",
x = "Year",
y = "Number of Mentions") +
theme_minimal()
supreme_court_mention <- vasus_sent %>%
mutate(mention_sup = str_detect(str_to_lower(sentence), "supreme court"))
mention_counts <- supreme_court_mention %>%
group_by(year) %>%
summarise(mentions = sum(mention_sup, na.rm = TRUE)) %>%
arrange(year)
ggplot(mention_counts, aes(x = year, y = mentions)) +
geom_line(color = "steelblue") +
geom_point(color = "red", size = 2) +
labs(title = "Mentions of 'supreme court' Over Year",
x = "Year",
y = "Number of Mentions") +
theme_minimal()
supreme_court_mention <- vasus_sent %>%
mutate(mention_sup = str_detect(str_to_lower(sentence), "women'?s health"))
mention_counts <- supreme_court_mention %>%
group_by(year) %>%
summarise(mentions = sum(mention_sup, na.rm = TRUE)) %>%
arrange(year)
ggplot(mention_counts, aes(x = year, y = mentions)) +
geom_line(color = "steelblue") +
geom_point(color = "red", size = 2) +
labs(title = "Mentions of 'women's health' Over Year",
x = "Year",
y = "Number of Mentions") +
theme_minimal()
vasuskwic <- vasus_sent %>%
mutate(file_id = as.numeric(factor(title)))
vasuskwic  <- vasuskwic %>%
mutate(unique_file_id = paste0(file_id, "_", row_number()))
index <- vasus %>%
mutate(file_id = as.numeric(factor(title)))
# Now create the corpus with the new unique identifier
kwic_corpus <- corpus(vasuskwic, text_field = "sentence",
docid_field = "unique_file_id")
# Tokenize the corpus：a word per row
vasuskwic_tokens <- tokens(kwic_corpus)
vasuskwic <- vasus_sent %>%
mutate(file_id = as.numeric(factor(title)))
vasuskwic  <- vasuskwic %>%
mutate(unique_file_id = paste0(file_id, "_", row_number()))
index <- vasus %>%
mutate(file_id = as.numeric(factor(title)))
# Now create the corpus with the new unique identifier
kwic_corpus <- corpus(vasuskwic, text_field = "sentence",
docid_field = "unique_file_id")
# Tokenize the corpus：a word per row
vasuskwic_tokens <- tokens(kwic_corpus)
kwic_results <- kwic(
vasuskwic_tokens,
phrase(c("vasectomy","vasectomies","reproductive","male contraception", "male birth","birth control")),
window = 50, valuetype = "regex"
) %>%
as.data.frame()
#Strips out unique file name from article index
kwic_results <- kwic_results |>
mutate(matchrow = str_extract(docname, "^[^_]+[^_]+"))
metadata <- index
# Join KWIC results with metadata
metadata$file_id <- as.character(metadata$file_id)
kwic_with_metadata <- kwic_results %>%
inner_join(metadata, by = c("matchrow"="file_id")) |>
distinct(docname, .keep_all= TRUE) |>
select(matchrow, title, published_date, publication_location, publication, publication_type_1, length, section, word_count, countries, byline, agg_copyright, cite, company, headline, hlead, pub_copyright, show, term, ticker, from, to, pre, keyword, post, pattern, docname)
#Strips out unique file name from article index
kwic_results <- kwic_results |>
mutate(matchrow = str_extract(docname, "^[^_]+[^_]+"))
metadata <- index
# Join KWIC results with metadata
metadata$file_id <- as.character(metadata$file_id)
kwic_with_metadata <- kwic_results %>%
inner_join(metadata, by = c("matchrow"="file_id")) |>
distinct(docname, .keep_all= TRUE) |>
select(matchrow, title, published_date, publication_location, publication, publication_type_1, length, section, word_count, countries, byline, agg_copyright, cite, company, headline, hlead, pub_copyright, show, term, ticker, from, to, pre, keyword, post, pattern, docname)
#Eliminate duplication
kwic_with_metadata_clean  <- kwic_with_metadata  %>%
distinct(docname, pre, keyword, post,published_date, publication_location, publication) %>%
rename('Beginning of Passage' = pre, 'End of Passage' = post, Location = publication_location, Newspaper = publication, Date =  published_date)
kwic_with_metadata_csv <- kwic_with_metadata_clean  %>%
add_column(Code = NA) %>%
add_column(Comments = NA) %>%
select(docname, Newspaper, Date, 'Beginning of Passage', keyword, 'End of Passage', Code, Comments)
#write_csv(kwic_with_metadata_csv, "kwic_with_metadata_us.csv")
library(wordcloud)
kwic_data <- read_csv("kwic_with_metadata_us.csv")
kwic_context <- paste(kwic_data$`Beginning of Passage`, kwic_data$`End of Passage`)
corpus <- Corpus(VectorSource(kwic_context))
corpus_clean <- corpus %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removePunctuation) %>%
tm_map(removeNumbers) %>%
tm_map(removeWords, stopwords("en")) %>%
tm_map(stripWhitespace)
dtm <- TermDocumentMatrix(corpus_clean)
m <- as.matrix(dtm)
word_freq <- sort(rowSums(m), decreasing = TRUE)
word_freq_df <- data.frame(word = names(word_freq), freq = word_freq)
set.seed(123)
wordcloud(words = word_freq_df$word,
freq = word_freq_df$freq,
min.freq = 2,
max.words = 50,
random.order = FALSE,
colors = brewer.pal(8, "Paired"))
library(wordcloud)
kwic_data <- read_csv("kwic_with_metadata_us.csv")
kwic_context <- paste(kwic_data$`Beginning of Passage`, kwic_data$`End of Passage`)
corpus <- Corpus(VectorSource(kwic_context))
corpus_clean <- corpus %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removePunctuation) %>%
tm_map(removeNumbers) %>%
tm_map(removeWords, stopwords("en")) %>%
tm_map(stripWhitespace)
dtm <- TermDocumentMatrix(corpus_clean)
m <- as.matrix(dtm)
word_freq <- sort(rowSums(m), decreasing = TRUE)
word_freq_df <- data.frame(word = names(word_freq), freq = word_freq)
set.seed(123)
wordcloud(words = word_freq_df$word,
freq = word_freq_df$freq,
min.freq = 2,
max.words = 50,
random.order = FALSE,
colors = brewer.pal(8, "Paired"))
#Tokenize the sentence column
vasus_bigram <- str_replace_all(vascn_sent$sentence, "- ", "")
vasus_bigram_df <- tibble(vasus_bigram,)
vasus_tokenized <- vasus_bigram_df %>%
unnest_tokens(word,vasus_bigram)
#remove stop words
data(stop_words)
vasus_tokenized <- vasus_tokenized %>%
anti_join(stop_words, by = c("word" = "word")) %>%
filter(word != "temp_file") %>%
filter(word != "stories_corpus") %>%
filter(!grepl('[0-9]', word))
usbigrams <- vasus_bigram_df %>%
unnest_tokens(bigram, vasus_bigram, token="ngrams", n=2)
#Filter out stop words.
usbigrams1 <- usbigrams %>%
separate(bigram, c("word1", "word2"), sep = " ")
usbigrams2 <- usbigrams1 %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
filter(!is.na(word1), !is.na(word2)) %>%
filter(!word1 %in% c("newspaper", "type","china","south","reserved","morning","copyright","type","publication","language","english","load","date")) %>%
filter(!word2 %in% c("newspaper", "type","china","south","reserved","morning","copyright","type","publication","language","english","load","date"))
# word count and top 30 bigram
usbigram3 <- usbigrams2 %>%
count(word1, word2, sort = TRUE)%>%
top_n(30, n)
usbigram3
#Tokenize sentences
vas_scent_df <- tibble(sentence = str_replace_all(vasus_sent$sentence, "- ", ""),
year = vasus_sent$year)
vas_tokenized <- vas_scent_df %>%
unnest_tokens(word, sentence)
#remove stop words
data(stop_words)
vas_tokenized <- vas_tokenized %>%
anti_join(stop_words, by = c("word" = "word")) %>%
filter(word != "temp_file", word != "stories_corpus") %>%
filter(!grepl("[0-9]", word))
#install.packages("textdata")
nrc_sentiments <- get_sentiments("nrc")
afinn_sentiments <- get_sentiments("afinn")
afinn_sentiment <- vas_tokenized %>%
inner_join(afinn_sentiments, by = "word")
afinn_yearly <- afinn_sentiment %>%
group_by(year) %>%
summarise(mean_sentiment = mean(value, na.rm = TRUE))
ggplot(afinn_yearly, aes(x = year, y = mean_sentiment, fill = mean_sentiment > 0)) +
geom_col() +
scale_fill_manual(values = c("TRUE" = "firebrick", "FALSE" = "forestgreen"),
labels = c("Negative", "Positive")) +
labs(title = "Average Sentiment of Vasectomy Coverage (AFINN)",
x = "Year",
y = "Mean Sentiment Score",
fill = "Sentiment") +
theme_minimal()
nrc_sentiments %>%
count(sentiment, sort = TRUE)
sentiments_all <- vas_tokenized %>%
inner_join(nrc_sentiments, by = "word")
vas_tokenized %>%
inner_join(nrc_sentiments) %>%
count(word, sort = TRUE)
sentiments_all %>%
count(sentiment, sort = TRUE)
over_sen <-sentiments_all %>%
count(sentiment, sort = TRUE)
ggplot(over_sen, aes(x = reorder(sentiment,n), y = n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
labs(title = "NRC Sentiment Analysis for Vasectomy Reportings, United States",
x = "Sentiment",
y = NULL)
sentiments_all %>%
count(sentiment, word) %>%
group_by(sentiment) %>%
slice_max(n, n = 10) %>%
ungroup() %>%
mutate(word = reorder_within(word, n, sentiment)) %>%
ggplot(aes(n, word, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
scale_y_reordered() +
labs(x = "Contribution to sentiment",
y = NULL)
library(reshape2)
vas_tokenized %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("forestgreen", "firebrick"), max.words = 100)
