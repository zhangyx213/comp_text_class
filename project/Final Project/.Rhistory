# set options
options(stringsAsFactors = F)         # no automatic data transformation
options("scipen" = 100, "digits" = 4) # suppress math annotation
# load packages
here::here()
library(tidyverse)
library(tidytext)
library(rio)
library(readtext)
#topic modeling
library(quanteda)
library(tm)
library(topicmodels)
library(lda)
library(ldatuning)
# from tutorial packages
library(DT)
library(knitr)
library(kableExtra)
library(reshape2)
library(ggplot2)
library(wordcloud)
library(pals)
library(SnowballC)
library(flextable)
#import text files that were compiled into a df
vasus_sent <- read_csv("./dataframes/us_article_df.csv")
#add a year column
vasus_sent <- vasus_sent %>%
mutate(year = year(published_date))
vascn_sent <- read_csv("./dataframes/cn_article_df.csv")
#add a year column
vascn_sent <- vascn_sent %>%
mutate(year = year(published_date))
textdata <- vasus_sent %>%
select(filename, sentence, year) %>%
as.data.frame() %>%
rename(doc_id = filename, text= sentence)
# load stopwords
english_stopwords <- readLines("https://slcladal.github.io/resources/stopwords_en.txt", encoding = "UTF-8")
custom_stop_words <- bind_rows(
tibble(word = c("U.S.", "date", "english", "language", "usa", "copyright",
"publication", "reserved", "news", "newspaper","photograph","photo"),
lexicon = rep("custom", 12)),
stop_words
)
# create corpus object
corpus_us <- Corpus(DataframeSource(textdata))
# Preprocessing chain
processedCorpus <- tm_map(corpus_us, content_transformer(tolower))
processedCorpus <- tm_map(processedCorpus, removeWords, english_stopwords)
processedCorpus <- tm_map(processedCorpus, removeWords, custom_stop_words$word)
processedCorpus <- tm_map(processedCorpus, removePunctuation, preserve_intra_word_dashes = TRUE)
processedCorpus <- tm_map(processedCorpus, removeNumbers)
processedCorpus <- tm_map(processedCorpus, stemDocument, language = "en")
processedCorpus <- tm_map(processedCorpus, stripWhitespace)
#DTM: rows correspond to the documents in the corpus. Columns correspond to the terms in the documents. Cells correspond to the weights of the terms. (Girder)
# compute document term matrix with terms >= minimumFrequency
minimumFrequency <- 5
DTM <- DocumentTermMatrix(processedCorpus, control = list(bounds = list(global = c(minimumFrequency, Inf))))
# have a look at the number of documents and terms in the matrix
dim(DTM)
# due to vocabulary pruning, we have empty rows in our DTM
# LDA does not like this. So we remove those docs from the
# DTM and the metadata
sel_idx <- slam::row_sums(DTM) > 0
DTM <- DTM[sel_idx, ]
textdata <- textdata[sel_idx, ]
# number of topics
# K <- 20
K <- 6
# set random number generator seed
set.seed(9161)
#Latent Dirichlet Allocation, LDA
topicModel2 <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25, alpha = 0.2))
tmResult <- posterior(topicModel2)
theta <- tmResult$topics
beta <- tmResult$terms
topicNames <- apply(terms(topicModel2, 10), 2, paste, collapse = " ")  # reset topicnames
# Step 1: Check dimensions
n_theta <- nrow(theta)
n_textdata <- length(textdata$year)
cat("Number of rows in theta: ", n_theta, "\n")
cat("Number of documents in textdata: ", n_textdata, "\n")
# Check if textdata contains all the documents in theta
common_ids <- intersect(rownames(theta), textdata$doc_id) # Assuming textdata has a 'doc_id' column
# Filter textdata to include only the documents present in theta
textdata_filtered <- textdata[textdata$doc_id %in% common_ids, ]
# Check dimensions after filtering
n_textdata_filtered <- nrow(textdata_filtered)
cat("Number of documents in filtered textdata: ", n_textdata_filtered, "\n")
# Ensure the lengths match now
if (n_theta != n_textdata_filtered) {
stop("The number of rows in 'theta' still does not match the length of 'textdata_filtered$year'.")
}
# Align rownames of theta with filtered textdata
theta_aligned <- theta[rownames(theta) %in% textdata_filtered$doc_id, ]
# Optional: Verify the order of documents
if (!all(rownames(theta_aligned) == textdata_filtered$doc_id)) {
# If the order doesn't match, reorder one to match the other
textdata_filtered <- textdata_filtered[match(rownames(theta_aligned), textdata_filtered$doc_id), ]
}
# Ensure they are now aligned and can be combined
if (!all(rownames(theta_aligned) == textdata_filtered$doc_id)) {
stop("The document IDs still do not match. Please check the data alignment.")
}
# Step 2: Combine data
topic_data <- data.frame(theta_aligned, year = textdata_filtered$year)
# Step 3: Aggregate data
topic_proportion_per_year <- aggregate(. ~ year, data = topic_data, FUN = mean)
# get mean topic proportions per decade
# topic_proportion_per_decade <- aggregate(theta, by = list(decade = textdata$decade), mean)
# set topic names to aggregated columns
colnames(topic_proportion_per_year)[2:(K+1)] <- topicNames
# reshape data frame
vizDataFrame <- melt(topic_proportion_per_year, id.vars = "year")
#enframe(): Converts a named list into a dataframe.
topics <- enframe(topicNames, name = "number", value = "text") %>%
unnest(cols = c(text))
topics
theta2 <- as.data.frame(theta)
topic1 <- theta2 %>%
rownames_to_column(var = "file") |>
mutate(file = str_remove(file, "^X"),
line = str_extract(file, "(?<=\\.txt)\\.\\d+")) |>
mutate(file = str_remove(file, "\\.\\d+$")) |>
rename(topic1 = '1') |>
top_n(20, topic1) |>
arrange(desc(topic1)) |>
select(file, line, topic1)
ggplot(vizDataFrame, aes(x=year, y=value, fill=category)) +
geom_bar(stat = "identity") + ylab("proportion") +
scale_fill_manual(values = paste0(alphabet(20), "FF"), name = "year") +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
scale_fill_manual(values=c("red",
"yellow",
"darkblue",
"blue",
"pink",
"orange")) +
labs(title = "Common Narratives in Vasectomy News Coverage 2000-2025",
subtitle = "Six probable topics in U.S. news sample. n=2000",
caption = "Aggregate mean topic proportions per year. Graphic by Catherine Zhang, 05-04-2025")
politics <- theta2 %>%
#renaming for a general topic
rename(politics = '1') %>%
top_n(20, politics) %>%
arrange(desc(politics )) %>%
select(politics )
# Apply rownames_to_column
politics <- tibble::rownames_to_column(politics , "story_id")
politics$story_id <- gsub("X", "", politics$story_id)
head(politics$story_id, 20)
vizDataFrame <- vizDataFrame %>%
mutate(variable = str_to_lower(variable),variable = str_squish(variable)) %>%
mutate(category = case_when(
variable == "abort health women peopl plan care law govern servic countri" ~ "politics",
variable == "women patient contracept vasectomi birth control pregnanc studi health procedur" ~ "gender equality",
variable == "time day children peopl famili life love feel live told" ~ "symbolic meaning/satire",
variable == "univers time york compani john citi michael edit david march" ~ "rhetoric and humours meaning",
variable == "word length right newstex content inform court employe provid law" ~ "law and rights",
variable == "special ave bodi surgeri document fairfax rockvill diseas arlington bethesda" ~ "medical",
TRUE ~ "other"
))
ggplot(vizDataFrame, aes(x=year, y=value, fill=category)) +
geom_bar(stat = "identity") + ylab("proportion") +
scale_fill_manual(values = paste0(alphabet(20), "FF"), name = "year") +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
scale_fill_manual(values=c("red",
"yellow",
"darkblue",
"blue",
"pink",
"orange")) +
labs(title = "Common Narratives in Vasectomy News Coverage 2000-2025",
subtitle = "Six probable topics in U.S. news sample. n=2000",
caption = "Aggregate mean topic proportions per year. Graphic by Catherine Zhang, 05-04-2025")
#ggsave(here::here("./Figure_US_topics_04_04_2025.png"),device = "png",width=9,height=6, dpi=800)
ggplot(vizDataFrame, aes(x=year, y=value, fill=category)) +
geom_bar(stat = "identity") + ylab("proportion") +
scale_fill_manual(values = paste0(alphabet(20), "FF"), name = "year") +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
scale_fill_manual(values=c("red",
"yellow",
"darkblue",
"blue",
"pink",
"orange")) +
labs(title = "Common Narratives in Vasectomy News Coverage 2000-2025",
subtitle = "Six probable topics in U.S. news sample. n=2000",
caption = "Aggregate mean topic proportions per year. Graphic by Catherine Zhang, 05-04-2025")
#ggsave(here::here("./Figure_US_topics_04_04_2025.png"),device = "png",width=9,height=6, dpi=800)
ggplot(vizDataFrame, aes(x=year, y=value, fill=category)) +
geom_bar(stat = "identity") + ylab("proportion") +
scale_fill_manual(values = paste0(alphabet(20), "FF"), name = "year") +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
scale_fill_manual(values=c("red",
"yellow",
"darkblue",
"blue",
"pink",
"orange")) +
labs(title = "Common Narratives in Vasectomy News Coverage 2000-2025",
subtitle = "Six probable topics in U.S. news sample. n=2000",
caption = "Aggregate mean topic proportions per year. Graphic by Catherine Zhang, 05-04-2025")
#ggsave(here::here("./Figure_US_topics_04_04_2025.png"),device = "png",width=9,height=6, dpi=800)
#Tokenize the sentence column
vasus_bigram <- str_replace_all(vasus_sent$sentence, "- ", "")
vasus_bigram_df <- tibble(vasus_bigram,)
vasus_tokenized <- vasus_bigram_df %>%
unnest_tokens(word,vasus_bigram)
#remove stop words
data(stop_words)
vasus_tokenized <- vasus_tokenized %>%
anti_join(stop_words, by = c("word" = "word")) %>%
filter(word != "temp_file") %>%
filter(word != "stories_corpus") %>%
filter(!grepl('[0-9]', word))
usbigrams <- vasus_bigram_df %>%
unnest_tokens(bigram, vasus_bigram, token="ngrams", n=2)
#Filter out stop words.
usbigrams1 <- usbigrams %>%
separate(bigram, c("word1", "word2"), sep = " ")
usbigrams2 <- usbigrams1 %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
filter(!is.na(word1), !is.na(word2)) %>%
filter(!str_detect(word1, "^\\d+$")) %>%
filter(!str_detect(word2, "^\\d+$")) %>%
filter(!word1 %in% c("nw", "york","times","ave","reserved","st","load","date")) %>%
filter(!word2 %in% c("nw", "york","times","ave","reserved","st","load","date"))
# word count and top 30 bigram
usbigram3 <- usbigrams2 %>%
count(word1, word2, sort = TRUE)%>%
top_n(30, n)
usbigram3
#Tokenize the hlead column
vasus_bigram <- str_replace_all(vasus$hlead, "- ", "")
vasus_bigram_df <- tibble(vasus_bigram,)
vasus_tokenized <- vasus_bigram_df %>%
unnest_tokens(word,vasus_bigram)
#remove stop words
data(stop_words)
vasus_tokenized <- vasus_tokenized %>%
anti_join(stop_words, by = c("word" = "word")) %>%
filter(word != "temp_file") %>%
filter(word != "stories_corpus") %>%
filter(!grepl('[0-9]', word))
usbigrams <- vasus_bigram_df %>%
unnest_tokens(bigram, vasus_bigram, token="ngrams", n=2)
#Filter out stop words.
usbigrams1 <- usbigrams %>%
separate(bigram, c("word1", "word2"), sep = " ")
usbigrams2 <- usbigrams1 %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
filter(!is.na(word1), !is.na(word2))
# word count and top 20 bigram
usbigram3 <- usbigrams2 %>%
count(word1, word2, sort = TRUE)%>%
top_n(20, n)
usbigram3
supreme_court_mention <- vasus_sent %>%
mutate(mention_sup = str_detect(str_to_lower(sentence), "birth control"))
mention_counts <- supreme_court_mention %>%
group_by(year) %>%
summarise(mentions = sum(mention_sup, na.rm = TRUE)) %>%
arrange(year)
ggplot(mention_counts, aes(x = year, y = mentions)) +
geom_line(color = "steelblue") +
geom_point(color = "red", size = 2) +
labs(title = "Mentions of 'birth control' Over Year, U.S. (n=2000)",
x = "Year",
y = "Number of Mentions",
caption = "Data source: Nexis Uni, Graphic created by Yuxiao Zhang, 05-03-2025") +
theme_minimal()
supreme_court_mention <- vasus_sent %>%
mutate(mention_sup = str_detect(str_to_lower(sentence), "supreme court"))
mention_counts <- supreme_court_mention %>%
group_by(year) %>%
summarise(mentions = sum(mention_sup, na.rm = TRUE)) %>%
arrange(year)
ggplot(mention_counts, aes(x = year, y = mentions)) +
geom_line(color = "steelblue") +
geom_point(color = "red", size = 2) +
labs(title = "Mentions of 'supreme court' Over Year, U.S. (n=2000)",
x = "Year",
y = "Number of Mentions",
caption = "Data source: Nexis Uni, Graphic created by Yuxiao Zhang, 05-03-2025") +
theme_minimal()
supreme_court_mention <- vasus_sent %>%
mutate(mention_sup = str_detect(str_to_lower(sentence), "women'?s health"))
mention_counts <- supreme_court_mention %>%
group_by(year) %>%
summarise(mentions = sum(mention_sup, na.rm = TRUE)) %>%
arrange(year)
ggplot(mention_counts, aes(x = year, y = mentions)) +
geom_line(color = "steelblue") +
geom_point(color = "red", size = 2) +
labs(title = "Mentions of 'women's health' Over Year, U.S. (n=2000)",
x = "Year",
y = "Number of Mentions",
caption = "Data source: Nexis Uni, Graphic created by Yuxiao Zhang, 05-03-2025") +
theme_minimal()
vasuskwic <- vasus_sent %>%
mutate(file_id = as.numeric(factor(title)))
vasuskwic  <- vasuskwic %>%
mutate(unique_file_id = paste0(file_id, "_", row_number()))
index <- vasus %>%
mutate(file_id = as.numeric(factor(title)))
# Now create the corpus with the new unique identifier
kwic_corpus <- corpus(vasuskwic, text_field = "sentence",
docid_field = "unique_file_id")
# Tokenize the corpus：a word per row
vasuskwic_tokens <- tokens(kwic_corpus)
kwic_results <- kwic(
vasuskwic_tokens,
phrase(c("vasectomy","vasectomies","reproductive","male contraception", "male birth","birth control")),
window = 50, valuetype = "regex"
) %>%
as.data.frame()
#Strips out unique file name from article index
kwic_results <- kwic_results |>
mutate(matchrow = str_extract(docname, "^[^_]+[^_]+"))
metadata <- index
# Join KWIC results with metadata
metadata$file_id <- as.character(metadata$file_id)
kwic_with_metadata <- kwic_results %>%
inner_join(metadata, by = c("matchrow"="file_id")) |>
distinct(docname, .keep_all= TRUE) |>
select(matchrow, title, published_date, publication_location, publication, publication_type_1, length, section, word_count, countries, byline, agg_copyright, cite, company, headline, hlead, pub_copyright, show, term, ticker, from, to, pre, keyword, post, pattern, docname)
#Eliminate duplication
kwic_with_metadata_clean  <- kwic_with_metadata  %>%
distinct(docname, pre, keyword, post,published_date, publication_location, publication) %>%
rename('Beginning of Passage' = pre, 'End of Passage' = post, Location = publication_location, Newspaper = publication, Date =  published_date)
kwic_with_metadata_csv <- kwic_with_metadata_clean  %>%
add_column(Code = NA) %>%
add_column(Comments = NA) %>%
select(docname, Newspaper, Date, 'Beginning of Passage', keyword, 'End of Passage', Code, Comments)
#write_csv(kwic_with_metadata_csv, "kwic_with_metadata_us.csv")
library(wordcloud)
kwic_data <- read_csv("kwic_with_metadata_us.csv")
library(wordcloud)
kwic_data <- read_csv("./dataframes/kwic_with_metadata_us.csv")
kwic_context <- paste(kwic_data$`Beginning of Passage`, kwic_data$`End of Passage`)
corpus <- Corpus(VectorSource(kwic_context))
corpus_clean <- corpus %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removePunctuation) %>%
tm_map(removeNumbers) %>%
tm_map(removeWords, stopwords("en")) %>%
tm_map(stripWhitespace)
dtm <- TermDocumentMatrix(corpus_clean)
m <- as.matrix(dtm)
word_freq <- sort(rowSums(m), decreasing = TRUE)
word_freq_df <- data.frame(word = names(word_freq), freq = word_freq)
set.seed(123)
wordcloud(words = word_freq_df$word,
freq = word_freq_df$freq,
min.freq = 2,
max.words = 50,
random.order = FALSE,
colors = brewer.pal(8, "Paired"))
mtext("Created by Yuxiao Zhang, 05-03-2025; Sample size: n = 2000",
side = 1, line = 4, cex = 0.8, col = "gray40")
#Tokenize the sentence column
vasus_bigram <- str_replace_all(vascn_sent$sentence, "- ", "")
vasus_bigram_df <- tibble(vasus_bigram,)
vasus_tokenized <- vasus_bigram_df %>%
unnest_tokens(word,vasus_bigram)
#remove stop words
data(stop_words)
vasus_tokenized <- vasus_tokenized %>%
anti_join(stop_words, by = c("word" = "word")) %>%
filter(word != "temp_file") %>%
filter(word != "stories_corpus") %>%
filter(!grepl('[0-9]', word))
usbigrams <- vasus_bigram_df %>%
unnest_tokens(bigram, vasus_bigram, token="ngrams", n=2)
#Filter out stop words.
usbigrams1 <- usbigrams %>%
separate(bigram, c("word1", "word2"), sep = " ")
usbigrams2 <- usbigrams1 %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
filter(!is.na(word1), !is.na(word2)) %>%
filter(!word1 %in% c("newspaper", "type","china","south","reserved","morning","copyright","type","publication","language","english","load","date")) %>%
filter(!word2 %in% c("newspaper", "type","china","south","reserved","morning","copyright","type","publication","language","english","load","date"))
# word count and top 30 bigram
usbigram3 <- usbigrams2 %>%
count(word1, word2, sort = TRUE)%>%
top_n(30, n)
usbigram3
#Tokenize sentences
vas_scent_df <- tibble(sentence = str_replace_all(vasus_sent$sentence, "- ", ""),
year = vasus_sent$year)
vas_tokenized <- vas_scent_df %>%
unnest_tokens(word, sentence)
#remove stop words
data(stop_words)
vas_tokenized <- vas_tokenized %>%
anti_join(stop_words, by = c("word" = "word")) %>%
filter(word != "temp_file", word != "stories_corpus") %>%
filter(!grepl("[0-9]", word))
#install.packages("textdata")
nrc_sentiments <- get_sentiments("nrc")
afinn_sentiments <- get_sentiments("afinn")
afinn_sentiment <- vas_tokenized %>%
inner_join(afinn_sentiments, by = "word")
afinn_yearly <- afinn_sentiment %>%
group_by(year) %>%
summarise(mean_sentiment = mean(value, na.rm = TRUE))
ggplot(afinn_yearly, aes(x = year, y = mean_sentiment, fill = mean_sentiment > 0)) +
geom_col() +
scale_fill_manual(values = c("TRUE" = "firebrick", "FALSE" = "forestgreen"),
labels = c("Negative", "Positive")) +
labs(title = "Average Sentiment of Vasectomy Coverage (AFINN), U.S. (n=2000)",
x = "Year",
y = "Mean Sentiment Score",
fill = "Sentiment",
caption = "Data source: Nexis Uni, Graphic created by Yuxiao Zhang, 05-03-2025") +
theme_minimal()
nrc_sentiments %>%
count(sentiment, sort = TRUE)
sentiments_all <- vas_tokenized %>%
inner_join(nrc_sentiments, by = "word")
vas_tokenized %>%
inner_join(nrc_sentiments) %>%
count(word, sort = TRUE)
sentiments_all %>%
count(sentiment, sort = TRUE)
over_sen <-sentiments_all %>%
count(sentiment, sort = TRUE)
nrc_sentiments %>%
count(sentiment, sort = TRUE)
sentiments_all <- vas_tokenized %>%
inner_join(nrc_sentiments, by = "word")
vas_tokenized %>%
inner_join(nrc_sentiments) %>%
count(word, sort = TRUE)
sentiments_all %>%
count(sentiment, sort = TRUE)
over_sen <-sentiments_all %>%
count(sentiment, sort = TRUE)
ggplot(over_sen, aes(x = reorder(sentiment,n), y = n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
labs(title = "NRC Sentiment Analysis for Vasectomy Reportings, U.S. (n=2000)",
x = "Sentiment",
y = NULL,
caption = "Data source: Nexis Uni, Graphic created by Yuxiao Zhang, 05-03-2025")
sentiments_all %>%
count(sentiment, word) %>%
group_by(sentiment) %>%
slice_max(n, n = 10) %>%
ungroup() %>%
mutate(word = reorder_within(word, n, sentiment)) %>%
ggplot(aes(n, word, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
scale_y_reordered() +
labs(x = "Contribution to sentiment, U.S. reporting (n=2000)",
y = NULL,
caption = "Data source: Nexis Uni, Graphic created by Yuxiao Zhang, 05-03-2025")+ theme(axis.text.x = element_text(angle = 45, hjust = 1))
sentiments_all %>%
count(sentiment, word) %>%
group_by(sentiment) %>%
slice_max(n, n = 10) %>%
ungroup() %>%
mutate(word = reorder_within(word, n, sentiment)) %>%
ggplot(aes(n, word, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
scale_y_reordered() +
labs(x = "Contribution to sentiment, U.S. reporting (n=2000)",
y = NULL,
caption = "Data source: Nexis Uni, Graphic created by Yuxiao Zhang, 05-03-2025")+ theme(axis.text.x = element_text(angle = 45, hjust = 1))
library(reshape2)
vas_tokenized %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("forestgreen", "firebrick"), max.words = 100)
mtext("Data source: Nexis Uni | Graphic by Yuxiao Zhang | Date: 2025-05-03 | n = 2000",
side = 1,       # Bottom of plot
line = 4,       # Distance from plot (adjust as needed)
cex = 0.8,      # Font size
col = "gray30")
vizDataFrame <- vizDataFrame %>%
mutate(variable = str_to_lower(variable),variable = str_squish(variable)) %>%
mutate(category = case_when(
variable == "abort health women peopl plan care law govern servic countri" ~ "Politics",
variable == "women patient contracept vasectomi birth control pregnanc studi health procedur" ~ "Gender Equality",
variable == "time day children peopl famili life love feel live told" ~ "Symbolic Meaning/Satire",
variable == "univers time york compani john citi michael edit david march" ~ "Rhetoric and Humorous Meaning",
variable == "word length right newstex content inform court employe provid law" ~ "Law and Rights",
variable == "special ave bodi surgeri document fairfax rockvill diseas arlington bethesda" ~ "Medical",
TRUE ~ "other"
))
ggplot(vizDataFrame, aes(x=year, y=value, fill=category)) +
geom_bar(stat = "identity") + ylab("proportion") +
scale_fill_manual(values = paste0(alphabet(20), "FF"), name = "year") +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
scale_fill_manual(values=c("red",
"yellow",
"darkblue",
"blue",
"pink",
"orange")) +
labs(title = "Common Narratives in Vasectomy News Coverage 2000-2025",
subtitle = "Six probable topics in U.S. news sample. n=2000",
caption = "Aggregate mean topic proportions per year. Graphic by Catherine Zhang, 05-04-2025")
#ggsave(here::here("./Figure_US_topics_04_04_2025.png"),device = "png",width=9,height=6, dpi=800)
