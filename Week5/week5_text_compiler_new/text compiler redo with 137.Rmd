---
title: "text compiler redo with 137"
output: html_document
date: "2025-04-04"
---
```{r}
library(tidyverse)
library(janitor)
library(striprtf)
```


# Reformat .RTF files

```{r}

# Set the paths for your folders
input_folder <- "./major_world_publications(137)" 
output_folder <- "./major_pubs_raw_text"

# Create output folder if it doesn't exist
if (!dir.exists(output_folder)) {
  dir.create(output_folder)
}

# Get a list of all .rtf files in the input folder
rtf_files <- list.files(path = input_folder, pattern = "\\.RTF$", full.names = TRUE)

# Convert each .rtf file to .txt
for (file in rtf_files) {
  # Extract the file name without extension
  file_name <- tools::file_path_sans_ext(basename(file))
  
  # Read the RTF content
  rtf_content <- read_rtf(file)
  
  # Create output file path
  output_file <- file.path(output_folder, paste0(file_name, ".txt"))
  
  # Write the content to a .txt file
  writeLines(rtf_content, output_file)
  
  # Print progress
  cat("Converted:", file, "to", output_file, "\n")
}

cat("Conversion complete!\n")
```

# Raw text compiler

```{r include=FALSE}
#This creates an index with the file path to the stories. And then it compiles the stories into a dataframe
#####################
# Begin SM Code #####
#####################

###
# List out text files that match pattern .txt, create DF
###

#Adjust thisline for your file name
files <- list.files("./major_pubs_raw_text", pattern="*.txt") %>% 
  as.data.frame() |> 
  rename(filename = 1) |> 
  #create an matching file name
  mutate(index = str_replace_all(filename, ".txt", "")) %>%
  mutate(index = tolower(gsub("[[:punct:][:space:]|]", "", index))) |> 
  mutate(index = tolower(index)) |> 
  mutate(index = str_sub(index, 1, 25)) |> 
  distinct(index, .keep_all = TRUE)


#Join the file list to the index

final_data <- rio::import("Results list for_vasectomy_137.XLSX") |> 
  clean_names() |> 
   #create an matching file name
  mutate(index = tolower(gsub("[[:punct:][:space:]|]", "", title))) |> 
  mutate(index = tolower(index)) |> 
  mutate(index = str_sub(index, 1, 25)) |> 
  distinct(index, .keep_all = TRUE)
```

### Check for duplicate entries

```{r}

final_data |> 
  count(title) |> 
  arrange(desc(n))
```

# why did it drop?

```{r}
dupe_data <- rio::import("Results list for_vasectomy_137.XLSX") |> 
  clean_names() |> 
   #create an matching file name
  mutate(index = tolower(gsub("[[:punct:][:space:]|]", "", title))) |> 
  mutate(index = tolower(index)) |> 
  mutate(index = str_sub(index, 1, 25))
```

```{r}
dupe_data |> 
  count(title) |> 
  arrange(desc(n))
```




```{r}

final_index <- final_data |> 
  inner_join(files, c("index")) |> 
#you need the actual hard-coded path on this line below to the text
  mutate(filepath = paste0("./major_pubs_raw_text/", filename))

head(final_index)
```

#Fact Check

```{r}

anti_final_index <- final_data |> 
  anti_join(files, c("index"))

```

#Checking for duplicates

```{r}
final_index |> 
  count(title) |> 
  arrange(desc(n))

```

#Text compiler

```{r}
###
# Define function to loop through each text file 
###

create_article_text <- function(row_value) {
  
  #row_value is the single argument that is passed to the function
  # Take each row of the dataframe
  temp <- final_index %>%
    slice(row_value)
  
  # Store the filename for  use in constructing articles dataframe
  temp_filename <- temp$filename
  
  # Create a dataframe by reading in lines of a given textfile
  # Add a filename column 
  articles_df_temp <- read_lines(temp$filepath) %>%
    as_tibble() %>%
    mutate(filename = temp_filename)
  
  # Bind results to master articles_df
  # <<- returns to global environment
  articles_df <<- articles_df %>%
    bind_rows(articles_df_temp)
}

###
# Create elements needed to run function
###

# Create empty tibble to store results
articles_df <- tibble()

# Create an array of numbers to loop through, from 1 to the number of rows in our index dataframe 
row_values <- 1:nrow(final_index)

###
# Execute function using lapply
# This loops through each row of the dataframe and append results to master file
###

lapply(row_values, create_article_text)

###
# Clean up articles_df and join to index dataframe
###

articles_df <- articles_df %>%
  select(filename, sentence=value) %>%
  inner_join(final_index)


write.csv(articles_df, "C:/Users/cathy/OneDrive/Desktop/jour689/comp_text_class/Week5/week5_text_compiler_new/article_df.csv")

```

Bigram:

#load tidyverse, tidytext, rio, janitor libraries
library(tidyverse)
library(janitor)
library(rio)
library(tidytext)
```

```{r}
#Import spreadsheet using rio::import and clean the names

vas <- rio::import("article_df.csv") 
vas <- vas |> 
  clean_names()

```
# Tokenize the hlead column

Copy the code from the in-class bigrams exercise and tokenize just the hlead column from your dataframe

Hint: you're changing just one variable

```{r}
vas_scent <- str_replace_all(vas$sentence, "- ", "")
vas_scent_df <- tibble(vas_scent,)

vas_tokenized <- vas_scent_df %>%
  unnest_tokens(word,vas_scent)

vas_tokenized


```

#Remove stopwords and count the words
```{r}

data(stop_words)

vas_tokenized <- vas_tokenized %>%
  anti_join(stop_words, by = c("word" = "word")) %>%
  filter(word != "temp_file") %>%
  filter(word != "stories_corpus") %>%
  filter(!grepl('[0-9]', word))

# Word Count

word_ct <- vas_tokenized %>%
  count(word, sort=TRUE)

word_ct
```

# Create Bigrams

```{r}
bigrams <- vas_scent_df %>%
  unnest_tokens(bigram, vas_scent, token="ngrams", n=2)

bigrams

#Filter out stop words.

bigrams1 <- bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams2 <- bigrams1 %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) |>
  filter(!is.na(word1)) |>
  filter(!is.na(word2))

# Word Count
bigram3 <- bigrams2 %>%
  count(word1, word2, sort = TRUE)

bigram3


```
