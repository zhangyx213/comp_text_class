---
title: "week5_text_compiler_assignment"
output: html_document
date: "2025-02-27"
author: Catherine Zhang
---
library(tidyverse)
library(janitor)
library(striprtf)

```

# Reformat .RTF files

```{r}

# Set the paths for your folders
input_folder <- "./Vasectomy_reporting_2020_2024(100)" 
output_folder <- "./Vasectomy_reporting_raw_text"

# Create output folder if it doesn't exist
if (!dir.exists(output_folder)) {
  dir.create(output_folder)
}

# Get a list of all .rtf files in the input folder
rtf_files <- list.files(path = input_folder, pattern = "\\.RTF$", full.names = TRUE)

# Convert each .rtf file to .txt
for (file in rtf_files) {
  # Extract the file name without extension
  file_name <- tools::file_path_sans_ext(basename(file))
  
  # Read the RTF content
  rtf_content <- read_rtf(file)
  
  # Create output file path
  output_file <- file.path(output_folder, paste0(file_name, ".txt"))
  
  # Write the content to a .txt file
  writeLines(rtf_content, output_file)
  
  # Print progress
  cat("Converted:", file, "to", output_file, "\n")
}

cat("Conversion complete!\n")
```

# Raw text compiler

```{r include=FALSE}
#This creates an index with the file path to the stories. And then it compiles the stories into a dataframe
#####################
# Begin SM Code #####
#####################

###
# List out text files that match pattern .txt, create DF
###

#Adjust thisline for your file name
files <- list.files("./Vasectomy_reporting_raw_text", pattern="*.txt") %>% 
  as.data.frame() |> 
  rename(filename = 1) |> 
  #create an matching file name
  mutate(index = str_replace_all(filename, ".txt", "")) %>%
  mutate(index = tolower(gsub("[[:punct:][:space:]|]", "", index))) |> 
  mutate(index = tolower(index)) |> 
  mutate(index = str_sub(index, 1, 25)) |> 
  distinct(index, .keep_all = TRUE)


#Join the file list to the index

final_data <- rio::import("Results list for_vasectomy, vasectomies.XLSX") |> 
  clean_names() |> 
   #create an matching file name
  mutate(index = tolower(gsub("[[:punct:][:space:]|]", "", title))) |> 
  mutate(index = tolower(index)) |> 
  mutate(index = str_sub(index, 1, 25)) |> 
  distinct(index, .keep_all = TRUE)
```

### Check for duplicate entries

```{r}

final_data |> 
  count(title) |> 
  arrange(desc(n))
```

# why did it drop?

```{r}
dupe_data <- rio::import("Results list for_vasectomy, vasectomies.XLSX") |> 
  clean_names() |> 
   #create an matching file name
  mutate(index = tolower(gsub("[[:punct:][:space:]|]", "", title))) |> 
  mutate(index = tolower(index)) |> 
  mutate(index = str_sub(index, 1, 25))
```

```{r}
dupe_data |> 
  count(title) |> 
  arrange(desc(n))
```




```{r}

final_index <- final_data |> 
  inner_join(files, c("index")) |> 
#you need the actual hard-coded path on this line below to the text
  mutate(filepath = paste0("./Vasectomy_reporting_raw_text/", filename))

head(final_index)
```

#Fact Check

```{r}

anti_final_index <- final_data |> 
  anti_join(files, c("index"))

```

#Checking for duplicates

```{r}
final_index |> 
  count(title) |> 
  arrange(desc(n))

```

#Text compiler

```{r}
###
# Define function to loop through each text file 
###

create_article_text <- function(row_value) {
  
  #row_value is the single argument that is passed to the function
  # Take each row of the dataframe
  temp <- final_index %>%
    slice(row_value)
  
  # Store the filename for  use in constructing articles dataframe
  temp_filename <- temp$filename
  
  # Create a dataframe by reading in lines of a given textfile
  # Add a filename column 
  articles_df_temp <- read_lines(temp$filepath) %>%
    as_tibble() %>%
    mutate(filename = temp_filename)
  
  # Bind results to master articles_df
  # <<- returns to global environment
  articles_df <<- articles_df %>%
    bind_rows(articles_df_temp)
}

###
# Create elements needed to run function
###

# Create empty tibble to store results
articles_df <- tibble()

# Create an array of numbers to loop through, from 1 to the number of rows in our index dataframe 
row_values <- 1:nrow(final_index)

###
# Execute function using lapply
# This loops through each row of the dataframe and append results to master file
###

lapply(row_values, create_article_text)

###
# Clean up articles_df and join to index dataframe
###

articles_df <- articles_df %>%
  select(filename, sentence=value) %>%
  inner_join(final_index)


write.csv(articles_df, "C:/Users/cathy/OneDrive/Desktop/jour689/comp_text_class/Week5/Week5_Text_Compiler_assignment/article_df.csv")

```

Bigram:

#load tidyverse, tidytext, rio, janitor libraries
library(tidyverse)
library(janitor)
library(rio)
library(tidytext)
```

```{r}
#Import spreadsheet using rio::import and clean the names

vas <- rio::import("article_df.csv") 
vas <- vas |> 
  clean_names()

```
# Tokenize the hlead column

Copy the code from the in-class bigrams exercise and tokenize just the hlead column from your dataframe

Hint: you're changing just one variable

```{r}
vas_scent <- str_replace_all(vas$sentence, "- ", "")
vas_scent_df <- tibble(vas_scent,)

vas_tokenized <- vas_scent_df %>%
  unnest_tokens(word,vas_scent)

vas_tokenized


```

#Remove stopwords and count the words
```{r}

data(stop_words)

vas_tokenized <- vas_tokenized %>%
  anti_join(stop_words, by = c("word" = "word")) %>%
  filter(word != "temp_file") %>%
  filter(word != "stories_corpus") %>%
  filter(!grepl('[0-9]', word))

# Word Count

word_ct <- vas_tokenized %>%
  count(word, sort=TRUE)

word_ct
```

# Create Bigrams

```{r}
bigrams <- vas_scent_df %>%
  unnest_tokens(bigram, vas_scent, token="ngrams", n=2)

bigrams

#Filter out stop words.

bigrams1 <- bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams2 <- bigrams1 %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) |>
  filter(!is.na(word1)) |>
  filter(!is.na(word2))

# Word Count
bigram3 <- bigrams2 %>%
  count(word1, word2, sort = TRUE)

bigram3


```

# Write a 250-300 word memo describing your findings in the bigrams. Note any words that should be cleaned out. Also note any issues or problems you see.

After cleaning stop words and numerical metadata from the articles, several key patterns emerged in the bigram analysis. Initially, many numbers—indicating publishing dates, weeks, and news services—appeared in the unigram data. While useful for metadata, these numerical elements obscure the actual content and should be excluded in content analysis.

In the cleaned data, common unigrams such as “vasectomy(n=971),” “abortion(n=368),” and “research(n=232)” continue to dominate. Notably, “health” (n=701) and “reproductive” (n=405) rank highly, suggesting that a significant portion of the content is focused on the reproductive health aspects of vasectomies.

When comparing sentence-level bigrams with headline bigrams, the sentence-level analysis revealed numerous “junk words” that offer little analytical value. Examples include “english language” (n=98), “load date” (n=98), and “publication type” (n=98). These phrases should be added to a custom stop word dictionary for future analysis.

Additional notable bigrams relate directly to reproductive health, such as “health care” (n=178), “reproductive health” (n=150), “planned parenthood” (n=117), and “birth control” (n=113). Phrases concerning the technical aspects of vasectomy, like “surgery transplantation” (n=92), also appear. While the phrase “rights reserved” occurs with a frequency of 91, it is still nearly half as frequent as “health care.” 

These findings suggest that, unlike the headlines, the article content tends to emphasize the procedural aspects of vasectomy rather than engaging in narratives on reproductive equality, gender, responsibility, or legal debates. The tone appears more neutral, focusing on explaining the surgery’s processes and techniques, with less emphasis on its political or gender implications. If I intend to conduct an analysis specifically on gender or masculinity narratives within the content, it would be necessary to filter out articles and media frames that do not contribute to that discussion.

Notably, “supreme court” is the most common word pair in the headline analysis, yet it is not as prominent in the article content. This discrepancy may indicate that many articles use the Roe v. Wade overturn as a “hook” to explain what vasectomy surgery is—a typical journalism practice.

These findings further confirm that the raw data may have collected too many news releases or government news blogs rather than news from media organizations. Future improvements could include refining the data collection process to better target traditional media outlets, developing a specialized stop word dictionary, and applying more rigorous content filtering to enhance the analysis.