group_by(filename) %>%
summarize(
# Combine all non-NA sentences
sentence = paste(na.omit(sentence), collapse = " "),
# Keep one value for each of the other columns
# consolidating metadata into single row per article
title                  = first(title),
published_date         = first(published_date),
publication_location   = first(publication_location),
publication_4          = first(publication_4),
publication_type_5     = first(publication_type_5),
length                 = first(length),
section                = first(section),
word_count             = first(word_count),
countries              = first(countries),
byline                 = first(byline),
agg_copyright          = first(agg_copyright),
cite                   = first(cite),
company                = first(company),
headline               = first(headline),
hlead                  = first(hlead),
publication_16         = first(publication_16),
publication_type_17    = first(publication_type_17),
pub_copyright          = first(pub_copyright),
show                   = first(show),
term                   = first(term),
ticker                 = first(ticker),
index                  = first(index),
filepath               = first(filepath),
date                   = first(date),
year                   = first(year),
.groups = "drop"
)
# Remove punctuation and spaces from the sentence column
compiled_df <- compiled_df %>%
mutate(sentence = str_replace_all(sentence, "[[:punct:][:space:]]", "")) |>
mutate(sentence = tolower(sentence))
return(compiled_df)
}
# Apply the function to your dataframe
processed_llm_text <- process_articles(llm_text)
# Prepare data with filenames and content
articles_for_analysis <- processed_llm_text %>%
select(filename, sentence) %>%
mutate(article_data = paste("FILENAME:", filename, "\nCONTENT:", sentence))
# Combine with clear separators
combined_text_with_filenames <- paste(articles_for_analysis$article_data,
collapse = "\n\n---ARTICLE SEPARATOR---\n\n")
# Update system prompt to request the specific format you want
chat <- chat_gemini(
system_prompt = "You are an academic researcher performing context analysis on selected Newsweek articles. Analyze each article and classify it based on these criteria:
1. If an article contains two or more mentions of Roe v. Wade case or surpreme court, classify it as 'politics'
2. If an article contains two or more mentions of gender equality, family planning decisions, or reproductive equality, classify it as 'rep_equal'
3. If an article contains two or more mentions of healthcare, medical surgery, medical process or doctors, classify it as 'medical'
4. If an article has adjectives that criticize vasectomy, classify it as 'vas_critique'. Name the adjectives in the explanations column.
5. If an article meets multiple criteria, assign multiple categories separated by semicolons
6. If an article meets none of the criteria, classify it as 'other'
Provide a brief explanation or example of the material you found that meets the criteria in prompts 1-6.
Return ONLY a CSV-formatted result with exactly three columns:
filename,category, explanation
For example:
filename,category, explanation
article1.txt,medical, three mentions of medical process
article2.txt,politics, two mentions of Roe v. Wade
article3.txt,rep_equal;politics, two mentions of reproductive equality, four of politics
article4.txt,other
No additional text, explanations, or summary counts."
)
# Send to the LLM
response <- chat$chat(combined_text_with_filenames)
process_llm_response_to_df <- function(response) {
# Extract lines
lines <- strsplit(response, "\n")[[1]]
# Remove markdown code block markers if present
lines <- lines[!grepl("^```", lines)]
# Initialize vectors for data
filenames <- c()
categories <- c()
# Flag to track if we're processing data (after header)
header_found <- FALSE
for (line in lines) {
# Skip empty lines
if (trimws(line) == "") next
# Skip row numbers or other artifacts (lines with asterisks)
if (grepl("\\*\\*", line)) next
# Check if this is the header line
if (grepl("filename,category", line, ignore.case = TRUE)) {
header_found <- TRUE
next
}
# Process data lines (only after header is found)
if (header_found) {
parts <- strsplit(line, ",")[[1]]
if (length(parts) >= 2) {
filenames <- c(filenames, parts[1])
categories <- c(categories, paste(parts[2:length(parts)], collapse=","))
}
}
}
# Create and return the dataframe
if (length(filenames) > 0) {
data.frame(
filename = filenames,
category = categories,
stringsAsFactors = FALSE
)
} else {
# Return empty dataframe with correct structure if no data found
data.frame(
filename = character(0),
category = character(0),
stringsAsFactors = FALSE
)
}
}
result_df <- process_llm_response_to_df(response)
result_df
politics <- result_df |>
filter(str_detect(category, "politics")) |>
mutate(
path = if_else(
str_detect(filename, "\\.txt$"),
file.path("Vasectomy_reporting_raw_text", filename),
file.path("Vasectomy_reporting_raw_text", paste0(filename, ".txt"))
)
)|>
mutate(ai_correct = " ",
ai_wrong = " ",
unsure = " ",
notes = " ")
write.csv(politics, "politics_ai_verification.csv")
file_paths1 <- politics$path
politics_list <- lapply(file_paths1, readLines)
#install.packages("ellmer")
library(ellmer)
library(tidyverse)
library(glue)
library(janitor)
library(readr)
library(dplyr)
library(lubridate)
#import Articles
text <- read_csv("article_df.csv") %>%
mutate(
pub_clean = sub(" [A-Za-z]+$", "", published_date),
date     = mdy(pub_clean),       # "%B %d, %Y"
year     = year(date)
) %>%
select(-pub_clean)
llm_text <- text[1:1000, ]
llm_text <- llm_text |>
mutate(sentence = gsub("\n", " ", sentence)) |>
mutate(sentence = gsub("\\s+", " ", sentence))
names(llm_text)
process_articles <- function(df) {
# First group by article identifiers and concatenate sentences
# Using filename as the identifier, but you might need to use other columns
compiled_df <- df %>%
group_by(filename) %>%
summarize(
# Combine all non-NA sentences
sentence = paste(na.omit(sentence), collapse = " "),
# Keep one value for each of the other columns
# consolidating metadata into single row per article
title                  = first(title),
published_date         = first(published_date),
publication_location   = first(publication_location),
publication_4          = first(publication_4),
publication_type_5     = first(publication_type_5),
length                 = first(length),
section                = first(section),
word_count             = first(word_count),
countries              = first(countries),
byline                 = first(byline),
agg_copyright          = first(agg_copyright),
cite                   = first(cite),
company                = first(company),
headline               = first(headline),
hlead                  = first(hlead),
publication_16         = first(publication_16),
publication_type_17    = first(publication_type_17),
pub_copyright          = first(pub_copyright),
show                   = first(show),
term                   = first(term),
ticker                 = first(ticker),
index                  = first(index),
filepath               = first(filepath),
date                   = first(date),
year                   = first(year),
.groups = "drop"
)
# Remove punctuation and spaces from the sentence column
compiled_df <- compiled_df %>%
mutate(sentence = str_replace_all(sentence, "[[:punct:][:space:]]", "")) |>
mutate(sentence = tolower(sentence))
return(compiled_df)
}
# Apply the function to your dataframe
processed_llm_text <- process_articles(llm_text)
# Prepare data with filenames and content
articles_for_analysis <- processed_llm_text %>%
select(filename, sentence) %>%
mutate(article_data = paste("FILENAME:", filename, "\nCONTENT:", sentence))
# Combine with clear separators
combined_text_with_filenames <- paste(articles_for_analysis$article_data,
collapse = "\n\n---ARTICLE SEPARATOR---\n\n")
# Update system prompt to request the specific format you want
chat <- chat_gemini(
system_prompt = "You are an academic researcher performing context analysis on selected Newsweek articles. Analyze each article and classify it based on these criteria:
1. If an article contains two or more mentions of Roe v. Wade case or surpreme court, classify it as 'politics'
2. If an article contains two or more mentions of gender equality, family planning decisions, or reproductive equality, classify it as 'rep_equal'
3. If an article contains two or more mentions of healthcare, medical surgery, medical process or doctors, classify it as 'medical'
4. If an article has adjectives that criticize vasectomy, classify it as 'vas_critique'. Name the adjectives in the explanations column.
5. If an article meets multiple criteria, assign multiple categories separated by semicolons
6. If an article meets none of the criteria, classify it as 'other'
Provide a brief explanation or example of the material you found that meets the criteria in prompts 1-6.
Return ONLY a CSV-formatted result with exactly three columns:
filename,category, explanation
For example:
filename,category, explanation
article1.txt,medical, three mentions of medical process
article2.txt,politics, two mentions of Roe v. Wade
article3.txt,rep_equal;politics, two mentions of reproductive equality, four of politics
article4.txt,other
No additional text, explanations, or summary counts."
)
# Send to the LLM
response <- chat$chat(combined_text_with_filenames)
process_llm_response_to_df <- function(response) {
# Extract lines
lines <- strsplit(response, "\n")[[1]]
# Remove markdown code block markers if present
lines <- lines[!grepl("^```", lines)]
# Initialize vectors for data
filenames <- c()
categories <- c()
# Flag to track if we're processing data (after header)
header_found <- FALSE
for (line in lines) {
# Skip empty lines
if (trimws(line) == "") next
# Skip row numbers or other artifacts (lines with asterisks)
if (grepl("\\*\\*", line)) next
# Check if this is the header line
if (grepl("filename,category", line, ignore.case = TRUE)) {
header_found <- TRUE
next
}
# Process data lines (only after header is found)
if (header_found) {
parts <- strsplit(line, ",")[[1]]
if (length(parts) >= 2) {
filenames <- c(filenames, parts[1])
categories <- c(categories, paste(parts[2:length(parts)], collapse=","))
}
}
}
# Create and return the dataframe
if (length(filenames) > 0) {
data.frame(
filename = filenames,
category = categories,
stringsAsFactors = FALSE
)
} else {
# Return empty dataframe with correct structure if no data found
data.frame(
filename = character(0),
category = character(0),
stringsAsFactors = FALSE
)
}
}
result_df <- process_llm_response_to_df(response)
result_df
politics <- result_df |>
filter(str_detect(category, "politics")) |>
mutate(
path = if_else(
str_detect(filename, "\\.txt$"),
file.path("Vasectomy_reporting_raw_text", filename),
file.path("Vasectomy_reporting_raw_text", paste0(filename, ".txt"))
)
)|>
mutate(ai_correct = " ",
ai_wrong = " ",
unsure = " ",
notes = " ")
write.csv(politics, "politics_ai_verification.csv")
file_paths1 <- politics$path
politics_list <- lapply(file_paths1, readLines)
politics <- result_df |>
filter(str_detect(category, "politics")) |>
mutate(
path = if_else(
str_detect(filename, "\\.txt$"),
file.path("Vasectomy_reporting_raw_text", filename),
file.path("Vasectomy_reporting_raw_text", paste0(filename, ".txt"))
)
)|>
mutate(ai_correct = " ",
ai_wrong = " ",
unsure = " ",
notes = " ")
write.csv(politics, "politics_ai_verification.csv")
file_paths1 <- politics$path
politics_list <- lapply(file_paths1, function(fp) {
if (file.exists(fp)) {
readLines(fp)
} else {
message("skip the non-existing files: ", fp)
NULL
}
})
write_combined_files <- function(file_list, file_paths, output_file) {
# Open connection to output file
con <- file(output_file, "w")
# Loop through each file
for (i in seq_along(file_list)) {
# Extract just the filename from the path
filename <- basename(file_paths[i])
# Write the separator with filename
writeLines(paste0("=== FILE: ", filename, " ==="), con)
# Write the content of the file
writeLines(file_list[[i]], con)
# Add a blank line between files (except after the last file)
if (i < length(file_list)) {
writeLines("", con)
}
}
# Close the connection
close(con)
# Return a message
message(paste("Successfully wrote", length(file_list), "files to", output_file))
}
write_combined_files(dem_list, file_paths1, "combined_politcs_files.txt")
write_combined_files <- function(file_list, file_paths, output_file) {
# Open connection to output file
con <- file(output_file, "w")
# Loop through each file
for (i in seq_along(file_list)) {
# Extract just the filename from the path
filename <- basename(file_paths[i])
# Write the separator with filename
writeLines(paste0("=== FILE: ", filename, " ==="), con)
# Write the content of the file
writeLines(file_list[[i]], con)
# Add a blank line between files (except after the last file)
if (i < length(file_list)) {
writeLines("", con)
}
}
# Close the connection
close(con)
# Return a message
message(paste("Successfully wrote", length(file_list), "files to", output_file))
}
write_combined_files(politics_list, file_paths1, "combined_politcs_files.txt")
#deleted null files
politics_list <- Filter(Negate(is.null), politics_list)
write_combined_files <- function(file_list, file_paths, output_file) {
# Open connection to output file
con <- file(output_file, "w")
# Loop through each file
for (i in seq_along(file_list)) {
# Extract just the filename from the path
filename <- basename(file_paths[i])
# Write the separator with filename
writeLines(paste0("=== FILE: ", filename, " ==="), con)
# Write the content of the file
writeLines(file_list[[i]], con)
# Add a blank line between files (except after the last file)
if (i < length(file_list)) {
writeLines("", con)
}
}
# Close the connection
close(con)
# Return a message
message(paste("Successfully wrote", length(file_list), "files to", output_file))
}
write_combined_files(politics_list, file_paths1, "combined_politcs_files.txt")
tokens <- token_usage()
chat <- chat_gemini()
#Prompt
chat$chat(paste0("How much will it cost me with my token usage so far using gemini 2.0 flash. My input token use is: ", tokens$input, "and my output token use is: ", tokens$output))
chat2 <- chat_gemini(
system_prompt = "You are an academic researcher performing context analysis on selected Newsweek articles. Analyze each article and classify it based on these criteria:
1. If an article contains two or more mentions of Roe v. Wade case or surpreme court, classify it as 'politics'
2. If an article contains two or more mentions of gender equality, family planning decisions, or reproductive equality, classify it as 'rep_equal'
3. If an article contains two or more mentions of healthcare, medical surgery, medical process or doctors, classify it as 'medical'
4. If an article contains two or more mentions of vasectomy or vasectomies and if adjectives modifying vasectoy are critical, classify it as 'vas_critique'
5. If an article meets multiple criteria, assign multiple categories separated by semicolons
6. If an article meets none of the criteria, classify it as 'other'
Return ONLY a CSV-formatted result with exactly THREE (3) columns:
filename, category, explanation
For example:
filename, category, explanation
article1.txt,nixon, nixon mentioned twice
article2.txt,democrat_critique, these words were critical of democrats: xyz
article3.txt,nixon;voter_turnout, nixon mentioned three times voter turnout twice
article4.txt,other
No additional text, explanations, or summary counts."
)
# Send to the LLM
response2 <- chat2$chat(combined_text_with_filenames)
process_llm_response_to_df <- function(response) {
# Extract lines
lines <- strsplit(response, "\n")[[1]]
# Remove markdown code block markers if present
lines <- lines[!grepl("^```", lines)]
# Initialize vectors for data
filenames <- c()
categories <- c()
explanations <- c()
# Flag to track if we're processing data (after header)
header_found <- FALSE
for (line in lines) {
# Skip empty lines
if (trimws(line) == "") next
# Skip row numbers or other artifacts (lines with asterisks)
if (grepl("\\*\\*", line)) next
# Check if this is the header line
if (grepl("filename.*category.*explanation", line, ignore.case = TRUE)) {
header_found <- TRUE
next
}
# Process data lines (only after header is found)
if (header_found || grep("batch.*\\.txt", line)) {
# Parse each row with better handling of commas within explanations
parts <- strsplit(line, ",\\s*", perl = TRUE, fixed = FALSE)[[1]]
if (length(parts) >= 1) {
filename <- parts[1]
category <- ifelse(length(parts) >= 2, parts[2], "")
# Handle the explanation (everything after second comma)
if (length(parts) >= 3) {
explanation <- paste(parts[3:length(parts)], collapse = ", ")
} else {
explanation <- NA
}
filenames <- c(filenames, filename)
categories <- c(categories, category)
explanations <- c(explanations, explanation)
}
}
}
# Create and return the dataframe
if (length(filenames) > 0) {
data.frame(
filename = filenames,
category = categories,
explanation = explanations,
stringsAsFactors = FALSE
)
} else {
# Return empty dataframe with correct structure if no data found
data.frame(
filename = character(0),
category = character(0),
explanation = character(0),
stringsAsFactors = FALSE
)
}
}
result_df2 <- process_llm_response_to_df(response)
vas <- result_df |>
filter(str_detect(category, "vas_critique")) |>
mutate(
path = if_else(
str_detect(filename, "\\.txt$"),
file.path("Vasectomy_reporting_raw_text", filename),
file.path("Vasectomy_reporting_raw_text", paste0(filename, ".txt"))
)
)|>
mutate(ai_correct = " ",
ai_wrong = " ",
unsure = " ",
notes = " ")
write.csv(vas, "vas_critique_ai_verification.csv")
file_paths1 <- vas$path
vas_list <- lapply(file_paths1, function(fp) {
if (file.exists(fp)) {
readLines(fp)
} else {
message("skip the non-existing files: ", fp)
NULL
}
})
View(result_df2)
vas <- result_df2 |>
filter(str_detect(category, "vas_critique")) |>
mutate(
path = if_else(
str_detect(filename, "\\.txt$"),
file.path("Vasectomy_reporting_raw_text", filename),
file.path("Vasectomy_reporting_raw_text", paste0(filename, ".txt"))
)
)|>
mutate(ai_correct = " ",
ai_wrong = " ",
unsure = " ",
notes = " ")
write.csv(vas, "vas_critique_ai_verification.csv")
file_paths1 <- vas$path
vas_list <- lapply(file_paths1, function(fp) {
if (file.exists(fp)) {
readLines(fp)
} else {
message("skip the non-existing files: ", fp)
NULL
}
})
