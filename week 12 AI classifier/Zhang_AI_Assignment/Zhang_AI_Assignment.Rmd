---
title: "AI Classifier"
author: "Catherine Zhang and her unpaid Claude"
date: 04-24-2025
output: pdf_document
---


```{r include=FALSE}
#install.packages("ellmer")
library(ellmer)
library(tidyverse)
library(glue)
library(janitor)
library(readr)
library(dplyr)
library(lubridate)

```

# API Key

```{r echo=FALSE}

#Sys.setenv(GOOGLE_API_KEY = "XXXX")

```

```{r echo = F}
#import Articles
text <- read_csv("article_df.csv") %>%
   mutate(
    pub_clean = sub(" [A-Za-z]+$", "", published_date),
    date     = mdy(pub_clean),       # "%B %d, %Y"
    year     = year(date)
  ) %>%
  select(-pub_clean)
  
```

# Load a very small subset: 10 articles

```{r echo=FALSE}

llm_text <- text[1:1000, ]

llm_text <- llm_text |> 
  mutate(sentence = gsub("\n", " ", sentence)) |> 
  mutate(sentence = gsub("\\s+", " ", sentence))

names(llm_text)

```

# Industrial strength cleaning and processing

--Articles put into a single string of text, no punctuation or spaces

```{r echo=FALSE}
process_articles <- function(df) {
  # First group by article identifiers and concatenate sentences
  # Using filename as the identifier, but you might need to use other columns
  compiled_df <- df %>%
    group_by(filename) %>%
    summarize(
      # Combine all non-NA sentences
      sentence = paste(na.omit(sentence), collapse = " "),
      # Keep one value for each of the other columns
      # consolidating metadata into single row per article
      title                  = first(title),
      published_date         = first(published_date),
      publication_location   = first(publication_location),
      publication_4          = first(publication_4),
      publication_type_5     = first(publication_type_5),
      length                 = first(length),
      section                = first(section),
      word_count             = first(word_count),
      countries              = first(countries),
      byline                 = first(byline),
      agg_copyright          = first(agg_copyright),
      cite                   = first(cite),
      company                = first(company),
      headline               = first(headline),
      hlead                  = first(hlead),
      publication_16         = first(publication_16),
      publication_type_17    = first(publication_type_17),
      pub_copyright          = first(pub_copyright),
      show                   = first(show),
      term                   = first(term),
      ticker                 = first(ticker),
      index                  = first(index),
      filepath               = first(filepath),
      date                   = first(date),
      year                   = first(year),
      .groups = "drop"
    )
  
  # Remove punctuation and spaces from the sentence column
  compiled_df <- compiled_df %>%
    mutate(sentence = str_replace_all(sentence, "[[:punct:][:space:]]", "")) |> 
    mutate(sentence = tolower(sentence))
  
  return(compiled_df)
}

# Apply the function to your dataframe
processed_llm_text <- process_articles(llm_text)

# Prepare data with filenames and content
articles_for_analysis <- processed_llm_text %>%
  select(filename, sentence) %>%
  mutate(article_data = paste("FILENAME:", filename, "\nCONTENT:", sentence))

# Combine with clear separators
combined_text_with_filenames <- paste(articles_for_analysis$article_data, 
                                    collapse = "\n\n---ARTICLE SEPARATOR---\n\n")
```


# AI prompt

```{r echo=FALSE}
# Update system prompt to request the specific format you want
chat <- chat_gemini(
  system_prompt = "You are an academic researcher performing context analysis on selected Newsweek articles. Analyze each article and classify it based on these criteria:
  
  1. If an article contains two or more mentions of Roe v. Wade case or surpreme court, classify it as 'politics'
  2. If an article contains two or more mentions of gender equality, family planning decisions, or reproductive equality, classify it as 'rep_equal'
  3. If an article contains two or more mentions of healthcare, medical surgery, medical process or doctors, classify it as 'medical'
  4. If an article has adjectives that criticize vasectomy, classify it as 'vas_critique'. Name the adjectives in the explanations column.
  5. If an article meets multiple criteria, assign multiple categories separated by semicolons
  6. If an article meets none of the criteria, classify it as 'other'
  
  Provide a brief explanation or example of the material you found that meets the criteria in prompts 1-6. 
  
  Return ONLY a CSV-formatted result with exactly three columns:
  filename,category, explanation
  
  For example:
  filename,category, explanation
  article1.txt,medical, three mentions of medical process
  article2.txt,politics, two mentions of Roe v. Wade
  article3.txt,rep_equal;politics, two mentions of reproductive equality, four of politics
  article4.txt,other
  
  No additional text, explanations, or summary counts."
)

# Send to the LLM
response <- chat$chat(combined_text_with_filenames)
```

#Process first response --We're taking the chat response and putting it into a dataframe

```{r echo=FALSE}
process_llm_response_to_df <- function(response) {
  # Extract lines
  lines <- strsplit(response, "\n")[[1]]
  
  # Remove markdown code block markers if present
  lines <- lines[!grepl("^```", lines)]
  
  # Initialize vectors for data
  filenames <- c()
  categories <- c()
  
  # Flag to track if we're processing data (after header)
  header_found <- FALSE
  
  for (line in lines) {
    # Skip empty lines
    if (trimws(line) == "") next
    
    # Skip row numbers or other artifacts (lines with asterisks)
    if (grepl("\\*\\*", line)) next
    
    # Check if this is the header line
    if (grepl("filename,category", line, ignore.case = TRUE)) {
      header_found <- TRUE
      next
    }
    
    # Process data lines (only after header is found)
    if (header_found) {
      parts <- strsplit(line, ",")[[1]]
      if (length(parts) >= 2) {
        filenames <- c(filenames, parts[1])
        categories <- c(categories, paste(parts[2:length(parts)], collapse=","))
      }
    }
  }
  
  # Create and return the dataframe
  if (length(filenames) > 0) {
    data.frame(
      filename = filenames,
      category = categories,
      stringsAsFactors = FALSE
    )
  } else {
    # Return empty dataframe with correct structure if no data found
    data.frame(
      filename = character(0),
      category = character(0),
      stringsAsFactors = FALSE
    )
  }
}

result_df <- process_llm_response_to_df(response)

result_df

```

# validate results

--politics

```{r echo=FALSE}
politics <- result_df |> 
  filter(str_detect(category, "politics")) |> 
  mutate(
      path = if_else(
      str_detect(filename, "\\.txt$"),
      file.path("Vasectomy_reporting_raw_text", filename),
      file.path("Vasectomy_reporting_raw_text", paste0(filename, ".txt"))
    )
  )|>
  mutate(ai_correct = " ",
         ai_wrong = " ",
         unsure = " ",
         notes = " ")

write.csv(politics, "politics_ai_verification.csv")

file_paths1 <- politics$path
politics_list <- lapply(file_paths1, function(fp) {
  if (file.exists(fp)) {
    readLines(fp)
  } else {
    message("skip the non-existing files: ", fp)
    NULL
  }
})

```

# process the results to a single file, separated by file name

```{r echo=FALSE}

#deleted null files
politics_list <- Filter(Negate(is.null), politics_list)

write_combined_files <- function(file_list, file_paths, output_file) {
  # Open connection to output file
  con <- file(output_file, "w")
  
  # Loop through each file
  for (i in seq_along(file_list)) {
    # Extract just the filename from the path
    filename <- basename(file_paths[i])
    
    # Write the separator with filename
    writeLines(paste0("=== FILE: ", filename, " ==="), con)
    
    # Write the content of the file
    writeLines(file_list[[i]], con)
    
    # Add a blank line between files (except after the last file)
    if (i < length(file_list)) {
      writeLines("", con)
    }
  }
  
  # Close the connection
  close(con)
  
  # Return a message
  message(paste("Successfully wrote", length(file_list), "files to", output_file))
}

write_combined_files(politics_list, file_paths1, "combined_politcs_files.txt")
```

Now, open a Google Sheet, import politics_ai_verification.csv, read the articles and rate the responses

# A better AI prompt

--Spoiler alert. --This prompt seeks to improve on the vas_critique : "If an article contains two or more mentions of vasectomy or vasectomies and if adjectives modifying vasectoy are critical, classify it as 'vas_critique'"

```{r echo=FALSE}
chat2 <- chat_gemini(
  system_prompt = "You are an academic researcher performing context analysis on selected Newsweek articles. Analyze each article and classify it based on these criteria:
  
  1. If an article contains two or more mentions of Roe v. Wade case or surpreme court, classify it as 'politics'
  2. If an article contains two or more mentions of gender equality, family planning decisions, or reproductive equality, classify it as 'rep_equal'
  3. If an article contains two or more mentions of healthcare, medical surgery, medical process or doctors, classify it as 'medical'
  4. If an article contains two or more mentions of vasectomy or vasectomies and if adjectives modifying vasectoy are critical, classify it as 'vas_critique'
  5. If an article meets multiple criteria, assign multiple categories separated by semicolons
  6. If an article meets none of the criteria, classify it as 'other'
  
  Return ONLY a CSV-formatted result with exactly THREE (3) columns:
  filename, category, explanation
  
  For example:
  filename, category, explanation
  article1.txt,nixon, nixon mentioned twice
  article2.txt,democrat_critique, these words were critical of democrats: xyz
  article3.txt,nixon;voter_turnout, nixon mentioned three times voter turnout twice
  article4.txt,other
  
  No additional text, explanations, or summary counts."
)

# Send to the LLM
response2 <- chat2$chat(combined_text_with_filenames)
```

```{r echo=FALSE}
process_llm_response_to_df <- function(response) {
  # Extract lines
  lines <- strsplit(response, "\n")[[1]]
  
  # Remove markdown code block markers if present
  lines <- lines[!grepl("^```", lines)]
  
  # Initialize vectors for data
  filenames <- c()
  categories <- c()
  explanations <- c()
  
  # Flag to track if we're processing data (after header)
  header_found <- FALSE
  
  for (line in lines) {
    # Skip empty lines
    if (trimws(line) == "") next
    
    # Skip row numbers or other artifacts (lines with asterisks)
    if (grepl("\\*\\*", line)) next
    
    # Check if this is the header line
    if (grepl("filename.*category.*explanation", line, ignore.case = TRUE)) {
      header_found <- TRUE
      next
    }
    
    # Process data lines (only after header is found)
    if (header_found || grep("batch.*\\.txt", line)) {
      # Parse each row with better handling of commas within explanations
      parts <- strsplit(line, ",\\s*", perl = TRUE, fixed = FALSE)[[1]]
      
      if (length(parts) >= 1) {
        filename <- parts[1]
        category <- ifelse(length(parts) >= 2, parts[2], "")
        
        # Handle the explanation (everything after second comma)
        if (length(parts) >= 3) {
          explanation <- paste(parts[3:length(parts)], collapse = ", ")
        } else {
          explanation <- NA
        }
        
        filenames <- c(filenames, filename)
        categories <- c(categories, category)
        explanations <- c(explanations, explanation)
      }
    }
  }
  
  # Create and return the dataframe
  if (length(filenames) > 0) {
    data.frame(
      filename = filenames,
      category = categories,
      explanation = explanations,
      stringsAsFactors = FALSE
    )
  } else {
    # Return empty dataframe with correct structure if no data found
    data.frame(
      filename = character(0),
      category = character(0),
      explanation = character(0),
      stringsAsFactors = FALSE
    )
  }
}
result_df2 <- process_llm_response_to_df(response)
```

### VAS Validate

```{r echo=FALSE}
vas <- result_df2 |> 
  filter(str_detect(category, "vas_critique")) |> 
  mutate(
      path = if_else(
      str_detect(filename, "\\.txt$"),
      file.path("Vasectomy_reporting_raw_text", filename),
      file.path("Vasectomy_reporting_raw_text", paste0(filename, ".txt"))
    )
  )|>
  mutate(ai_correct = " ",
         ai_wrong = " ",
         unsure = " ",
         notes = " ")

write.csv(vas, "vas_critique_ai_verification.csv")

file_paths1 <- vas$path
vas_list <- lapply(file_paths1, function(fp) {
  if (file.exists(fp)) {
    readLines(fp)
  } else {
    message("skip the non-existing files: ", fp)
    NULL
  }
})

```

No vas critique found.

### Summarize

Using AI to assist with content analysis is a helpful tool for automating what we usually call "human coding." It can save researchers a lot of time by reading and labeling large numbers of articles quickly and efficiently. This makes it especially useful when working with massive datasets.

However, AI still has clear limitations. It often struggles to pick up on subtle emotions or sentiment in language. For example, when I asked it to find negative adjectives used in vasectomy-related reporting, it returned nothing. This suggests that AI isn't yet as sensitive to tone and nuance as a human reader would be.

When it comes to classifying topics, I think AI can be just as useful as traditional topic modeling. But the difference is that AI is much more of a “black box.” We don’t really know how it decides which topic fits each article. In contrast, topic modeling is built on a transparent and logical process, using statistics that researchers can follow and understand. If we had a better understanding of how AI detects topics—how it breaks down each article and decides which themes are present—it would be much easier to trust and apply it in our research. 

Another issue is that the training data behind most AI models isn’t very transparent either. Since these models are usually trained on mostly U.S.-based and English-language content, and developed by private companies like Google, it’s likely that some built-in bias exists. That could affect how topics are identified or which kinds of content get emphasized.

In short, AI is a powerful tool for coding, but we need more transparency—both in how it works and what it was trained on—if we want to use it responsibly and effectively for content analysis.