library(tidyverse)
#import df created from sequence below
lynch <- read.csv("articles_oct_19.csv")
lynch_decade <- lynch %>%
mutate(decade = case_when(
year < 1870 ~ "pre1870",
year >= 1870 & year <=1879 ~ "1870s",
year >= 1880 & year <=1889 ~ "1880s",
year >= 1890 & year <=1899 ~ "1890s",
year >= 1900 & year <=1909 ~ "1900s",
year >= 1910 & year <=1919 ~ "1910s",
year >= 1920 & year <=1929 ~ "1920s",
year >= 1930 ~ "post1930s"
))
View(lynch)
lynch_decade <- lynch %>%
mutate(decade = case_when(
year < 1870 ~ "pre1870",
year >= 1870 & year <=1879 ~ "1870s",
year >= 1880 & year <=1889 ~ "1880s",
year >= 1890 & year <=1899 ~ "1890s",
year >= 1900 & year <=1909 ~ "1900s",
year >= 1910 & year <=1919 ~ "1910s",
year >= 1920 & year <=1929 ~ "1920s",
year >= 1930 ~ "post1930s"
))
View(lynch_decade)
lynch_decade |>
count(decade)
# lynch_decade %>%
#   count(newspaper_state)
lynch_decade <- lynch_decade %>%
mutate(region=newspaper_state) %>%
mutate(region = case_when(region=="South Carolina" ~ "South",
region=="Texas" ~ "South",
region=="Louisiana" ~ "South",
region=="Tennessee" ~ "South",
region=="Mississippi" ~ "South",
region=="Arkansas" ~ "South",
region=="Alabama" ~ "South",
region=="Georgia" ~ "South",
region=="Virginia" ~ "South",
region=="Florida" ~ "South",
region=="North Carolina" ~ "South",
region=="Maryland" ~ "Border",
region=="Delaware" ~ "Border",
region=="West Virginia" ~ "Border",
region=="Kentucky" ~ "Border",
region=="Missouri" ~ "Border",
region=="Maine" ~ "North",
region=="New York" ~ "North",
region=="New Hampshire" ~ "North",
region=="Vermont" ~ "North",
region=="Massassachusetts" ~ "North",
region=="Connecticut" ~ "North",
region=="Rhode Island" ~ "North",
region=="Pennsylvania" ~ "North",
region=="New Jersey" ~ "North",
region=="Ohio" ~ "North",
region=="Indiana" ~ "North",
region=="Kansas" ~ "North",
region=="Michigan" ~ "North",
region=="Wisconsin" ~ "North",
region=="Minnesota" ~ "North",
region=="Iowa" ~ "North",
region=="California" ~ "North",
region=="Nevada" ~ "North",
region=="Oregon" ~ "North",
region=="Illinois" ~ "North",
region=="Nebraska" ~ "Misc",
region=="Colorado" ~ "Misc",
region=="North Dakota" ~ "Misc",
region=="South Dakota" ~ "Misc",
region=="Montana" ~ "Misc",
region=="Washington" ~ "Misc",
region=="Idaho" ~ "Misc",
region=="Wyoming" ~ "Misc",
region=="Utah" ~ "Misc",
region=="Oklahoma" ~ "Misc",
region=="New Mexico" ~ "Misc",
region=="Arizona" ~ "Misc",
region=="Alaska" ~ "Misc",
region=="Hawaii" ~ "Misc",
region=="District of Columbia" ~ "Misc",
region=="Virgin Islands" ~ "Misc",
TRUE~region))
lynch_decade |>
distinct(filename, .keep_all = TRUE) |>
count(region) |>
ggplot(aes(x = region, y = n, fill=n)) +
ylab("Count of newspapers by region") +
xlab("") +
geom_col() +
labs(title = "Regional Grouping of Newspapers in Lynching Coverage",
subtitle = "Newspapers from Chronicling America, LOC",
caption = "n=6,448 newspapers. Graphic by Rob Wells, 2/27/2025")
lynch_decade |>
distinct(filename, .keep_all = TRUE) |>
count(region)
lynch_decade |>
distinct(filename, .keep_all = TRUE) |>
count(decade)
lynch_decade |>
count(decade)
library(tidytext)
get_sentiments("afinn")
install.packages("textdata")
get_sentiments("afinn")
library(tidytext)
load("../data/afinn.rda")
get_sentiments("bing")
get_sentiments("nrc")
get_sentiments("bing")
load("../data/nrc.rda")
library(janeaustenr)
library(dplyr)
library(stringr)
tidy_books <- austen_books() %>%
group_by(book) %>%
mutate(
linenumber = row_number(),
chapter = cumsum(str_detect(text,
regex("^chapter [\\divxlc]",
ignore_case = TRUE)))) %>%
ungroup() %>%
unnest_tokens(word, text)
View(tidy_books)
nrc_joy <- get_sentiments("nrc") %>%
filter(sentiment == "joy")
tidy_books %>%
filter(book == "Emma") %>%
inner_join(nrc_joy) %>%
count(word, sort = TRUE)
View(nrc_joy)
nrc_joy <- nrc %>%
filter(sentiment == "joy")
tidy_books %>%
filter(book == "Emma") %>%
inner_join(nrc_joy) %>%
count(word, sort = TRUE)
library(tidyr)
jane_austen_sentiment <- tidy_books %>%
inner_join(get_sentiments("bing")) %>%
count(book, index = linenumber %/% 80, sentiment) %>%
pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
mutate(sentiment = positive - negative)
View(jane_austen_sentiment)
library(ggplot2)
ggplot(jane_austen_sentiment, aes(index, sentiment, fill = book)) +
geom_col(show.legend = FALSE) +
facet_wrap(~book, ncol = 2, scales = "free_x")
nrc_joy <- get_sentiments("nrc") %>%
filter(sentiment == "joy")
tidy_books %>%
filter(book == "Emma") %>%
inner_join(nrc_joy) %>%
count(word, sort = TRUE)
pride_prejudice <- tidy_books %>%
filter(book == "Pride & Prejudice")
pride_prejudice
afinn <- pride_prejudice %>%
inner_join(get_sentiments("afinn")) %>%
group_by(index = linenumber %/% 80) %>%
summarise(sentiment = sum(value)) %>%
mutate(method = "AFINN")
bing_and_nrc <- bind_rows(
pride_prejudice %>%
inner_join(get_sentiments("bing")) %>%
mutate(method = "Bing et al."),
pride_prejudice %>%
inner_join(get_sentiments("nrc") %>%
filter(sentiment %in% c("positive",
"negative"))) %>%
mutate(method = "NRC")) %>%
count(method, index = linenumber %/% 80, sentiment) %>%
pivot_wider(names_from = sentiment,
values_from = n,
values_fill = 0) %>%
mutate(sentiment = positive - negative)
View(afinn)
bind_rows(afinn,
bing_and_nrc) %>%
ggplot(aes(index, sentiment, fill = method)) +
geom_col(show.legend = FALSE) +
facet_wrap(~method, ncol = 1, scales = "free_y")
get_sentiments("nrc") %>%
filter(sentiment %in% c("positive", "negative")) %>%
count(sentiment)
nrc %>%
filter(sentiment %in% c("positive", "negative")) %>%
count(sentiment)
get_sentiments("bing") %>%
count(sentiment)
bing_word_counts <- tidy_books %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
ungroup()
bing_word_counts
bing_word_counts %>%
group_by(sentiment) %>%
slice_max(n, n = 10) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(n, word, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(x = "Contribution to sentiment",
y = NULL)
custom_stop_words <- bind_rows(tibble(word = c("miss"),
lexicon = c("custom")),
stop_words)
custom_stop_words
library(wordcloud)
tidy_books %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 100))
library(reshape2)
tidy_books %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("gray20", "gray80"),
max.words = 100)
p_and_p_sentences <- tibble(text = prideprejudice) %>%
unnest_tokens(sentence, text, token = "sentences")
p_and_p_sentences$sentence[2]
p_and_p_sentences$sentence[2]
austen_chapters <- austen_books() %>%
group_by(book) %>%
unnest_tokens(chapter, text, token = "regex",
pattern = "Chapter|CHAPTER [\\dIVXLC]") %>%
ungroup()
austen_chapters %>%
group_by(book) %>%
summarise(chapters = n())
bingnegative <- get_sentiments("bing") %>%
filter(sentiment == "negative")
wordcounts <- tidy_books %>%
group_by(book, chapter) %>%
summarize(words = n())
tidy_books %>%
semi_join(bingnegative) %>%
group_by(book, chapter) %>%
summarize(negativewords = n()) %>%
left_join(wordcounts, by = c("book", "chapter")) %>%
mutate(ratio = negativewords/words) %>%
filter(chapter != 0) %>%
slice_max(ratio, n = 1) %>%
ungroup()
View(afinn)
View(bing_and_nrc)
p_and_p_sentences <- tibble(text = prideprejudice) %>%
unnest_tokens(sentence, text, token = "sentences")
austen_chapters <- austen_books() %>%
group_by(book) %>%
unnest_tokens(chapter, text, token = "regex",
pattern = "Chapter|CHAPTER [\\dIVXLC]") %>%
ungroup()
austen_chapters %>%
group_by(book) %>%
summarise(chapters = n())
View(austen_chapters)
View(tidy_books)
View(wordcounts)
View(wordcounts)
bingnegative <- get_sentiments("bing") %>%
filter(sentiment == "negative")
wordcounts <- tidy_books %>%
group_by(book, chapter) %>%
summarize(words = n())
tidy_books %>%
semi_join(bingnegative) %>%
group_by(book, chapter) %>%
summarize(negativewords = n()) %>%
left_join(wordcounts, by = c("book", "chapter")) %>%
mutate(ratio = negativewords/words) %>%
filter(chapter != 0) %>%
slice_max(ratio, n = 1) %>%
ungroup()
View(tidy_books)
wordcounts <- tidy_books %>%
group_by(book, chapter) %>%
summarize(words = n())
