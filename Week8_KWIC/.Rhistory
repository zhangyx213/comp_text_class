library(quanteda)
library(readtext)
library(tidyverse)
articles <- read.csv("https://raw.githubusercontent.com/wellsdata/CompText_Jour/refs/heads/main/exercises/assets/data/kemi_df2.csv")
View(articles)
# Create a corpus using the 'sentence' column as the text field
my_corpus <- corpus(articles, text_field = "sentence") # build a new corpus from the texts
head(my_corpus)
# Tokenize the corpus
my_tokens <- tokens(my_corpus)
# Perform KWIC (Key Word in Context) search on the tokens
quanteda_test <- kwic(my_tokens, pattern = "journalist", valuetype = "regex") %>%
as.data.frame()
quanteda_test2 <- kwic(my_tokens, phrase(c("journalist", "reporter", "newspaper", "killer", "murdered", "killed", "investigate crimes against")), window = 50, valuetype = "regex") %>% as.data.frame()
#write.csv(quanteda_test, "quanteda_test.csv")
# Tokenize the corpus
my_tokens <- tokens(my_corpus)
# Perform KWIC (Key Word in Context) search on the tokens
quanteda_test <- kwic(my_tokens, pattern = "journalist", valuetype = "regex") %>%
as.data.frame()
quanteda_test2 <- kwic(my_tokens, phrase(c("journalist", "reporter", "newspaper", "killer", "murdered", "killed", "investigate crimes against")), window = 50, valuetype = "regex") %>% as.data.frame()
write.csv(quanteda_test, "quanteda_test.csv")
View(quanteda_test)
View(quanteda_test2)
extracted_articles_index_oct_16_2024 <- read.csv("https://osf.io/download/uxw3a/?view_only=6c106acd6cb54f6f849e8c6f9098809f")
extracted_text_oct_16_2024 <- read_csv("https://osf.io/download/gw5dk/?view_only=6c106acd6cb54f6f849e8c6f9098809f")
lynch <- extracted_text_oct_16_2024 |>
filter(year > 1890 & year < 1895)
View(lynch)
lynch <- lynch %>%
mutate(unique_file_id = paste0(file_id, "_", row_number()))
# Now create the corpus with the new unique identifier
lynch_corpus <- corpus(lynch, text_field = "sentence",
docid_field = "unique_file_id")
# Tokenize the corpus
lynch_tokens <- tokens(lynch_corpus)
kwic_results <- kwic(
lynch_tokens,
phrase(c("mob", "masked men", "mask", "captors", "a party of",
"mob attacked", "mob caught", "overpowered by a mob",
"law into their own hands", "law unto themselves")),
window = 50, valuetype = "regex"
) %>%
as.data.frame()
View(kwic_results)
View(lynch_tokens)
kwic_results <- kwic(
lynch_tokens,
phrase(c("mob", "masked men", "mask", "captors", "a party of",
"mob attacked", "mob caught", "overpowered by a mob",
"law into their own hands", "law unto themselves")),
window = 50, valuetype = "regex"
) %>%
as.data.frame()
View(kwic_results)
#Strips out unique file name from article index
kwic_results <- kwic_results |>
mutate(matchrow = str_extract(docname, "^[^_]+_[^_]+"))
metadata <- extracted_articles_index_oct_16_2024
# Join KWIC results with metadata
kwic_with_metadata <- kwic_results %>%
inner_join(metadata, by = c("matchrow"="file_id")) |>
distinct(docname, .keep_all= TRUE) |>
select(matchrow, date, from, to, pre, keyword, post, pattern, newspaper_name, newspaper_city, newspaper_state,black_press,url, docname)
#cut useless returns
mobjunk <- c("mobile",  "automobile",  "automobiles", "mobilize", "mobley", "mobilizing", "democratic party of", "mobe", "moberly",  "mobil", "mobiles", "mobilized", "mobold")
kwic_with_metadata <- kwic_with_metadata %>%
filter(!keyword %in% mobjunk)
#Eliminate duplication
kwic_with_metadata  <- kwic_with_metadata  %>%
distinct(docname, pre, keyword, post, newspaper_name, date, url) %>%
rename('Beginning of Passage' = pre, 'End of Passage' = post, Newspaper = newspaper_name, Date = date)
kwic_with_metadata_csv <- kwic_with_metadata  %>%
add_column(Code = NA) %>%
add_column(Comments = NA) %>%
select(docname, Newspaper, Date, 'Beginning of Passage', keyword, 'End of Passage', Code, Comments)
#write_csv(kwic_with_metadata_csv, "../exercises/assets/data/kwic_with_metadata.csv")
write_csv(kwic_with_metadata_csv, "../exercises/assets/data/kwic_with_metadata.csv")
write_csv(kwic_with_metadata_csv, "kwic_with_metadata.csv")
#install.packages("readtext")
library(quanteda)
library(readtext)
library(tidyverse)
articles <- read.csv("articles_df.csv")
articles <- read.csv("article_df.csv")
articles <- read.csv("article_df.csv")
# Create a corpus using the 'sentence' column as the text field
my_corpus <- corpus(articles, text_field = "sentence") # build a new corpus from the texts
head(my_corpus)
View(articles)
View(articles)
articles <- articles %>%
mutate(unique_file_id = paste0(file_id, "_", row_number()))
articles <- articles %>%
mutate(unique_file_id = paste0(filename, "_", row_number()))
articles <- articles %>%
mutate(file_id = as.numeric(factor(filename)))
articles <- articles %>%
mutate(unique_file_id = paste0(file_id, "_", row_number()))
# Now create the corpus with the new unique identifier
vas_corpus <- corpus(articles, text_field = "sentence",
docid_field = "unique_file_id")
# Tokenize the corpus：a word per row
vas_tokens <- tokens(vas_corpus)
View(vas_tokens)
kwic_results <- kwic(
vas_tokens,
phrase(c("vasectomy", "vasectomies","procedure", "safe", "medical", "health", "recovery")),
window = 50, valuetype = "regex"
) %>%
as.data.frame()
View(kwic_results)
#Strips out unique file name from article index
kwic_results <- kwic_results |>
mutate(matchrow = str_extract(docname, "^[^_]+_[^_]+"))
#Strips out unique file name from article index
kwic_results <- kwic_results |>
mutate(matchrow = str_extract(docname, "^[^_]+_[^_]+"))
metadata <- articles
# Join KWIC results with metadata
kwic_with_metadata <- kwic_results %>%
inner_join(metadata, by = c("matchrow"="file_id")) |>
distinct(docname, .keep_all= TRUE) |>
select(matchrow, date, from, to, pre, keyword, post, pattern, newspaper_name, newspaper_city, newspaper_state,black_press,url, docname)
metadata$file_id <- as.character(metadata$file_id)
kwic_with_metadata <- kwic_results %>%
inner_join(metadata, by = c("matchrow"="file_id")) |>
distinct(docname, .keep_all= TRUE) |>
select(matchrow, date, from, to, pre, keyword, post, pattern, newspaper_name, newspaper_city, newspaper_state,black_press,url, docname)
kwic_with_metadata <- kwic_results %>%
inner_join(metadata, by = c("matchrow"="file_id")) |>
distinct(docname, .keep_all= TRUE) |>
select(matchrow, published_date, publication_location, publication_type_5,  from, to, pre, keyword, post, pattern, url, docname)
kwic_with_metadata <- kwic_results %>%
inner_join(metadata, by = c("matchrow"="file_id")) |>
distinct(docname, .keep_all= TRUE) |>
select(matchrow, published_date, publication_location, publication_type_5,  from, to, pre, keyword, post, pattern, docname)
metadata_grouped <- metadata %>%
group_by(filename) %>%
summarise(content = paste(sentence, collapse = " "))
View(metadata_grouped)
library(readxl)
Results_list_for_vasectomy_vasectomies <- read_excel("Results list for_vasectomy, vasectomies.XLSX")
View(Results_list_for_vasectomy_vasectomies)
articles <- read.csv("article_df.csv")
library(readxl)
index <- read_excel("Results list for_vasectomy, vasectomies.XLSX")
# Create a corpus using the 'sentence' column as the text field
my_corpus <- corpus(articles, text_field = "sentence") # build a new corpus from the texts
head(my_corpus)
articles <- articles %>%
mutate(file_id = as.numeric(factor(filename)))
articles <- articles %>%
mutate(unique_file_id = paste0(file_id, "_", row_number()))
# Now create the corpus with the new unique identifier
vas_corpus <- corpus(articles, text_field = "sentence",
docid_field = "unique_file_id")
# Tokenize the corpus：a word per row
vas_tokens <- tokens(vas_corpus)
kwic_results <- kwic(
vas_tokens,
phrase(c("vasectomy", "vasectomies","procedure", "safe", "medical", "health", "recovery")),
window = 50, valuetype = "regex"
) %>%
as.data.frame()
#Strips out unique file name from article index
kwic_results <- kwic_results |>
mutate(matchrow = str_extract(docname, "^[^_]+_[^_]+"))
metadata <- index
# Join KWIC results with metadata
metadata$file_id <- as.character(metadata$file_id)
View(index)
index <- index %>%
mutate(file_id = as.numeric(factor(filename)))
index <- index %>%
mutate(file_id = as.numeric(factor(Title)))
#install.packages("readtext")
library(quanteda)
library(readtext)
library(tidyverse)
articles <- read.csv("article_df.csv")
library(readxl)
index <- read_excel("Results list for_vasectomy, vasectomies.XLSX")
articles <- articles %>%
mutate(file_id = as.numeric(factor(filename)))
articles <- articles %>%
mutate(unique_file_id = paste0(file_id, "_", row_number()))
index <- index %>%
mutate(file_id = as.numeric(factor(Title)))
# Now create the corpus with the new unique identifier
vas_corpus <- corpus(articles, text_field = "sentence",
docid_field = "unique_file_id")
# Tokenize the corpus：a word per row
vas_tokens <- tokens(vas_corpus)
kwic_results <- kwic(
vas_tokens,
phrase(c("vasectomy", "vasectomies","procedure", "safe", "medical", "health", "recovery")),
window = 50, valuetype = "regex"
) %>%
as.data.frame()
#Strips out unique file name from article index
kwic_results <- kwic_results |>
mutate(matchrow = str_extract(docname, "^[^_]+_[^_]+"))
metadata <- index
# Join KWIC results with metadata
metadata$file_id <- as.character(metadata$file_id)
kwic_with_metadata <- kwic_results %>%
inner_join(metadata, by = c("matchrow"="file_id")) |>
distinct(docname, .keep_all= TRUE) |>
select(matchrow, published_date, publication_location, publication_type_5,  from, to, pre, keyword, post, pattern, docname)
View(articles)
View(kwic_results)
index <- read_excel("Results list for_vasectomy, vasectomies.XLSX") %>%
janitor::clean_names()
articles <- articles %>%
mutate(file_id = as.numeric(factor(title)))
articles <- articles %>%
mutate(unique_file_id = paste0(file_id, "_", row_number()))
index <- index %>%
mutate(file_id = as.numeric(factor(title)))
# Now create the corpus with the new unique identifier
vas_corpus <- corpus(articles, text_field = "sentence",
docid_field = "unique_file_id")
# Tokenize the corpus：a word per row
vas_tokens <- tokens(vas_corpus)
kwic_results <- kwic(
vas_tokens,
phrase(c("vasectomy", "vasectomies","procedure", "safe", "medical", "health", "recovery")),
window = 50, valuetype = "regex"
) %>%
as.data.frame()
kwic_results <- kwic(
vas_tokens,
phrase(c("vasectomy", "vasectomies","procedure", "safe", "medical", "health", "recovery")),
window = 50, valuetype = "regex"
) %>%
as.data.frame()
metadata_grouped <- metadata %>%
group_by(filename) %>%
summarise(content = paste(sentence, collapse = " "))
names(index)
kwic_with_metadata <- kwic_results %>%
inner_join(metadata, by = c("matchrow"="file_id")) |>
distinct(docname, .keep_all= TRUE) |>
select(matchrow, title, published_date, publication_location, publication_4, publication_type_5, length, section, word_count, countries, byline, agg_copyright, cite, company, headline, hlead, publication_16, publication_type_17, pub_copyright, show, term, ticker, file_id, from, to, pre, keyword, post, pattern, docname)
#Strips out unique file name from article index
kwic_results <- kwic_results |>
mutate(matchrow = str_extract(docname, "^[^_]+_[^_]+"))
metadata <- index
# Join KWIC results with metadata
metadata$file_id <- as.character(metadata$file_id)
kwic_with_metadata <- kwic_results %>%
inner_join(metadata, by = c("matchrow"="file_id")) |>
distinct(docname, .keep_all= TRUE) |>
select(matchrow, title, published_date, publication_location, publication_4, publication_type_5, length, section, word_count, countries, byline, agg_copyright, cite, company, headline, hlead, publication_16, publication_type_17, pub_copyright, show, term, ticker, file_id, from, to, pre, keyword, post, pattern, docname)
View(metadata)
#Strips out unique file name from article index
kwic_results <- kwic_results |>
mutate(matchrow = str_extract(docname, "^[^_]+_[^_]+"))
#Strips out unique file name from article index
kwic_results <- kwic_results |>
mutate(matchrow = str_extract(docname, "^[^_]"))
kwic_results <- kwic_results |>
mutate(matchrow = str_extract(docname, "^[^_]+[^_]+"))
#Strips out unique file name from article index
kwic_results <- kwic_results |>
mutate(matchrow = str_extract(docname, "^[^_]+[^_]+"))
metadata <- index
# Join KWIC results with metadata
metadata$file_id <- as.character(metadata$file_id)
kwic_with_metadata <- kwic_results %>%
inner_join(metadata, by = c("matchrow"="file_id")) |>
distinct(docname, .keep_all= TRUE) |>
select(matchrow, title, published_date, publication_location, publication_4, publication_type_5, length, section, word_count, countries, byline, agg_copyright, cite, company, headline, hlead, publication_16, publication_type_17, pub_copyright, show, term, ticker, file_id, from, to, pre, keyword, post, pattern, docname)
kwic_with_metadata <- kwic_results %>%
inner_join(metadata, by = c("matchrow"="file_id")) |>
distinct(docname, .keep_all= TRUE) |>
select(matchrow, title, published_date, publication_location, publication_4, publication_type_5, length, section, word_count, countries, byline, agg_copyright, cite, company, headline, hlead, publication_16, publication_type_17, pub_copyright, show, term, ticker, from, to, pre, keyword, post, pattern, docname)
View(kwic_with_metadata)
#Eliminate duplication
kwic_with_metadata  <- kwic_with_metadata  %>%
distinct(docname, pre, keyword, post, newspaper_name, date, url) %>%
rename('Beginning of Passage' = pre, 'End of Passage' = post, Newspaper = newspaper_name, Date = date)
#Eliminate duplication
kwic_with_metadata  <- kwic_with_metadata  %>%
distinct(docname, pre, keyword, post, title) %>%
rename('Beginning of Passage' = pre, 'End of Passage' = post, Newspaper = company, Date =  published_date)
#Eliminate duplication
kwic_with_metadata  <- kwic_with_metadata  %>%
distinct(docname, pre, keyword, post, title) %>%
rename('Beginning of Passage' = pre, 'End of Passage' = post, Newspaper = publication_4, Date =  published_date)
#Eliminate duplication
kwic_with_metadata  <- kwic_with_metadata  %>%
distinct(docname, pre, keyword, post,published_date, publication_4) %>%
rename('Beginning of Passage' = pre, 'End of Passage' = post, Newspaper = publication_4, Date =  published_date)
View(index)
#Eliminate duplication
kwic_with_metadata  <- kwic_with_metadata  %>%
distinct(docname, pre, keyword, post,published_date, publication_16) %>%
rename('Beginning of Passage' = pre, 'End of Passage' = post, Newspaper = publication_16, Date =  published_date)
#Strips out unique file name from article index
kwic_results <- kwic_results |>
mutate(matchrow = str_extract(docname, "^[^_]+[^_]+"))
metadata <- index
# Join KWIC results with metadata
metadata$file_id <- as.character(metadata$file_id)
kwic_with_metadata <- kwic_results %>%
inner_join(metadata, by = c("matchrow"="file_id")) |>
distinct(docname, .keep_all= TRUE) |>
select(matchrow, title, published_date, publication_location, publication_4, publication_type_5, length, section, word_count, countries, byline, agg_copyright, cite, company, headline, hlead, publication_16, publication_type_17, pub_copyright, show, term, ticker, from, to, pre, keyword, post, pattern, docname)
kwic_with_metadata_clean  <- kwic_with_metadata  %>%
distinct(docname, pre, keyword, post,published_date, publication_16) %>%
rename('Beginning of Passage' = pre, 'End of Passage' = post, Newspaper = publication_16, Date =  published_date)
View(kwic_with_metadata_clean)
kwic_with_metadata_csv <- kwic_with_metadata_clean  %>%
add_column(Code = NA) %>%
add_column(Comments = NA) %>%
select(docname, Newspaper, Date, 'Beginning of Passage', keyword, 'End of Passage', Code, Comments)
write_csv(kwic_with_metadata_csv, "kwic_with_metadata.csv")
#install.packages("readtext")
library(quanteda)
library(readtext)
library(tidyverse)
articles <- read.csv("https://raw.githubusercontent.com/wellsdata/CompText_Jour/refs/heads/main/exercises/assets/data/kemi_df2.csv")
# Create a corpus using the 'sentence' column as the text field
my_corpus <- corpus(articles, text_field = "sentence") # build a new corpus from the texts
head(my_corpus)
# Tokenize the corpus
my_tokens <- tokens(my_corpus)
# Perform KWIC (Key Word in Context) search on the tokens
quanteda_test <- kwic(my_tokens, pattern = "journalist", valuetype = "regex") %>%
as.data.frame()
quanteda_test2 <- kwic(my_tokens, phrase(c("journalist", "reporter", "newspaper", "killer", "murdered", "killed", "investigate crimes against")), window = 50, valuetype = "regex") %>% as.data.frame()
#write.csv(quanteda_test, "quanteda_test.csv")
extracted_articles_index_oct_16_2024 <- read.csv("https://osf.io/download/uxw3a/?view_only=6c106acd6cb54f6f849e8c6f9098809f")
extracted_text_oct_16_2024 <- read_csv("https://osf.io/download/gw5dk/?view_only=6c106acd6cb54f6f849e8c6f9098809f")
lynch <- extracted_text_oct_16_2024 |>
filter(year > 1890 & year < 1895)
lynch <- lynch %>%
mutate(unique_file_id = paste0(file_id, "_", row_number()))
# Now create the corpus with the new unique identifier
lynch_corpus <- corpus(lynch, text_field = "sentence",
docid_field = "unique_file_id")
# Tokenize the corpus：a word per row
lynch_tokens <- tokens(lynch_corpus)
kwic_results <- kwic(
lynch_tokens,
phrase(c("mob", "masked men", "mask", "captors", "a party of",
"mob attacked", "mob caught", "overpowered by a mob",
"law into their own hands", "law unto themselves")),
window = 50, valuetype = "regex"
) %>%
as.data.frame()
#Strips out unique file name from article index
kwic_results <- kwic_results |>
mutate(matchrow = str_extract(docname, "^[^_]+_[^_]+"))
metadata <- extracted_articles_index_oct_16_2024
# Join KWIC results with metadata
kwic_with_metadata <- kwic_results %>%
inner_join(metadata, by = c("matchrow"="file_id")) |>
distinct(docname, .keep_all= TRUE) |>
select(matchrow, date, from, to, pre, keyword, post, pattern, newspaper_name, newspaper_city, newspaper_state,black_press,url, docname)
#cut useless returns
mobjunk <- c("mobile",  "automobile",  "automobiles", "mobilize", "mobley", "mobilizing", "democratic party of", "mobe", "moberly",  "mobil", "mobiles", "mobilized", "mobold")
kwic_with_metadata <- kwic_with_metadata %>%
filter(!keyword %in% mobjunk)
#Eliminate duplication
kwic_with_metadata  <- kwic_with_metadata  %>%
distinct(docname, pre, keyword, post, newspaper_name, date, url) %>%
rename('Beginning of Passage' = pre, 'End of Passage' = post, Newspaper = newspaper_name, Date = date)
kwic_with_metadata_csv <- kwic_with_metadata  %>%
add_column(Code = NA) %>%
add_column(Comments = NA) %>%
select(docname, Newspaper, Date, 'Beginning of Passage', keyword, 'End of Passage', Code, Comments)
#write_csv(kwic_with_metadata_csv, "kwic_with_metadata.csv")
