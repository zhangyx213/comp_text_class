---
title: "Baltimore Crime Topic Modeling HW"
author: "Breana Stevens"
date: '2025-04-03'
output: html_document
---

#--------------------------------------------- \# Mainstream Papers Topic Modeling #--------------------------------------------- This notebook will import 11,194 text files and related metadata files and execute basic topic modeling with LADAL Method I've adapted this LADAL tutorial for the lynching research: <https://ladal.edu.au/topicmodels.html>

Load up the packages if you haven't already....

```{r}
# install.packages("here")
# install.packages("tidytext")
# install.packages("quanteda")
# install.packages("tm")
# install.packages("topicmodels")
# install.packages("reshape2")
# install.packages("ggplot2")
# install.packages("wordcloud")
# install.packages("pals")
# install.packages("SnowballC")
# install.packages("lda")
# install.packages("ldatuning")
# install.packages("kableExtra")
# install.packages("DT")
# install.packages("flextable")
# install.packages("remotes")
# remotes::install_github("rlesur/klippy")
# install.packages("rio")
# install.packages("readtext")
# install.packages("formattable")

```

```{r include=FALSE}
# set options
options(stringsAsFactors = F)         # no automatic data transformation
options("scipen" = 100, "digits" = 4) # suppress math annotation
# load packages
here::here()
library(tidyverse)
library(tidytext)
library(rio)
library(readtext)
#topic modeling
library(quanteda)
library(tm)
library(topicmodels)
library(lda)
library(ldatuning)
# from tutorial packages
library(DT)
library(knitr) 
library(kableExtra) 
library(reshape2)
library(ggplot2)
library(wordcloud)
library(pals)
library(SnowballC)
library(flextable)

```

### Import Data

```{r include=FALSE}
#import 
crime <- read_csv("articles_df_copy.csv")

```

# Topic Modeling Predmoninantly White-Owned Papers

### Process into corpus object

#rsw comment - Nice job with the custom stop words.

```{r}
textdata <- crime %>% 
  select(filename, sentence, published_date) %>% 
  as.data.frame() %>% 
  rename(doc_id = filename, text= sentence)

# load stopwords
english_stopwords <- readLines("https://slcladal.github.io/resources/stopwords_en.txt", encoding = "UTF-8")

custom_stop_words1 <- bind_rows(
  tibble(word = c("baltimore", "date", "load", "sun", "english", "language", "usa", "md", "copyright", "publication", "reserved", "90", "newspaper", "copyright"))) |> 
  as.character()

custom_stop_words <- bind_rows(
  tibble(word = c("baltimore", "date", "load", "sun", "english", "language", "usa", "md", "copyright", "publication", "reserved", "90", "newspaper", "copyright"),
         lexicon = rep("custom", 14)), 
  stop_words
) |> 
  as.character()
# create corpus object
corpus <- Corpus(DataframeSource(textdata))
# Preprocessing chain
processedCorpus <- tm_map(corpus, content_transformer(tolower))
processedCorpus <- tm_map(processedCorpus, removeWords, english_stopwords)
processedCorpus <- tm_map(processedCorpus, removeWords, custom_stop_words)
processedCorpus <- tm_map(processedCorpus, removeWords, custom_stop_words1)
processedCorpus <- tm_map(processedCorpus, removePunctuation, preserve_intra_word_dashes = TRUE)
processedCorpus <- tm_map(processedCorpus, removeNumbers)
processedCorpus <- tm_map(processedCorpus, stemDocument, language = "en")
processedCorpus <- tm_map(processedCorpus, stripWhitespace)
```

```{r tm3a}
#DTM: rows correspond to the documents in the corpus. Columns correspond to the terms in the documents. Cells correspond to the weights of the terms. (Girder)
# compute document term matrix with terms >= minimumFrequency
minimumFrequency <- 5
DTM <- DocumentTermMatrix(processedCorpus, control = list(bounds = list(global = c(minimumFrequency, Inf))))
# have a look at the number of documents and terms in the matrix
dim(DTM)
# due to vocabulary pruning, we have empty rows in our DTM
# LDA does not like this. So we remove those docs from the
# DTM and the metadata
sel_idx <- slam::row_sums(DTM) > 0
DTM <- DTM[sel_idx, ]
textdata <- textdata[sel_idx, ]
#5 term minimum[1] 1387 3019
#5 term minimum[1] 308597 10339

```

## Topic proportions over time {.unnumbered}

We examine topics in the data over time by aggregating mean topic proportions per decade. These aggregated topic proportions can then be visualized, e.g. as a bar plot.

```{r}
# append month information for aggregation
textdata$month <- paste0(substr(textdata$published_date, 0, 3), "0")
```

Articles per month

```{r}
#install.packages("formattable")
articles_month <- textdata %>% 
  distinct(doc_id, .keep_all=TRUE) %>% 
  count(month) %>% 
  mutate(pct_total= (n/sum(n))) %>% 
  mutate(pct_total= formattable::percent(pct_total)) %>% 
  # mutate(pct_total = round(pct_total, 1)) %>% 
  arrange(desc(month))

library(kableExtra)
articles_month %>%
  kbl(caption = "Baltimore Crime Articles by Month (n=81, 04/03/2025", font_size = 30) %>%
  kable_classic(full_width = F, html_font = "Cambria") %>% 
  column_spec(1, bold = T, border_right = T) %>%
  column_spec(2, width = "5em") %>% 
  column_spec(3, width = "5em", background = "yellow") 



#Fact check 81 articles
#sum(articles_month$n)
```

```{r tm12}
# number of topics
# K <- 20
K <- 6
# set random number generator seed
set.seed(9161)
#Latent Dirichlet Allocation, LDA, Gibbs is the sampling method 
topicModel2 <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25, alpha = 0.2))
tmResult <- posterior(topicModel2)
theta <- tmResult$topics
beta <- tmResult$terms
topicNames <- apply(terms(topicModel2, 10), 2, paste, collapse = " ")  # reset topicnames
```

### Mean topic proportions per month

```{r}
# Step 1: Check dimensions
n_theta <- nrow(theta)
n_textdata <- length(textdata$month)

cat("Number of rows in theta: ", n_theta, "\n")
cat("Number of documents in textdata: ", n_textdata, "\n")

# Check if textdata contains all the documents in theta
common_ids <- intersect(rownames(theta), textdata$doc_id) # Assuming textdata has a 'doc_id' column

# Filter textdata to include only the documents present in theta
textdata_filtered <- textdata[textdata$doc_id %in% common_ids, ]

# Check dimensions after filtering
n_textdata_filtered <- nrow(textdata_filtered)
cat("Number of documents in filtered textdata: ", n_textdata_filtered, "\n")

# Ensure the lengths match now
if (n_theta != n_textdata_filtered) {
  stop("The number of rows in 'theta' still does not match the length of 'textdata_filtered$decade'.")
}

# Align rownames of theta with filtered textdata
theta_aligned <- theta[rownames(theta) %in% textdata_filtered$doc_id, ]

# Optional: Verify the order of documents
if (!all(rownames(theta_aligned) == textdata_filtered$doc_id)) {
  # If the order doesn't match, reorder one to match the other
  textdata_filtered <- textdata_filtered[match(rownames(theta_aligned), textdata_filtered$doc_id), ]
}

# Ensure they are now aligned and can be combined
if (!all(rownames(theta_aligned) == textdata_filtered$doc_id)) {
  stop("The document IDs still do not match. Please check the data alignment.")
}

# Step 2: Combine data
topic_data <- data.frame(theta_aligned, month = textdata_filtered$month)

# Step 3: Aggregate data
topic_proportion_per_month <- aggregate(. ~ month, data = topic_data, FUN = mean)


# get mean topic proportions per month
# topic_proportion_per_month <- aggregate(theta, by = list(month = textdata$month), mean)
# set topic names to aggregated columns
colnames(topic_proportion_per_month)[2:(K+1)] <- topicNames
# reshape data frame
vizDataFrame <- melt(topic_proportion_per_month, id.vars = "month")


```

#Examine topic names

```{r}
#enframe(): Converts a named list into a dataframe.
topics <- enframe(topicNames, name = "number", value = "text") %>% 
  unnest(cols = c(text)) 
  
topics
```

### Review the topics and determine a 1-2 word label after reading the source documents.

```{r}

#Topic 1	citi crime baltimor violenc communiti peopl word scott work length 

theta2 <- as.data.frame(theta)

topic1 <- theta2 %>% 
  rownames_to_column(var = "file") |> # putting the rownames into a new column called file
  mutate(file = str_remove(file, "^X"),  # Remove leading 'X'
         line = str_extract(file, "(?<=\\.txt)\\.\\d+")) |>   # Extract number after .txt
  mutate(file = str_remove(file, "\\.\\d+$")) |> 
  rename(topic1 = '1') |> # looking at first topic: citi crime baltimor violenc communiti peopl word scott work length 
  top_n(20, topic1) |> 
  arrange(desc(topic1)) |>  
  select(file, line, topic1) 


```

```{r}

#add categories

vizDataFrame <- vizDataFrame %>% 
  mutate(category = case_when(
    str_detect(variable,  "citi crime baltimor violenc communiti peopl word scott work length") ~ "crime mayor",
    str_detect(variable, "state usa maryland baltimor attorney offic unit investig depart drug") ~ "crime_policy",
    str_detect(variable, "crime law news enforc crimin negat govern investig correct bodi") ~ "crime_leg",
    str_detect(variable, "	classif language english gun publication-type newspap bodi street edit block load-date januari section main") ~ "junk words",
     str_detect(variable, "year baltimor crime polic juvenil end homicid document citi arrest") ~ "crime_policing",
    str_detect(variable, "baltimor news sun copyright right reserv servic restaur media compani") ~ "more junk",
    ))


```

# Fact Check and Validate Topics

Topic 1: lynchings "counti citi night mile jail day town morn march juli" Topic 2: criticizing_lynchings "law crime peopl lynch great excit state good citizen countri" Topic 3: negro_lynching "lynch mob negro jail men hang night crowd prison attempt" Topic 4: female_victim "negro murder white lynch man kill year assault charg mrs" Topic 5: 5_legal_proceedings "sheriff state court juri governor order offic prison judg deputi" Topic 6: lynch_mob "bodi fire shot hang hous tree found street rope door"

### for junk_words

```{r}
theta2 <- as.data.frame(theta)

junk_words <- theta2 %>% 
  #renaming for a general topic
  rename(junk_words = '4') %>% 
  top_n(20, junk_words ) %>%
  arrange(desc(junk_words )) %>% 
  select(junk_words)

# Apply rownames_to_column
junk_words  <- tibble::rownames_to_column(junk_words , "story_id") 

junk_words $story_id <- gsub("X", "", junk_words $story_id)

head(junk_words$story_id, 20)
#Checks out


```

### for 5 crime_policing

```{r}
theta2 <- as.data.frame(theta)

crime_policing <- theta2 %>% 
  #renaming for a general topic
  rename(crime_policing = '5') %>% 
  top_n(20, crime_policing ) %>%
  arrange(desc(crime_policing )) %>% 
  select(crime_policing)

# Apply rownames_to_column
crime_policing  <- tibble::rownames_to_column(crime_policing, "story_id") 

crime_policing $story_id <- gsub("X", "", crime_policing$story_id)

head(crime_policing$story_id, 20)
#Checks out


```

### for crime_leg

```{r}
theta2 <- as.data.frame(theta)

crime_leg <- theta2 %>% 
  #renaming for a general topic
  rename(crime_leg = '3') %>% 
  top_n(20, crime_leg ) %>%
  arrange(desc(crime_leg )) %>% 
  select(crime_leg )

# Apply rownames_to_column
crime_leg  <- tibble::rownames_to_column(crime_leg , "story_id") 

crime_leg $story_id <- gsub("X", "", crime_leg $story_id)

head(crime_leg$story_id, 20)
#Checks out 


```

#### crime_policy & more_junk

```{r}
theta2 <- as.data.frame(theta)

crime_policy<- theta2 %>% 
  #renaming for a general topic
  rename(crime_policy = '2') %>% 
  top_n(20, crime_policy) %>%
  arrange(desc(crime_policy)) %>% 
  select(crime_policy)

# Apply rownames_to_column
crime_policy <- tibble::rownames_to_column(crime_policy, "story_id") 

crime_policy$story_id <- gsub("X", "", crime_policy$story_id)

#Checks out 


theta2 <- as.data.frame(theta)

more_junk<- theta2 %>% 
  #renaming for a general topic
  rename(more_junk = '6') %>% 
  top_n(20, more_junk) %>%
  arrange(desc(more_junk)) %>% 
  select(more_junk)

# Apply rownames_to_column
more_junk <- tibble::rownames_to_column(more_junk, "story_id") 

more_junk$story_id <- gsub("X", "", more_junk$story_id)

#Checks out 

```

#Figure 15: white_paper_topics_oct_19_2024

```{r}
# plot topic proportions per decade as bar plot
ggplot(vizDataFrame, aes(x=month, y=value, fill=category)) + 
  geom_bar(stat = "identity") + ylab("proportion") + 
  scale_fill_manual(values = paste0(alphabet(20), "FF"), name = "month") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
   scale_fill_manual(values=c("#9933FF",
                              "#33FFFF",
                              "red",
                              "yellow",
                              "darkblue",
                              "green"))+
   #                           "blue"))+ 
   #                           #"pink",
   #                           #"gray",
   #                           #"orange")) +
  labs(title = "Common Narratives in Baltimore Crime News Coverage",
       subtitle = "Six probable topics in sample. n=81",
       caption = "Aggregate mean topic proportions per month Graphic by Breana Stevens, 04-03-2024")

# ggsave(here::here("../lynching_press/output_images_tables/Article_Images/Figure_15_white_topics_oct_19_2024.png"),device = "png",width=9,height=6, dpi=800)
```
#This exercise is beneficial because it helps us categorize the content in articles and understand the probability of those categories appearing. I think this will be especially interesting to see when applied to social media posts because those posts will touch on a broad range of issues. I would be able to understand the probability of the topic "crime" appearing versus "events". The shortfall here is that I have a good amount of junk words and categories that cover those junk words. The other shortfall is that my sample just simply is not large enough for this to be super meaningful. Rather than pulling articles just on crime, I would've preferred to run this on a large sample of articles about Baltimore...then I could have seen the probability of crime coming up in those articles. I'm less interesting in crime_policing or crime_legislation showing up, for example. It was interesting to see the word "Scott" come up in the first category, which I assigned to cover the topic "crime mayor" because it mentioned the mayor's name. 