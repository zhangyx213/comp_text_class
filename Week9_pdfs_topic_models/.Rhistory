install.packages("pdftools")
library(tidyverse)
library(pdftools)
#Using pdftools package. Good for basic PDF extraction
#Oct 17: removed split_file folder in a cleanup
text <- pdf_text("kemi.PDF")
#pdf_text reads the text from a PDF file.
writeLines(text, "./extracted_text/kemi_text.txt")
#writeLines writes this text to a text file
# Step 1: Read the entire text file into R
#You will need to alter this for your computer
#For Mac: In Finder, Cntl + click on the filename, NOW hold down Alt/Option, and an item to copy file path will appear as Copy "Filename" as Pathname
#https://stackoverflow.com/questions/52695546/how-to-copy-path-of-a-file-in-mac-os
file_path <- "./extracted_text/kemi_text.txt"
text_data <- readLines(file_path)
# Step 2: Combine lines into one single string
text_combined <- paste(text_data, collapse = "\n")
# Step 3: Split the text by the "End of Document" phrase
documents <- strsplit(text_combined, "End of Document")[[1]]
# Step 4: Write each section to a new file
output_dir <- "./extracted_text/"
for (i in seq_along(documents)) {
output_file <- file.path(output_dir, paste0("kemi_extracted_", i, ".txt"))
writeLines(documents[[i]], output_file)
}
cat("Files created:", length(documents), "\n")
kemi_index <- read_lines("./extracted_text/kemi_extracted_1.txt")
# Extract lines 16 to 58
extracted_lines <- kemi_index[16:58]
# Print the extracted lines to the console
cat(extracted_lines, sep = "\n")
extracted_lines <- extracted_lines |>
as.data.frame()
extracted_lines <- extracted_lines |>
mutate(extracted_lines = str_remove(extracted_lines, "\\| About LexisNexis \\| Privacy Policy \\| Terms & Conditions \\| Copyright Â© 2020 LexisNexis"))
# Step 1: Trim spaces and detect rows with titles and dates
cleaned_data <- extracted_lines |>
mutate(
# Trim leading and trailing spaces before detection
trimmed_line = str_trim(extracted_lines),
# Detect titles (start with a number and a period)
is_title = str_detect(trimmed_line, "^\\d+\\. "),
# Detect dates (e.g., "Aug 14, 2024")
is_date = str_detect(trimmed_line, "\\b\\w{3} \\d{1,2}, \\d{4}\\b")
)
# Step 2: Shift dates to align with corresponding titles
aligned_data <- cleaned_data |>
mutate(
date = ifelse(lead(is_date, 1), lead(trimmed_line, 1), NA_character_)  # Shift date to title's row
) |>
filter(is_title) |>
select(trimmed_line, date)  # Keep only the relevant columns
# Step 3: Rename columns for clarity
final_data <- aligned_data |>
rename(
title = trimmed_line,
date = date
)
#Step 4: Date and Publication in separate columns, and formatted
final_data <- separate(data = final_data, col = date, into = c("date2", "publication"), sep = "  ", extra = "merge", fill = "right")
#Step 5: Format date, clean headline
final_data <- final_data |>
mutate(date = as.Date(date2,format = "%b %d, %Y")) |>
mutate(title =str_remove(title, "^\\d+\\. ")) |>
subset(select = -(date2)) |>
mutate(index = row_number()) |>
select(index, date, title, publication)
write_csv(final_data, "final_data.csv")
View(aligned_data)
View(cleaned_data)
View(extracted_lines)
View(final_data)
# Step 1: Trim spaces and detect rows with titles and dates
cleaned_data <- extracted_lines |>
mutate(
# Trim leading and trailing spaces before detection
trimmed_line = str_trim(extracted_lines),
# Detect titles (start with a number and a period)
is_title = str_detect(trimmed_line, "^\\d+\\. "),
# Detect dates (e.g., "Aug 14, 2024")
is_date = str_detect(trimmed_line, "\\b\\w{3} \\d{1,2}, \\d{4}\\b")
)
# Step 2: Shift dates to align with corresponding titles
aligned_data <- cleaned_data |>
mutate(
date = ifelse(lead(is_date, 1), lead(trimmed_line, 1), NA_character_)  # Shift date to title's row
) |>
filter(is_title) |>
select(trimmed_line, date)  # Keep only the relevant columns
# Step 3: Rename columns for clarity
final_data <- aligned_data |>
rename(
title = trimmed_line,
date = date
)
#Step 4: Date and Publication in separate columns, and formatted
final_data <- separate(data = final_data, col = date, into = c("date2", "publication"), sep = "  ", extra = "merge", fill = "right")
#Step 5: Format date, clean headline
final_data <- final_data |>
mutate(date = as.Date(date2,format = "%b %d, %Y")) |>
mutate(title =str_remove(title, "^\\d+\\. ")) |>
subset(select = -(date2)) |>
mutate(index = row_number()) |>
select(index, date, title, publication)
write_csv(final_data, "final_data.csv")
#Install Required Tools
#Using the system() function to execute a command-line operation directly from within R.
system("brew install tesseract")
system("brew install xpdf")
system("xcode-select --install")
system("brew install libtiff")
system("brew install ghostscript")
system("brew install imagemagick")
#Executes pdftotext, a command-line tool used to convert PDF files to plain text.
system("pdftotext kemi.PDF ./extracted_text/kemi_tesseract.txt")
# set options
options(stringsAsFactors = F)         # no automatic data transformation
options("scipen" = 100, "digits" = 4) # suppress math annotation
# load packages
here::here()
library(tidyverse)
library(tidytext)
library(rio)
library(readtext)
#topic modeling
library(quanteda)
library(tm)
library(topicmodels)
library(lda)
library(ldatuning)
# from tutorial packages
library(DT)
library(knitr)
library(kableExtra)
library(reshape2)
library(ggplot2)
library(wordcloud)
library(pals)
library(SnowballC)
library(flextable)
#import 11,194 text files that were compiled into a df
lynch <- read_csv("https://osf.io/download/gw5dk/?view_only=6c106acd6cb54f6f849e8c6f9098809f")
#subset 9,589 predominantly white papers
lynch1 <- lynch %>%
filter(black_press == "N")
#subset the 1634 Black press news articles
black_articles <- lynch %>%
filter(black_press == "Y")
textdata <- lynch1 %>%
select(filename, sentence, year) %>%
as.data.frame() %>%
rename(doc_id = filename, text= sentence)
# load stopwords
english_stopwords <- readLines("https://slcladal.github.io/resources/stopwords_en.txt", encoding = "UTF-8")
# create corpus object
corpus <- Corpus(DataframeSource(textdata))
# Preprocessing chain
processedCorpus <- tm_map(corpus, content_transformer(tolower))
processedCorpus <- tm_map(processedCorpus, removeWords, english_stopwords)
processedCorpus <- tm_map(processedCorpus, removePunctuation, preserve_intra_word_dashes = TRUE)
processedCorpus <- tm_map(processedCorpus, removeNumbers)
processedCorpus <- tm_map(processedCorpus, stemDocument, language = "en")
processedCorpus <- tm_map(processedCorpus, stripWhitespace)
#DTM: rows correspond to the documents in the corpus. Columns correspond to the terms in the documents. Cells correspond to the weights of the terms. (Girder)
# compute document term matrix with terms >= minimumFrequency
minimumFrequency <- 5
DTM <- DocumentTermMatrix(processedCorpus, control = list(bounds = list(global = c(minimumFrequency, Inf))))
# have a look at the number of documents and terms in the matrix
dim(DTM)
# due to vocabulary pruning, we have empty rows in our DTM
# LDA does not like this. So we remove those docs from the
# DTM and the metadata
sel_idx <- slam::row_sums(DTM) > 0
DTM <- DTM[sel_idx, ]
textdata <- textdata[sel_idx, ]
#5 term minimum[1] 1387 3019
#5 term minimum[1] 308597 10339
# append decade information for aggregation
textdata$decade <- paste0(substr(textdata$year, 0, 3), "0")
#install.packages("formattable")
articles_decades <- textdata %>%
distinct(doc_id, .keep_all=TRUE) %>%
count(decade) %>%
mutate(pct_total= (n/sum(n))) %>%
mutate(pct_total= formattable::percent(pct_total)) %>%
# mutate(pct_total = round(pct_total, 1)) %>%
arrange(desc(decade))
library(kableExtra)
articles_decades %>%
kbl(caption = "LOC Lynching Articles by Decade (n=9,589, 10/23/2024)", font_size = 30) %>%
kable_classic(full_width = F, html_font = "Cambria") %>%
column_spec(1, bold = T, border_right = T) %>%
column_spec(2, width = "5em") %>%
column_spec(3, width = "5em", background = "yellow")
#Fact check 9589 articles
#sum(articles_decades$n)
# number of topics
# K <- 20
K <- 6
# set random number generator seed
set.seed(9161)
#Latent Dirichlet Allocation, LDA
topicModel2 <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25, alpha = 0.2))
tmResult <- posterior(topicModel2)
theta <- tmResult$topics
beta <- tmResult$terms
topicNames <- apply(terms(topicModel2, 10), 2, paste, collapse = " ")  # reset topicnames
# Step 1: Check dimensions
n_theta <- nrow(theta)
n_textdata <- length(textdata$decade)
cat("Number of rows in theta: ", n_theta, "\n")
cat("Number of documents in textdata: ", n_textdata, "\n")
# Check if textdata contains all the documents in theta
common_ids <- intersect(rownames(theta), textdata$doc_id) # Assuming textdata has a 'doc_id' column
# Filter textdata to include only the documents present in theta
textdata_filtered <- textdata[textdata$doc_id %in% common_ids, ]
# Check dimensions after filtering
n_textdata_filtered <- nrow(textdata_filtered)
cat("Number of documents in filtered textdata: ", n_textdata_filtered, "\n")
# Ensure the lengths match now
if (n_theta != n_textdata_filtered) {
stop("The number of rows in 'theta' still does not match the length of 'textdata_filtered$decade'.")
}
# Align rownames of theta with filtered textdata
theta_aligned <- theta[rownames(theta) %in% textdata_filtered$doc_id, ]
# Optional: Verify the order of documents
if (!all(rownames(theta_aligned) == textdata_filtered$doc_id)) {
# If the order doesn't match, reorder one to match the other
textdata_filtered <- textdata_filtered[match(rownames(theta_aligned), textdata_filtered$doc_id), ]
}
# Ensure they are now aligned and can be combined
if (!all(rownames(theta_aligned) == textdata_filtered$doc_id)) {
stop("The document IDs still do not match. Please check the data alignment.")
}
# Step 2: Combine data
topic_data <- data.frame(theta_aligned, decade = textdata_filtered$decade)
# Step 3: Aggregate data
topic_proportion_per_decade <- aggregate(. ~ decade, data = topic_data, FUN = mean)
# get mean topic proportions per decade
# topic_proportion_per_decade <- aggregate(theta, by = list(decade = textdata$decade), mean)
# set topic names to aggregated columns
colnames(topic_proportion_per_decade)[2:(K+1)] <- topicNames
# reshape data frame
vizDataFrame <- melt(topic_proportion_per_decade, id.vars = "decade")
# #filter out 1960 - one article
vizDataFrame <- vizDataFrame %>%
filter(!decade==1960)
#enframe(): Converts a named list into a dataframe.
topics <- enframe(topicNames, name = "number", value = "text") %>%
unnest(cols = c(text))
topics
#Topic 1	counti citi night mile jail day town morn march juli
theta2 <- as.data.frame(theta)
topic1 <- theta2 %>%
rownames_to_column(var = "file") |> # putting the rownames into a new column called file
mutate(file = str_remove(file, "^X"),  # Remove leading 'X'
line = str_extract(file, "(?<=\\.txt)\\.\\d+")) |>   # Extract number after .txt
mutate(file = str_remove(file, "\\.\\d+$")) |>
rename(topic1 = '1') |> # looking at first topic: ounti citi night mile jail day town morn march juli
top_n(20, topic1) |>
arrange(desc(topic1)) |>
select(file, line, topic1)
#add categories
vizDataFrame <- vizDataFrame %>%
mutate(category = case_when(
str_detect(variable,  "counti citi night mile jail day town morn march juli") ~ "lynchings",
str_detect(variable, "law crime peopl lynch great excit state good citizen countri") ~ "critizing_lynching",
str_detect(variable, "lynch mob negro jail men hang night crowd prison attempt") ~ "negro_lynching",
str_detect(variable, "negro murder white lynch man kill year assault charg mrs") ~ "female_victim",
str_detect(variable, "sheriff state court juri governor order offic prison judg deputi") ~ "legal",
str_detect(variable, "bodi fire shot hang hous tree found street rope door") ~ "lynch_mob",
))
View(vizDataFrame)
#add categories
vizDataFrame <- vizDataFrame %>%
mutate(category = case_when(
str_detect(variable,  "counti citi night mile jail day town morn march juli") ~ "lynchings",
str_detect(variable, "law crime peopl lynch great excit state good citizen countri") ~ "critizing_lynching",
str_detect(variable, "lynch mob negro jail men hang night crowd prison attempt") ~ "negro_lynching",
str_detect(variable, "negro murder white lynch man kill year assault charg mrs") ~ "female_victim",
str_detect(variable, "sheriff state court juri governor order offic prison judg deputi") ~ "legal",
str_detect(variable, "bodi fire shot hang hous tree found street rope door") ~ "lynch_mob",
))
theta2 <- as.data.frame(theta)
female <- theta2 %>%
#renaming for a general topic
rename(female = '4') %>%
top_n(20, female ) %>%
arrange(desc(female )) %>%
select(female )
# Apply rownames_to_column
female  <- tibble::rownames_to_column(female , "story_id")
female $story_id <- gsub("X", "", female $story_id)
head(female$story_id, 20)
#Checks out
theta2 <- as.data.frame(theta)
legal <- theta2 %>%
#renaming for a general topic
rename(legal = '5') %>%
top_n(20, legal ) %>%
arrange(desc(legal )) %>%
select(legal )
# Apply rownames_to_column
legal  <- tibble::rownames_to_column(legal , "story_id")
legal $story_id <- gsub("X", "", legal $story_id)
head(legal$story_id, 20)
#Checks out
#Topic 1	counti citi night mile jail day town morn march juli
theta2 <- as.data.frame(theta)
topic1 <- theta2 %>%
rownames_to_column(var = "file") |> # putting the rownames into a new column called file
mutate(file = str_remove(file, "^X"),  # Remove leading 'X'
line = str_extract(file, "(?<=\\.txt)\\.\\d+")) |>   # Extract number after .txt
mutate(file = str_remove(file, "\\.\\d+$")) |>
rename(topic1 = '1') |> # looking at first topic: ounti citi night mile jail day town morn march juli
top_n(20, topic1) |>
arrange(desc(topic1)) |>
select(file, line, topic1)
View(topic1)
theta2 <- as.data.frame(theta)
unknown <- theta2 %>%
#renaming for a general topic
rename(unknown = '3') %>%
top_n(20, unknown ) %>%
arrange(desc(unknown )) %>%
select(unknown )
# Apply rownames_to_column
unknown  <- tibble::rownames_to_column(unknown , "story_id")
unknown $story_id <- gsub("X", "", unknown $story_id)
head(unknown$story_id, 20)
#Checks out
#main theme is negros are lynching victims
theta2 <- as.data.frame(theta)
critic<- theta2 %>%
#renaming for a general topic
rename(critic = '2') %>%
top_n(20, critic) %>%
arrange(desc(critic)) %>%
select(critic)
# Apply rownames_to_column
critic <- tibble::rownames_to_column(critic, "story_id")
critic$story_id <- gsub("X", "", critic$story_id)
#Checks out
# plot topic proportions per decade as bar plot
ggplot(vizDataFrame, aes(x=decade, y=value, fill=category)) +
geom_bar(stat = "identity") + ylab("proportion") +
scale_fill_manual(values = paste0(alphabet(20), "FF"), name = "decade") +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
scale_fill_manual(values=c("#9933FF",
"#33FFFF",
"red",
"yellow",
"darkblue",
"green"))+
#                           "blue"))+
#                           #"pink",
#                           #"gray",
#                           #"orange")) +
labs(title = "Common Narratives in Lynching News Coverage",
subtitle = "Six probable topics in white press sample. n=9,590",
caption = "Aggregate mean topic proportions per decade. Graphic by (redated - peer review) & (redated - peer review), 10-19-2024")
# ggsave(here::here("../lynching_press/output_images_tables/Article_Images/Figure_15_white_topics_oct_19_2024.png"),device = "png",width=9,height=6, dpi=800)
textdata <- black_articles %>%
select(filename, sentence, year) %>%
as.data.frame() %>%
rename(doc_id = filename, text= sentence)
# load stopwords
english_stopwords <- readLines("https://slcladal.github.io/resources/stopwords_en.txt", encoding = "UTF-8")
# create corpus object
corpus <- Corpus(DataframeSource(textdata))
# Preprocessing chain
processedCorpus <- tm_map(corpus, content_transformer(tolower))
processedCorpus <- tm_map(processedCorpus, removeWords, english_stopwords)
processedCorpus <- tm_map(processedCorpus, removePunctuation, preserve_intra_word_dashes = TRUE)
processedCorpus <- tm_map(processedCorpus, removeNumbers)
processedCorpus <- tm_map(processedCorpus, stemDocument, language = "en")
processedCorpus <- tm_map(processedCorpus, stripWhitespace)
